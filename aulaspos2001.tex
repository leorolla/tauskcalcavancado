
% !TeX program = pdftex
% !TeX spellcheck = pt_BR

\magnification\magstep1
\input acentos.tex
\input amssym.def
\input xyall

\font\pequeno cmr10 scaled 700

\font\titfont cmbx10 scaled \magstep1

\def\R{I\!\!R}
\def\Q{{\Bbb Q}}
\def\N{I\!\!N}
\def\Z{{\Bbb Z}}
\def\Bola{{\rm B}}
\def\compl{{\rm c}}
\def\diam{{\rm diam}}
\def\Int{{\rm int}}
\def\Ext{{\rm ext}}
\def\dd{{\rm d}}
\def\Jac{{\rm J}}
\def\Id{{\rm Id}}
\def\Img{{\rm Im}}
\def\Dim{{\rm dim}}
\def\Bfrak{{\frak B}}
\def\Matr#1#2{{\rm M}_{#1\times#2}}
\def\Lin{{\rm Lin}}
\def\Multlin{\hbox{\rm Mult-lin}}
\def\Multlinsym{\hbox{\rm Mult-lin}^{\rm sim}}
\def\Bil{{\rm Bil}}
\def\sen{\mathop{\rm sen}\nolimits}
\def\atg{\mathop{\rm arc\;tg}\nolimits}
\def\arccos{\mathop{\rm arc\;cos}\nolimits}
\def\arcsen{\mathop{\rm arc\;sen}\nolimits}
\def\GL{{\rm GL}}
\def\sen{\mathop{\rm sen}\nolimits}
\def\norma{{\Vert\cdot\Vert}}
\def\pint{\langle\cdot,\cdot\rangle}
\def\Ker{{\rm Ker}}
\def\anul{{\rm o}}
\def\Hess{{\rm Hess}}
\def\Pol{{\cal P}}
\def\Sym{{\rm Sim}}
\def\V{{\rm V}}
\def\graph{{\rm Gr}}
\def\limpp{\vbox{\offinterlineskip\hbox to 20pt{\hfil\pequeno pp\hfil}\hbox to 20pt{\leaders\hbox{$-$}\hfil\hskip-5pt$\longrightarrow$}}}
\def\limu{\vbox{\offinterlineskip\hbox to 20pt{\hfil\pequeno u\hfil}\hbox to 20pt{\leaders\hbox{$-$}\hfil\hskip-5pt$\longrightarrow$}}}
\def\Prova{\noindent{\bf Demonstração.}\enspace}
\def\fimprova{\hskip 3pt\vrule height 6pt depth 0pt width 6pt}

\

\bigskip
\bigskip
\bigskip

\hfil
{\bf Notas de Aula em Cálculo Avançado}

\bigskip
\bigskip

\hfil
Daniel Tausk

\bigskip
\bigskip
\bigskip
\bigskip

\copyright\ 2001 Daniel Tausk.
Permitido o uso nos termos da licença CC BY-SA 4.0.

\vfil\eject

\centerline{\bf Aula número $2$ (08/03)}
\bigskip
\bigskip
\proclaim Definição. Um subconjunto $U\subset\R^n$ é chamado {\rm
aberto} se para todo $x\in U$ existe $r>0$ tal que
$\Bola(x;r)\subset U$.

\smallskip

\noindent{\tt Notação}: para $x\in\R^n$ e $r>0$ denotamos por
$\Bola(x;r)$ a {\sl bola aberta\/} de centro $x$ e raio $r$
definida por:
$$\Bola(x;r)=\big\{y\in\R^n:\Vert x-y\Vert<r\big\}$$
e por $\Bola[x;r]$ a {\sl bola fechada\/} de centro $x$ e raio $r$
definida por:
$$\Bola[x;r]=\big\{y\in\R^n:\Vert x-y\Vert\le r\big\}.$$
Por $\Vert\cdot\Vert$ denotamos a {\sl norma Euclideana}: $\Vert
x\Vert=\Big(\sum_{i=1}^nx_i^2\Big)^{1\over2}$.

\smallskip

\proclaim Definição. Seja $U\subset\R^m$ um aberto e considere uma
função $f:U\to\R^n$. Dizemos que $f$ é {\rm diferenciável} num
ponto $x_0\in U$ se existe uma transformação linear
$T:\R^m\to\R^n$ de modo que a aplicação $r$ definida por:
$$f(x_0+h)=f(x_0)+T(h)+r(h),$$
satisfaz a condição $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$.

De modo abreviado: $f$ é diferenciável em $x_0\in U$ se existe
$T:\R^m\to\R^n$ linear tal que:
$$\lim_{h\to 0}{f(x_0+h)-f(x_0)-T(h)\over\Vert h\Vert}=0.$$

\medskip

\item{(1)} {\bf O caso ${\bf n=m=1}$}.

\smallskip

Recordar que as transformações lineares $T:\R\to\R$ são todas da
forma $T(h)=Lh$, onde $L$ é um número real.

\smallskip

\proclaim Teorema. Uma função $f:U\subset\R\to\R$ é diferenciável
num ponto $x_0\in U$ se e somente se $f$ é derivável no ponto
$x_0$ (no sentido do Cálculo I), i.e., se existe o limite:
$$f'(x_0)=\lim_{h\to0}{f(x_0+h)-f(x_0)\over h}.$$

\Prova Observe que $f$ é diferenciável no ponto $x_0$ se e somente
se existe $L\in\R$ tal que:
$$\lim_{h\to0}{f(x_0+h)-f(x_0)-Lh\over h}\;{h\over\vert h\vert}=0;$$
notando que a quantidade ${h\over\vert h\vert}=\pm1$ é limitada,
vemos que a condição acima equivale a:
$$\lim_{h\to0}{f(x_0+h)-f(x_0)-Lh\over h}=\lim_{h\to0}{f(x_0+h)-f(x_0)\over
h}-L=0.$$ Isso completa a demonstração.\fimprova

\noindent{\tt Observação}: note que a transformação linear $T$ que
aparece na definição de função diferenciável é $T(h)=f'(x_0)h$
nesse caso.

\smallskip

\noindent{\tt Exemplo}: Considerando a função $f:\R\to\R$,
$f(x)=x^3$, e o ponto $x_0=4$ então escrevemos:
$$f(x_0+h)=(4+h)^3=64+48h+12h^2+h^3=f(x_0)+T(h)+r(h),$$
onde escolhemos $T(h)=48h$ (que é linear!) e daí $r(h)=12h^2+h^3$.
Observe que de fato $\lim_{h\to0}{r(h)\over\vert
h\vert}=\lim_{h\to0}{r(h)\over h}=0$.

\medskip

\item{(2)} {\bf Relacionando diferenciação com derivadas direcionais e
derivadas parciais}.

\smallskip

\proclaim Definição. Dada uma função $f:U\to\R^n$ definida num
aberto $U\subset\R^m$, um ponto $x_0\in U$ e um vetor $v\in\R^m$
então a {\rm derivada direcional de $f$ no ponto $x_0$ e na
direção $v$} é definida por (se existir):
$${\partial f\over\partial
v}(x_0)=\lim_{t\to0}{f(x_0+tv)-f(x_0)\over t}.$$ Se $e_i$ denota o
$i$-ésimo vetor da {\rm base canônica} de $\R^m$ então a derivada
direcional ${\partial f\over\partial e_i}(x_0)$ é denotada também
por ${\partial f\over\partial x_i}(x_0)$ e é chamada a {\rm
$i$-ésima derivada parcial} de $f$ no ponto $x_0$.

\smallskip

Suponha que $f:U\subset\R^m\to\R^n$ é diferenciável no ponto
$x_0\in U$, de modo que existe $T:\R^m\to\R^n$ linear tal que:
$$\lim_{h\to0}{f(x_0+h)-f(x_0)-T(h)\over\Vert h\Vert}=0.$$
Fixe $v\in\R^m$ não nulo; vamos fazer a mudança de variável $h=tv$
no limite acima, onde $t\to0$:
$$\lim_{t\to0}{f(x_0+tv)-f(x_0)-T(tv)\over\Vert tv\Vert}=0.$$
Obtemos então:
$$\lim_{t\to0}{1\over\Vert
v\Vert}\left[{f(x_0+tv)-f(x_0)-T(tv)\over t}\right]{t\over\vert
t\vert}=0;$$ pela linearidade de $T$, $T(tv)=tT(v)$ e portanto:
$$\lim_{t\to0}{1\over\Vert
v\Vert}\left[{f(x_0+tv)-f(x_0)\over t}-T(v)\right]{t\over\vert
t\vert}=0.$$ Como ${1\over\Vert v\Vert}$ é constante e
${t\over\vert t\vert}=\pm1$ é limitado, a igualdade acima é
equivalente a:
$$\lim_{t\to0}{f(x_0+tv)-f(x_0)\over t}-T(v)=0,$$
o que mostra que a derivada direcional ${\partial f\over\partial
v}(x_0)$ existe e é igual a $T(v)$.

\proclaim Teorema. Se $f:U\subset\R^m\to\R^n$ é diferenciável num
ponto $x_0\in U$ então $f$ admite derivadas direcionais no ponto
$x_0$ em qualquer direção e vale a fórmula:
$${\partial f\over\partial v}(x_0)=T(v),$$
para todo $v\in\R^m$, onde $T:\R^m\to\R^n$ é uma transformação
linear como a que aparece na definição de função diferenciável.

\Prova O caso $v\ne0$ foi provado acima e o caso $v=0$ é
trivial.\fimprova

\smallskip

\proclaim Corolário. A transformação linear $T$ que aparece na
definição de função diferenciável é {\rm única}.

\Prova Se $T$ e $T'$ são duas transformações lineares que
``servem'' para a definição de função diferenciável então
$T(v)={\partial f\over\partial v}(x_0)=T'(v)$ para todo
$v\in\R^m$, pelo Teorema acima.\fimprova

\proclaim Definição. A transformação linear $T$ que aparece na
definição de função diferenciável (que é única, pelo Corolário
acima) é denotada por $\dd f(x_0)$ e é chamada a {\rm diferencial
da função $f$ no ponto $x_0$}.

Quando $f$ é diferenciável no ponto $x_0$ podemos escrever agora:
$$f(x_0+h)=f(x_0)+\dd f(x_0)\cdot h+r(h),$$
com $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$.

\noindent{\tt Observação}: provamos a seguinte {\bf fórmula
importante} para uma função diferenciável $f:U\subset\R^m\to\R^n$
num ponto $x_0\in U$:
$$\dd f(x_0)\cdot v={\partial f\over\partial v}(x_0),\quad\hbox{para todo}\ v\in\R^m.$$

\medskip

\item{(3)} {\bf Matriz Jacobiana}.

\smallskip

Recorde da álgebra linear que se $T:V\to V'$ é um operador linear,
com $V$, $V'$ espaços vetoriais, ${\frak B}=(b_1,\ldots,b_m)$ uma
base de $V$ e ${\frak B}'=(b'_1,\ldots,b'_n)$ uma base de $V'$
então a {\sl matriz que representa $T$ nas bases ${\frak B}$ e
${\frak B}'$}, denotada por $[T]_{{\frak B}{\frak B}'}$, é a
matriz $n\times m$ cuja entrada na linha $i$, coluna $j$, é a
$i$-ésima coordenada do vetor $T(b_j)$ na base ${\frak B}'$ (veja
exercícios).

\smallskip

\proclaim Definição. Se $f:U\subset\R^m\to\R^n$ é uma função
diferenciável num ponto $x_0\in U$ então a {\rm matriz Jacobiana}
de $f$ no ponto $x_0$, denotada por $\Jac f(x_0)$, é a matriz que
representa o operador linear $\dd f(x_0):\R^m\to\R^n$ nas bases
canônicas de $\R^m$ e $\R^n$.

Para calcular as entradas de $\Jac f(x_0)$ fazemos o seguinte:
$$\eqalign{\Jac f(x_0)_{ij}=\big[\dd f(x_0)\cdot e_j\big]^i=\left[{\partial
f\over\partial e_j}(x_0)\right]^i=\left[{\partial f\over\partial
x_j}(x_0)\right]^i&={\partial f^i\over\partial x_j}(x_0),\cr
&i=1,\ldots,n,\ j=1,\ldots,m;\cr}$$ para a última passagem acima
veja os exercícios.

Provamos o seguinte: \proclaim Teorema. Se $f:U\subset\R^m\to\R^n$
é uma função diferenciável num ponto $x_0\in U$ então a matriz
Jacobiana de $f$ no ponto $x_0$ é dada por:
$$\Jac f(x_0)=\pmatrix{{\partial f^1\over\partial
x_1}(x_0)&\cdots&{\partial f^1\over\partial x^m}(x_0)\cr
\vdots&\ddots&\vdots\cr {\partial f^n\over\partial
x_1}(x_0)&\cdots&{\partial f^n\over\partial x^m}(x_0)\cr}.$$

\smallskip

\noindent{\tt Exemplo}: Se $f:U\subset\R^2\to\R^2$ é dada por
$$f(x,y)=\left({1\over xy},\cos x\right),\quad
U=\big\{(x,y)\in\R^2:xy\ne0\big\}\ \ \hbox{($U$ é aberto!)}$$
então a
diferencial de $f$ num ponto $(x_0,y_0)\in U$ é dada por (use a
matriz Jacobiana!):
$$\dd f(x_0,y_0)\cdot(h,k)=\left(-{h\over x_0^2y_0}-{k\over
x_0y_0^2},-h\sen x_0\right),$$ para $(h,k)\in\R^2$. Para ser
rigoroso, {\sl ainda não temos teoria suficiente para ver
rapidamente que $f$ é diferenciável\/}! Admitindo esse fato por
enquanto, obtemos a fórmula acima para $\dd f(x_0,y_0)$.

\smallskip

\noindent{\tt Observação}: Quando $f:U\subset\R^m\to\R^n$ é
diferenciável em todos os pontos de $U$ então dizemos que $f$ é
{\sl diferenciável em $U$}. Para cada $x_0\in U$, $\dd f(x_0)$ é
um operador linear de $\R^m$ em $\R^n$; que tipo de objeto seria
$\dd f$? Considere o espaço vetorial:
$$\Lin(\R^m,\R^n)=\hbox{operadores lineares de $\R^m$ em
$\R^n$};$$ recorde da álgebra linear que $\Lin(\R^m,\R^n)$ pode
ser identificado naturalmente com o espaço vetorial de todas as
matrizes reais $n\times m$ (que tem dimensão $nm$). A {\sl
diferencial\/} de $f$ nos dá então uma aplicação:
$$\dd f:U\subset\R^m\longrightarrow\Lin(\R^m,\R^n),$$
que leva cada $x_0\in U$ em $\dd f(x_0)\in\Lin(\R^m,\R^n)$. No
caso $m=n=1$ então $\Lin(\R^m,\R^n)$ pode ser identificado com
$\R$ (matrizes $1\times1$!) e daí $\dd f$ vira uma aplicação de
$U\subset\R$ em $\R$; essa aplicação é a clássica derivada $f'$ de
$f$ do Cálculo I!

\medskip

\item{(4)} {\bf O caso n=1: gradientes}.

\smallskip

{\bf Esse assunto será retomado depois com mais detalhes.}

\smallskip

Se $f:U\subset\R^m\to\R$ é uma função {\sl a valores reais\/}
diferenciável num ponto $x_0\in U$ então a diferencial de $f$ em
$x_0$ define um elemento do dual de $\R^m$, i.e., $\dd
f(x_0)\in{\R^m}^*$ (recorde que o {\sl dual\/} de um espaço
vetorial real $V$ é o espaço vetorial $\Lin(V,\R)$).

O produto interno canônico de $\R^m$ induz um isomorfismo de
$\R^m$ sobre seu dual ${\R^m}^*$ dado por $v\mapsto\langle
v,\cdot\rangle$. Esse isomorfismo leva o vetor
$v=(v_1,\ldots,v_m)$ de $\R^m$ sobre o elemento de ${\R^m}^*$ que
é representado pela matriz $(v_1\ \cdots\ v_m)$ (é uma matriz
$1\times m$!). Já vimos que a matriz que representa $\dd f(x_0)$
(ou seja, $\Jac f(x_0)$) é dada por:
$$\Jac f(x_0)=\left({\partial f\over\partial x_1}(x_0)\ \cdots\
{\partial f\over\partial x_m}(x_0)\right).$$ O vetor de $\R^m$
correspondente a $\dd f(x_0)\in{\R^m}^*$ é portanto o vetor
$$\nabla f(x_0)=\left({\partial f\over\partial
x_1}(x_0),\ldots,{\partial f\over\partial x_m}(x_0)\right)$$
conhecido como o {\sl gradiente\/} de $f$ no ponto $x_0$. Obtemos
então a fórmula:
$${\partial f\over\partial v}(x_0)=\dd f(x_0)\cdot v=\langle\nabla
f(x_0),v\rangle,$$ para todo $v\in\R^m$.

\medskip

\item{(5)} {\bf Funções diferenciáveis são contínuas}.

\smallskip

Vamos usar aqui que transformações lineares $T:\R^m\to\R^n$ são
contínuas; isso segue do fato que as mesmas podem ser escritas em
termos de somas e produtos (isso será rediscutido depois).

\proclaim Teorema. Se $f:U\subset\R^m\to\R^n$ é diferenciável no
ponto $x_0\in U$ então $f$ é contínua no ponto $x_0\in U$.

\Prova Escrevemos:
$$f(x_0+h)=f(x_0)+\dd f(x_0)\cdot h+r(h),$$
com $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$; {\sl a fortiori\/}
$\lim_{h\to0}r(h)=\lim_{h\to0}{r(h)\over\Vert h\Vert}\Vert
h\Vert=0$. Fazendo $h\to0$ na fórmula acima obtemos, usando a
continuidade da transformação linear $\dd f(x_0)$:
$$\lim_{h\to0}f(x_0+h)=f(x_0)+\dd f(x_0)\cdot 0=f(x_0).$$
Isso completa a demonstração.\fimprova

\smallskip

\noindent{\tt Observação}: A existência das derivadas parciais
${\partial f\over\partial x_i}(x_0)$, $i=1,\ldots,m$, não garante
que $f$ é diferenciável em $x_0$. Nem a existência de todas as
derivadas direcionais de $f$ em $x_0$, junto com a continuidade de
$f$ em $x_0$ (e até mesmo junto com a linearidade de
$v\mapsto{\partial f\over\partial v}(x_0)$) implicam a
diferenciabilidade de $f$ em $x_0$. Para contra-exemplos estranhos
procurem o Curso de Análise Vol.\ II (capítulo 3) ou o livro de
Cálculo II do Guidorizzi. Não daremos importância a tais exemplos.

\smallskip

\noindent{\tt Observação}: Toda teoria desenvolvida até agora pode
ser feita trocando $\R^m$ e $\R^n$ por espaços vetoriais reais de
dimensão finita arbitrários! Para isso precisamos apenas fazer
alguns esclarecimentos sobre normas em espaços vetoriais quaisquer
(fica para depois). Observamos também que a noção de derivada
parcial não faz sentido quando trocamos $\R^m$ por um espaço
vetorial qualquer, a não ser que uma base seja fixada em tal
espaço; similarmente, para dar sentido à matriz Jacobiana,
precisamos fazer escolhas de bases nos espaços vetoriais em
questão.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver problemas me procura).

\bigskip

\noindent{\tt Álgebra Linear}.

\medskip

\item{1.} Seja $T:V\to V'$ uma transformação linear, onde $V$ e
$V'$ são espaços vetoriais de dimensão finita. Sejam ${\frak B}$ e
${\frak B}'$ bases de $V$ e $V'$ respectivamente. Denote por
$[T]_{{\frak B}{\frak B}'}$ a matriz que representa $T$ com
respeito às bases ${\frak B}$ e ${\frak B'}$; por $[x]_{\frak B}$
denotamos o vetor coluna que contém as coordenadas com respeito à
base ${\frak B}$ de um vetor $x\in V$.

\smallskip

\itemitem{(a)} Mostre que
$$[T]_{{\frak B}{\frak B}'}[x]_{\frak
B}=\big[T(x)\big]_{{\frak B}'}.$$

\itemitem{(b)} Se $S:V'\to V''$ é uma outra transformação linear,
com $V''$ um espaço vetorial de dimensão finita e ${\frak B}''$
uma base de $V''$, mostre que
$$[S]_{{\frak B}'{\frak
B}''}[T]_{{\frak B}{\frak B}'}=[S\circ T]_{{\frak B}{\frak
B}''}.$$

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{2.} Seja $f:U\to\R^n$ uma função com
$U\subset\R^m$ aberto. Escreva $f=(f^1,\ldots,f^n)$ com cada
$f^i:U\to\R$. Para $x_0\in U$, mostre que:

\smallskip

\itemitem{(a)} $f$ é diferenciável em $x_0$ se e somente se cada
$f^i$ é diferenciável em $x_0$, sendo nesse caso $\big[\dd
f(x_0)\big]^i=\dd f^i(x_0)$, $i=1,\ldots,n$ (``a $i$-ésima
coordenada da diferencial é a diferencial da $i$-ésima
coordenada'').

\itemitem{(b)} para $v\in\R^m$, mostre que ${\partial
f\over\partial v}(x_0)$ existe se e somente se ${\partial
f^i\over\partial v}(x_0)$ existe para cada $i=1,\ldots,n$; nesse caso,
mostre que $\big[{\partial f\over\partial v}(x_0)\big]^i={\partial
f^i\over\partial v}(x_0)$, $i=1,\ldots,n$ (`` a $i$-ésima coordenada de
uma derivada direcional é a derivada direcional da $i$-ésima
coordenada'').

\vfil\eject

\centerline{\bf Aula número $3$ (13/03)}
\bigskip
\bigskip

\item{(1)} {\bf O caso ${\bf m=1}$: curvas em ${\bf\R^n}$; relacionando vetores
tangentes e diferenciais}.

\smallskip

Quando $m=n=1$, vimos que a noção de diferenciabilidade coincide
com a noção clássica de derivada do Cálculo I: mais precisamente,
$f:U\subset\R\to\R$ é diferenciável em $x_0\in U$ se e somente se
existe a derivada $f'(x_0)\in\R$ (do Cálculo I) e nesse caso a
diferencial de $f$ no ponto $x_0$ é dada por $\dd f(x_0)\cdot
h=f'(x_0)h$. Vamos agora ver o que acontece quando apenas $m=1$;
estamos falando então de curvas em $\R^n$. Como mostraremos a
seguir, o caso $m=1$ com $n$ qualquer é essencialmente idêntico ao
caso $m=n=1$ discutido na aula anterior (na verdade, só separamos
o caso $m=n=1$ por motivos didáticos).

\proclaim Definição. Dada uma aplicação $f:U\subset\R\to\R^n$
então o {\rm vetor tangente} a $f$ no ponto $x_0\in U$ é definido
pelo limite (se existir):
$$f'(x_0)=\lim_{t\to0}{f(x_0+t)-f(x_0)\over t}\in\R^n.$$

Observe que $f'(x_0)$ é a mesma coisa que a derivada direcional
${\partial f\over\partial v}(x_0)$ com $v=1$ (parece estranho?
números reais também são vetores!).

\smallskip

Recorde que uma transformação linear $T:\R\to\R^n$ é sempre da
forma $T(h)=zh$, onde $z\in\R^n$ é um vetor (veja os exercícios).

\proclaim Teorema. Uma aplicação $f:U\subset\R\to\R^n$ é
diferenciável num ponto $x_0\in U$ se e somente se o vetor
tangente $f'(x_0)$ existe; as duas seguintes fórmulas relacionam a
diferencial $\dd f(x_0):\R\to\R^n$ com o vetor tangente
$f'(x_0)\in\R^n$:
$$f'(x_0)=\dd f(x_0)\cdot 1,\qquad\qquad\dd f(x_0)\cdot h=f'(x_0)h,\quad
h\in\R.$$

\Prova É igual a que fizemos quando $n=m=1$. A função $f$ é
diferenciável no ponto $x_0\in U$ se e somente se existe
$z\in\R^n$ tal que:
$$\lim_{h\to0}{f(x_0+h)-f(x_0)-zh\over\vert h\vert}=0;$$
como já fizemos várias vezes agora, trocamos ${1\over\vert
h\vert}$ por ${1\over h}{h\over\vert h\vert}$ e observamos que
${h\over\vert h\vert}=\pm1$ é limitado. Daí, $f$ é diferenciável
em $x_0$ se e somente se:
$$\lim_{h\to0}{f(x_0+h)-f(x_0)-zh\over
h}=\lim_{h\to0}{f(x_0+h)-f(x_0)\over h}-z=0.$$ Isso completa a
demonstração.\fimprova

\smallskip

\noindent{\tt Observação}: Quando $m=1$, flexibilizamos um pouco a
hipótese que $U$ seja um aberto; admitimos também que $U\subset\R$
seja um intervalo não necessariamente aberto. Para quem se sentir
incomodado com isso, observe que todas as demonstrações que
fizermos funcionam quando $m=1$ e $U$ é um intervalo qualquer.
Também no caso $U\subset\R^m$ ($m$ arbitrário) é possível
flexibilizar a hipótese que $U$ seja aberto na maioria dos
teoremas, mas é {\sl bem\/} mais difícil descrever quais são os
subconjuntos $U\subset\R^m$ ``admissíveis'' para o desenvolvimento
da teoria e por isso preferimos trabalhar só com abertos quando
$m\ne1$ (voltaremos um pouco a esse assunto quando discutirmos
variedades com bordo).

\smallskip

\noindent{\tt Observação}: Quando $m=1$ é mais comum pensar nas
coisas da seguinte maneira: em vez de escrever
$f:U\subset\R\to\R^n$ escrevemos $\gamma:I\to\R^n$, onde
$I\subset\R$ é um intervalo (tipicamente, $I=[a,b]$). Dizemos que
$\gamma$ é uma {\sl curva\/} em $\R^n$; os elementos de $I$ são
tipicamente denotados por letras como $t$ e $s$ e são chamados de
{\sl instantes\/} (em vez de pontos). É comum também dizer que $t$
é o {\sl parâmetro\/} da curva $\gamma$; dizemos aí que
$\gamma'(t)$ é o {\sl vetor tangente a $\gamma$ no instante $t$}.

\medskip

\item{(2)} {\bf Dois exemplos fáceis}.

\smallskip

\noindent{\tt Exemplo}. Se $f:\R^m\to\R^n$ é uma função constante
então $f$ é diferenciável e sua diferencial é zero em qualquer
ponto.

\smallskip

\noindent{\tt Exemplo}. Se $f:\R^m\to\R^n$ é uma transformação
linear então $f$ é diferenciável e $\dd f(x_0)=f$ para todo
$x_0\in\R^m$.

(afinal de contas, a melhor aproximação linear de uma
transformação linear é ela mesma!).

\smallskip

Em ambos os exemplos acima, quando escrevemos a definição de
diferenciabilidade vemos que o resto $r(h)$ é zero, de modo que
$\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$ é trivial.

\medskip

\item{(3)} {\bf Soma e produto de aplicações diferenciáveis}.

\smallskip

\proclaim Teorema. Se $f,g:U\subset\R^m\to\R^n$ são ambas
diferenciáveis em $x_0\in U$ então $f+g$ é diferenciável em $x_0$
e $\dd(f+g)(x_0)=\dd f(x_0)+\dd g(x_0)$.

Intuitivamente, se $\dd f(x_0)$ é uma boa aproximação linear para
$f$ perto de $x_0$ e $\dd g(x_0)$ é uma boa aproximação linear
para $g$ perto de $x_0$ então $\dd f(x_0)+\dd g(x_0)$ é uma boa
aproximação linear para $f+g$ perto de $x_0$. A prova segue
basicamente esse padrão:

\Prova Escrevemos:
$$\eqalign{%
f(x_0+h)&=f(x_0)+\dd f(x_0)\cdot h+r_1(h),\cr g(x_0+h)&=g(x_0)+\dd
g(x_0)\cdot h+r_2(h),\cr}$$ com $\lim_{h\to0}{r_1(h)\over\Vert
h\Vert}=\lim_{h\to0}{r_2(h)\over\Vert h\Vert}=0$. Somando as duas
igualdades obtemos:
$$(f+g)(x_0+h)=(f+g)(x_0)+\big[\dd f(x_0)+\dd g(x_0)\big]\cdot
h+r(h),$$ com $r=r_1+r_2$. Obviamente,
$\lim_{h\to0}{r(h)\over\Vert
h\Vert}=\lim_{h\to0}{r_1(h)+r_2(h)\over\Vert h\Vert}=0$ e $\dd
f(x_0)+\dd g(x_0)$ é uma transformação linear.\fimprova

\smallskip

\noindent{\tt Observação}: O seguinte truque (parece bobagem, mas
não é) facilita às vezes o trabalho com a definição de
diferenciabilidade: usamos
$$f(x_0+h)=f(x_0)+\dd f(x_0)\cdot h+\rho(h)\Vert h\Vert,$$
com $\lim_{h\to0}\rho(h)=0$, em vez da formulação dada
inicialmente com o resto $r$.

\smallskip

Como é de se esperar, a diferenciação de um produto dá um pouco
mais de trabalho. Nesse caso, só faz sentido considerar aplicações
a valores reais.

\proclaim Teorema. Se $f,g:U\subset\R^m\to\R$ são diferenciáveis
num ponto $x_0\in U$ então o produto $fg$ é diferenciável em $x_0$
e vale a fórmula:
$$\dd(fg)(x_0)\cdot h=\big[\dd f(x_0)\cdot h\big]g(x_0)+f(x_0)\big[\dd
g(x_0)\cdot h\big],$$ para todo $h\in\R^m$.

\Prova O lado direito da fórmula acima define de fato uma
transformação linear em $h$, o que já é um bom começo. Escrevemos:
$$\eqalign{%
f(x_0+h)&=f(x_0)+\dd f(x_0)\cdot h+\rho_1(h)\Vert h\Vert,\cr
g(x_0+h)&=g(x_0)+\dd g(x_0)\cdot h+\rho_2(h)\Vert h\Vert,\cr}$$
com $\lim_{h\to0}\rho_1(h)=\lim_{h\to0}\rho_2(h)=0$. Multiplicando
as duas igualdades acima obtemos:
$$(fg)(x_0+h)=(fg)(x_0)+\big[\dd f(x_0)\cdot h\big]g(x_0)+f(x_0)\big[\dd
g(x_0)\cdot h\big]+\rho(h)\Vert h\Vert,$$ onde $\rho$ é dado por:
$$\eqalign{\rho(h)&={\big[\dd f(x_0)\cdot h\big]\big[\dd g(x_0)\cdot
h\big]\over\Vert h\Vert}+g(x_0)\rho_1(h)+\big[\dd g(x_0)\cdot
h\big]\rho_1(h)\cr\noalign{\smallskip}&+f(x_0)\rho_2(h)+\big[\dd
f(x_0)\cdot h\big]\rho_2(h)+\rho_1(h)\rho_2(h)\Vert h\Vert.\cr}$$
Devemos mostrar que $\lim_{h\to0}\rho(h)=0$. Para isso, observe
que os seis termos aparecendo na expressão para $\rho$ são o
produto de alguma coisa que tende a zero por alguma coisa limitada
(quando $h$ está próximo de zero) --- alguns termos têm até mais
de um fator que vai para zero. Só o primeiro termo é um pouco mais
complicado: sua análise depende da observação que $\dd
f(x_0)\cdot\big({h\over\Vert h\Vert}\big)$ é limitado; isso segue
do fato que a aplicação contínua $\dd f(x_0):\R^m\to\R$ é limitada
na esfera unitária $\big\{v\in\R^m:\Vert v\Vert=1\big\}$ de
$\R^m$, já que a mesma é compacta (voltaremos a esse assunto na
revisão de topologia).\fimprova

\medskip

\item{(4)} {\bf Mais álgebra linear: aplicações bilineares e
multi-lineares}.

\smallskip

\proclaim Definição. Sejam $V_1$, $V_2$, $W$ espaços vetoriais.
Uma aplicação $B:V_1\times V_2\to W$ é chamada {\rm bilinear}
quando $B$ for linear em cada variável, i.e., para todos $v_1\in
V_1$, $v_2\in V_2$ as aplicações $B(v_1,\cdot):V_2\to W$ e
$B(\cdot,v_2):V_1\to W$ são lineares. De maneira mais explícita,
$B$ é bilinear quando valem as seguintes condições:\smallskip
\item{\rm(i)}$B(v_1+v'_1,v_2)=B(v_1,v_2)+B(v'_1,v_2);$
\item{\rm(ii)}$B(cv_1,v_2)=c\,B(v_1,v_2)$;
\item{\rm(iii)}$B(v_1,v_2+v'_2)=B(v_1,v_2)+B(v_1,v'_2)$;
\item{\rm(iv)}$B(v_1,cv_2)=c\,B(v_1,v_2)$;\hfil\break
para todos $v_1,v'_1\in V_1$, $v_2,v'_2\in V_2$ e $c\in\R$. Quando
$W=\R$ dizemos também que $B$ é uma {\rm forma bilinear}.

\smallskip

\noindent{\tt Observação}: Acustumem-se com a notação $\cdot$
porque ela é muito prática. Por exemplo, na definição acima,
tínhamos uma aplicação $B$ de duas variáveis; fixando uma das
variáveis, sobra uma aplicação de uma única variável. Essa
aplicação obtida de $B$ fixando uma variável, é denotada colocando
um $\cdot$ na variável que ficou livre. Assim, por exemplo, se
$B:V_1\times V_2\to W$ e $v_1\in V_1$ então $B(v_1,\cdot)$ denota
a aplicação de $V_2$ em $W$ que leva $v_2$ em $B(v_1,v_2)$.

\smallskip

\noindent{\tt Exemplo}: A {\sl multiplicação de números reais\/} é
um operador (e na verdade uma forma) bilinear, i.e., a aplicação:
$$m:\R\times\R\longrightarrow\R$$
dada por $m(x,y)=xy$ é bilinear.

\smallskip

\noindent{\tt Exemplo}: O {\sl produto interno canônico\/} de
$\R^n$ definido por $\langle x,y\rangle=\sum_{i=1}^nx_iy_i$ é uma
aplicação bilinear $\langle\cdot,\cdot\rangle:\R^n\times\R^n\to\R$
(e também uma forma bilinear).

\smallskip

\noindent{\tt Exemplo}: O {\sl produto vetorial\/} em $\R^3$,
i.e., a aplicação $\R^3\times\R^3\ni(x,y)\mapsto x\times y\in\R^3$
é bilinear.

Em vista dos exemplos acima, observe que uma aplicação bilinear
$(x,y)\mapsto B(x,y)$ pode ser visualizada como uma ``operação
binária que satisfaz a propriedade distributiva'' (na verdade, as
propriedades (i) e (iii) são a propriedade distributiva, (ii) e
(iv) dizem um pouco a mais).

\smallskip

\noindent{\tt Exemplo}: Se $\Matr nm(\R)$ denota o espaço vetorial
de todas as matrizes reais $n\times m$ então a {\sl multiplicação
de matrizes}:
$$\Matr nm(\R)\times\Matr mp(\R)\ni(A,B)\longmapsto AB\in\Matr
np(\R)$$ é uma aplicação bilinear.

\smallskip

\noindent{\tt Exemplo}: Se $V$, $V'$ e $V''$ são espaços vetoriais
então a aplicação de {\sl composição de transformações lineares}:
$$\Lin(V',V'')\times\Lin(V,V')\ni(T,S)\longmapsto T\circ
S\in\Lin(V,V'')$$ é bilinear. Se $V=\R^p$, $V'=\R^m$, $V''=\R^n$
então este exemplo é essencialmente o mesmo que o anterior,
identificando operadores lineares e matrizes através das bases
canônicas.

\smallskip

\noindent{\tt Não-Exemplo}: A aplicação {\sl soma de vetores\/}
$s:\R^n\times\R^n\to\R^n$ definida por:
$$s(x,y)=x+y,\quad x,y\in\R^n,$$
{\bf não} é bilinear. Na verdade,
$s$ é {\sl linear\/} quando identificamos $\R^n\times\R^n$ com
$\R^{2n}$!

\smallskip

É bastante natural generalizar o conceito de bilinearidade para
aplicações de mais de duas variáveis: obtemos então as noções de
trilinearidade, quadrilinearidade, \dots, ou em geral
$n$-linearidade: todas essas noções são denominadas por {\sl
multi-linearidade}, quando não se quer especificar o número de
variáveis na sentença em questão.

\proclaim Definição. Sejam $V_1$, $V_2$, \dots, $V_n$, $W$ espaços
vetoriais e seja $B:V_1\times V_2\times\cdots\times V_n\to W$ uma
aplicação. Dizemos que $B$ é {\rm multi-linear} (ou {\rm
$n$-linear}) quando $B$ for linear em cada variável; mais
explicitamente, $B$ é multi-linear quando dado $i=1,\ldots,n$ e
dados vetores $v_j\in V_j$, $j=1,\ldots,i-1,i+1,\ldots,n$ então a
aplicação
$$B(v_1,\ldots,v_{i-1},\cdot,v_{i+1},\ldots,v_n):V_i\longrightarrow W$$
é linear. Quando $W=\R$ dizemos também que $B$ é uma {\rm forma
multi-linear}.

\smallskip

\noindent{\tt Exemplo}: A aplicação:
$$\det:\underbrace{\R^n\times\R^n\times\cdots\times\R^n}_{n\rm\
fatores}\longrightarrow\R$$ que associa a cada $n$-upla
$(v_1,\ldots,v_n)\in(\R^n)^n$ o determinante da matriz $n\times n$
que possui os vetores $v_i$ em suas colunas (ou linhas, tanto faz
aqui) é multi-linear; $\det$ é portanto uma forma $n$-linear em
$\R^n$.

\smallskip

Recorde que um operador linear $T:V\to W$ fica univocamente
determinado quando especificamos seu valor sobre os vetores de uma
base de $V$: de maneira mais explícita, se ${\frak
B}=(b_1,\ldots,b_m)$ é uma base de $V$ então dados vetores
$w_1,\ldots,w_m\in W$, existe um único operador linear $T:V\to W$
tal que $T(b_i)=w_i$, $i=1,\ldots,m$. Como se calcula $T$ num
vetor arbitrário $v\in V$? Escreva $v=\sum_{i=1}^m\alpha_ib_i$ e
daí:
$$T(v)=\sum_{i=1}^m\alpha_iT(b_i)=\sum_{i=1}^m\alpha_iw_i;$$
quem trabalhou com os exercícios de álgebra linear da aula
anterior (ou quem já tinha uma formação básica de álgebra linear)
não vai ter problemas com as afirmações acima!

Quando escolhemos também uma base ${\frak B}'$ em $W$ então os
vetores $T(b_i)\in W$ podem ser descritos através de suas
coordenadas na base ${\frak B}'$ e daí obtemos o fato básico que o
operador linear $T$ pode ser descrito completamente através da
matriz $[T]_{{\frak B}{\frak B}'}$.

Quando trabalhamos com operadores bilineares e multi-lineares em
geral, a situação não é muito diferente (a diferença é que a
notação fica mais feia e começam a aparecer um monte de índices).
Veja os exercícios!

\smallskip

\noindent{\tt Observação}: Aplicações multi-lineares não serão
muito importantes agora. Voltaremos a esse assunto quando
estudarmos {\sl formas diferenciais\/} na parte final do curso.

\medskip

\item{(5)} {\bf Diferenciando aplicações bilineares e
multi-lineares}.

\smallskip

Vamos assumir agora que aplicações multi-lineares
$B:\R^{m_1}\times\cdots\times\R^{m_r}\to\R^n$ são contínuas (veja
os exercícios).

\proclaim Teorema. Seja $B:\R^m\times\R^n\to\R^p$ uma aplicação
bilinear. Então $B$ é diferenciável e sua diferencial é dada por:
$$\dd B(x,y)\cdot (h,k)=B(h,y)+B(x,k),$$
para todos $(x,y)\in\R^m\times\R^n$ e $(h,k)\in\R^m\times\R^n$.

\Prova Observe primeiro que a expressão acima define uma aplicação
linear com respeito a $(h,k)$. Pela bilinearidade de $B$ temos:
$$B(x+h,y+k)=B(x,y)+B(h,y)+B(x,k)+r(h,k),$$
onde $r(h,k)=B(h,k)$. Devemos mostrar que:
$$\lim_{(h,k)\to0}{B(h,k)\over\Vert(h,k)\Vert}=0,$$
onde $\Vert(h,k)\Vert=\sqrt{\Vert h\Vert^2+\Vert k\Vert^2}$.
Temos:
$${B(h,k)\over\Vert(h,k)\Vert}=B\left({h\over\Vert h\Vert},{k\over\Vert
k\Vert}\right)\cdot{\Vert h\Vert\over\Vert(h,k)\Vert}\cdot\Vert
k\Vert;$$ o terceiro fator à direita da igualdade acima tende a
zero e o segundo é limitado (menor ou igual a $1$). O primeiro
também é limitado, pois $B$ é contínua e o conjunto
$$\big\{(v,w)\in\R^m\times\R^n:\Vert v\Vert=\Vert w\Vert=1\big\}$$
é compacto (retomaremos esse assunto na revisão de topologia).

Segue então que
$\lim_{(h,k)\to0}{B(h,k)\over\Vert(h,k)\Vert}=\lim_{(h,k)\to0}{r(h,k)\over\Vert(h,k)\Vert}=0$.\fimprova

\smallskip

\noindent{\tt Observação}: Com um pouco de paciência é possível
demonstrar (seguindo as idéias da demonstração anterior) que toda
aplicação multi-linear
$B:\R^{m_1}\times\cdots\times\R^{m_r}\to\R^n$ é diferenciável e
que sua diferencial é dada por:
$$\dd
B(x_1,\ldots,x_r)\cdot(h_1,\ldots,h_r)=\sum_{i=1}^rB(x_1,\ldots,x_{i-1},h_i,x_{i+1},\ldots,x_r),$$
para todos $x_i\in\R^{m_i}$, $h_i\in\R^{m_i}$, $i=1,\ldots,r$.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Álgebra Linear}.

\medskip

\item{1.}Seja $V$ um espaço vetorial real. Mostre que todas as
transformações lineares $$T:\R\to V$$
são da forma $T(h)=zh$ para
algum vetor $z\in V$ ({\sl dica}: $T(h)=T(h\cdot1)$). Conclua que
$\Lin(\R,V)$ é isomorfo a $V$; mais precisamente, mostre que a
aplicação:
$$\Lin(\R,V)\ni T\longmapsto T(1)\in V$$
é um isomorfismo.

\smallskip

\item{2.} Sejam $V$, $V'$, $V''$ espaços vetoriais e sejam
${\frak B}=(b_1,\ldots,b_n)$, ${\frak B}'=(b'_1,\ldots,b'_m)$ e
${\frak B}''=(b''_1,\ldots,b''_p)$ bases de $V$, $V'$ e $V''$
respectivamente. Mostre que:
\smallskip
\itemitem{(a)}Se $B:V\times V'\to V''$ é uma aplicação bilinear
então para todos $v=\sum_{i=1}^n\alpha_ib_i\in V$,
$v'=\sum_{j=1}^m\alpha'_jb'_j\in V'$ vale a identidade:
$$B(v,v')=\sum_{i=1}^n\sum_{j=1}^m\alpha_i\alpha'_jB(b_i,b'_j).$$

\itemitem{(b)}Mostre que uma aplicação bilinear $V\times V'\to
V''$ fica univocamente determinada por seus valores em vetores de
uma base de $V$ e de uma base de $V'$, i.e., mostre que dados
vetores $v''_{ij}\in V''$, $i=1,\ldots,n$, $j=1,\ldots,m$ então
existe uma única aplicação bilinear $B:V\times V'\to V''$ tal que
$B(b_i,b'_j)=v''_{ij}$ para todos $i=1,\ldots,n$ e $j=1,\ldots,m$.

\itemitem{(c)}Se $V=\R^n$, $V'=\R^m$ e $V''=\R^p$, conclua do item
(a) que toda aplicação bilinear $B:V\times V'\to V''$ é contínua
({\sl dica}: somas e produtos de aplicações contínuas são
contínuas).

\itemitem{(d)}Dada uma aplicação bilinear $B:V\times V'\to V''$,
denote por $B_{ij}^k$ a $k$-ésima coordenada na base ${\frak B}''$
do vetor $B(b_i,b'_j)\in V''$. Obtemos então uma ``matriz
tri-dimensional'' $(B_{ij}^k)_{n\times m\times p}$. Conclua do
item (b) que existe uma correspondência biunívoca entre operadores
bilineares $B:V\times V'\to V''$ e matrizes tri-dimensionais
$n\times m\times p$ de números reais. Dizemos que a matriz
tri-dimensional $(B_{ij}^k)_{n\times m\times p}$ {\sl
representa\/} a aplicação bilinear $B$ com respeito às bases
${\frak B}$, ${\frak B}'$ e ${\frak B}''$.

\smallskip

\noindent{\tt Observação}: Quando $V''=\R$, i.e., no caso de
formas bilineares $B:V\times V'\to\R$ então é possível omitir o
índice $k$ nas matrizes tri-dimensionais do item (d), de modo que
formas bilineares $B:V\times V'\to\R$ são representadas por
matrizes comuns $(B_{ij})_{n\times m}$.

\smallskip

\itemitem{(e)} Generalize (se você tiver muita paciência com
notação chata, somatórias e ín\-di\-ces) os itens anteriores para
o caso de aplicações multi-lineares.

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{3.} Uma {\sl transformação afim\/} $A:\R^m\to\R^n$ é uma
aplicação da forma $A(x)=T(x)+v$, com $T:\R^m\to\R^n$ uma
aplicação linear e $v\in\R^n$ um vetor fixado. Mostre que $\dd
A(x)=T$ para todo $x\in\R^m$. Conclua que se $A$ é uma {\sl
translação\/} (i.e., se $m=n$ e $T$ é a identidade) então $\dd
A(x)=\Id$, para todo $x\in\R^m$.

\vfil\eject

\centerline{\bf Aula número $4$ (15/03)}
\bigskip
\bigskip

Enunciamos agora a {\bf regra da cadeia}:

\proclaim Teorema. Sejam $f:U\subset\R^m\to\R^n$ e
$g:V\subset\R^n\to\R^p$ aplicações com $U\subset\R^m$,
$V\subset\R^n$ abertos e $f(U)\subset V$. Se $f$ é diferenciável
num ponto $x_0\in U$ e $g$ é diferenciável no ponto $f(x_0)\in V$
então a composta $g\circ f:U\to\R^p$ é diferenciável no ponto
$x_0\in U$ e sua diferencial é dada por:
$$\dd(g\circ f)(x_0)=\dd g\big(f(x_0)\big)\circ\dd f(x_0).$$

Numa frase: ``a composta de aplicações diferenciáveis é
diferenciável e a diferencial da composta é a composta das
diferenciais''.

A regra da cadeia pode ser pensada de modo intuitivo da seguinte
forma: se $\dd f(x_0)$ é uma boa aproximação linear para $f$ perto
de $x_0$ e $\dd g\big(f(x_0)\big)$ é uma boa aproximação linear
para $g$ perto de $f(x_0)$ então a composta $\dd
g\big(f(x_0)\big)\circ f(x_0)$ é uma boa aproximação linear para a
composta $g\circ f$ perto de $x_0$.

A demonstração da regra da cadeia, embora seja simples, será
deixada para depois da revisão de topologia. O objetivo dessa aula
é compreender vários aspectos do enunciado da regra da cadeia.

\smallskip

\noindent{\tt Observação}: Nesse ponto já temos várias ferramentas
para mostrar que uma aplicação é diferenciável. A saber:
\item{(i)}aplicações constantes, lineares e multi-lineares são
diferenciáveis;

\item{(ii)}a soma e o produto de aplicações diferenciáveis é
diferenciável (no caso do produto, só podemos considerar
aplicações a valores reais);

\item{(iii)}quando o domínio é unidimensional, diferenciabilidade
equivale à derivabilidade do Cálculo I; em particular, aplicações
familiares como seno, cosseno, exponencial, etc, etc, etc, são
diferenciáveis;

\item{(iv)}uma aplicação a valores em $\R^n$ é diferenciável se e
somente se cada uma de suas $n$ coordenadas é diferenciável;

\item{(v)}a composta de aplicações diferenciáveis é diferenciável.

Obtemos em particular que aplicações definidas por fórmulas são
diferenciáveis (com exceção das que envolvem raízes de expressões
que podem se anular, já que $\sqrt{\cdot}$ não é derivável na
origem).

\medskip

\item{(1)} {\bf Interpretando a regra da cadeia em termos de matrizes
Jacobianas: a regra da cadeia clássica}.

\smallskip

Como vimos nos exercícios da aula número $2$, a matriz que
representa a composta de duas transformações lineares é obtida
fazendo o produto das matrizes que representam cada transformação
linear envolvida. Em termos de matrizes, a regra da cadeia nos diz
então que:
$$\Jac(g\circ f)(x_0)=\Jac g\big(f(x_0)\big)\Jac f(x_0);$$
expandindo o produto de matrizes obtemos:
$${\partial(g\circ f)^i\over\partial x_j}(x_0)=\sum_{k=1}^n{\partial
g^i\over\partial x_k}\big(f(x_0)\big){\partial f^k\over\partial
x_j}(x_0).$$ Alternativamente, aplicando os dois lados da
igualdade $\dd(g\circ f)(x_0)=\dd g\big(f(x_0)\big)\circ\dd
f(x_0)$ no $j$-ésimo vetor $e_j$ da base canônica de $\R^m$
obtemos:
$$\eqalign{{\partial(g\circ f)\over\partial x_j}(x_0)&=\dd
g\big(f(x_0)\big)\cdot{\partial f\over\partial x_j}(x_0)=\dd
g\big(f(x_0)\big)\cdot\left(\sum_{k=1}^n{\partial f^k\over\partial
x_j}(x_0)\,e_k\right)\cr &=\sum_{k=1}^n\Big[\dd
g\big(f(x_0)\big)\cdot e_k\Big]{\partial f^k\over\partial
x_j}(x_0)=\sum_{k=1}^n{\partial g\over\partial
x_k}\big(f(x_0)\big){\partial f^k\over\partial x_j}(x_0),\cr}$$ ou
seja:
$${\partial(g\circ f)\over\partial x_j}(x_0)=\sum_{k=1}^n{\partial
g\over\partial x_k}\big(f(x_0)\big){\partial f^k\over\partial
x_j}(x_0).$$

É muito comum denotar pontos de ``ambientes diferentes'' por
letras diferentes; por exemplo, os pontos do domínio de $f$ podem
ser denotados com $x$ e os pontos do domínio de $g$ com $y$ (não
por motivos matemáticos, apenas porque muitas vezes facilita a
leitura). Daí, intuitivamente pensa-se que $f$ é ``função de $x$''
e $g$ é ``função de $y$'' (embora isso não queira dizer nada,
formalmente) e reescreve-se a regra da cadeia sob a forma:
$${\partial(g\circ f)\over\partial x_j}(x_0)=\sum_{k=1}^n{\partial
g\over\partial y_k}\big(f(x_0)\big){\partial f^k\over\partial
x_j}(x_0).$$ Muitas vezes também os matemáticos resolvem aderir
àquela notação (às vezes, confusa, para dizer a verdade) que é
encontrada principalmente em textos de física, onde não se fala
explicitamente em funções e composição de funções, mas apenas em
``variáveis dependendo umas das outras'' e as composições são
todas subentendidas (essa notação também tem seus méritos).
Pensando então em ``variáveis'' $x\in U\subset\R^m$, $y\in
V\subset\R^n$ e $z\in\R^p$, com $y$ dependendo de $x$ (i.e., temos
a função $f$), $z$ dependendo de $y$ (i.e., temos a função $g$) e
daí $z$ também depende de $x$ (i.e., subentende-se a composição
$g\circ f$) então a regra da cadeia fica:
$${\partial z\over\partial x_j}=\sum_{k=1}^n{\partial
z\over\partial y_k}\,{\partial y_k\over\partial x_j},$$ onde
também os pontos onde as derivadas são calculadas são omitidos (e
devem ser ``adivinhados'' pelo contexto).

\medskip

\item{(2)} {\bf A visão funtorial da regra da cadeia}.

\smallskip

Algumas vezes em matemática precisamos descrever igualdades entre
composições de certas funções que seriam difíceis de serem
escritas e de serem visualizadas usando a ``notação bola'' padrão
($g\circ f$). Em vez disso, utiliza-se um recurso gráfico mais
eficiente, conhecido como {\sl diagrama comutativo}. Abaixo temos
um exemplo:

\smallskip

$$\xymatrix@R-15pt{%
\R^m\ar[r]^f\ar@/^2.5pc/[rrd]|{\;\rho\;}\ar[dd]_g&\R^n\ar[dr]^h\\
&&\R^k\\
\R^p\ar[r]_{{\cal
F}}\ar[uur]|*+<2pt>{\hbox{$\scriptstyle\sigma$}}&\R^m\ar[ur]_\alpha}$$

\smallskip

O diagrama acima codifica as igualdades $\sigma\circ g=f$,
$\alpha\circ{\cal F}=h\circ\sigma$ e $\rho=h\circ f$. A vantagem
da formulação da regra da cadeia na forma ``a diferencial da
composta é a composta das diferenciais'' é que podemos diferenciar
todas as flechas que aparecem num diagrama comutativo obtendo um
novo diagrama comutativo. No nosso exemplo:
\smallskip

$$\xymatrix@R-15pt{%
\R^m\ar[r]^{\dd f}\ar@/^2.5pc/[rrd]|{\;\dd\rho\;}\ar[dd]_{\dd g}&\R^n\ar[dr]^{\dd h}\\
&&\R^k\\
\R^p\ar[r]_{\dd{\cal
F}}\ar[uur]|*+<2pt>{\hbox{$\scriptstyle\dd\sigma$}}&\R^m\ar[ur]_{\dd\alpha}}$$
no diagrama acima omitimos os pontos onde as diferenciais são
calculadas para simplificar a exposição, mas não é difícil
adivinhar quais são tais pontos: por exemplo, se $f$ é
diferenciada no ponto $x_0\in\R^m$ então $h$ é diferenciada no
ponto $f(x_0)$ (complete o resto!).

\smallskip

\noindent{\tt Observação}: O nome ``funtorial'' usado nessa seção
vem da {\sl teoria das categorias}: um {\sl funtor\/} é
essencialmente uma regra que pode ser aplicada em um diagrama
comutativo produzindo um novo diagrama comutativo (mas esse
assunto não tem nada a ver com o nosso curso).

\medskip

\item{(3)} {\bf Compondo funções e curvas}.

\smallskip

Seja $\gamma:I\subset\R\to\R^m$ uma curva (onde $I\subset\R$ é um
intervalo) e $f:U\subset\R^m\to\R^n$ uma função definida num
aberto $U\subset\R^m$ contendo a imagem de $\gamma$. Suponha que
$t\in I$ é tal que $\gamma$ é diferenciável em $t$ e $f$ é
diferenciável em $\gamma(t)$ (isso acontece, por exemplo, se
$\gamma$ e $f$ forem diferenciáveis em todo seu domínio). A
aplicação composta $f\circ\gamma:I\subset\R\to\R^n$ é uma curva em
$\R^n$ que é diferenciável em $t$ pela regra da cadeia; a
diferencial de $f\circ\gamma$ no instante $t$ é dada por:
$$\dd(f\circ\gamma)(t)=\dd
f\big(\gamma(t)\big)\circ\dd\gamma(t).$$ Recorde que o vetor
tangente a uma curva é obtido aplicando sua diferencial ao vetor
$1\in\R$ (vide início da aula número $3$); aplicando os dois lados
da igualdade acima a $1\in\R$ obtemos:
$$(f\circ\gamma)'(t)=\dd f\big(\gamma(t)\big)\cdot\gamma'(t).$$

\smallskip

\noindent{\tt Exemplo}: Se $T:\R^m\to\R^n$ é uma transformação
linear (por exemplo, imagine o caso que $m=n=3$ e $T$ é uma
rotação) e se $\gamma:I\subset\R\to\R^m$ é uma curva diferenciável
no instante $t$ então:
$$(T\circ\gamma)'(t)=T\big(\gamma'(t)\big).$$
(quando $T$ é uma rotação a igualdade acima exprime o fato que ``o
vetor tangente de uma rotação de $\gamma$ é a rotação do vetor
tangente de $\gamma$''). Se $T:\R^n\to\R^n$ é uma {\sl
translação}, i.e., se $T(x)=x+v$ para algum $v\in\R^n$ então $\dd
T(x)=\Id$ para todo $x\in\R^n$ e:
$$(T\circ\gamma)'(t)=\gamma'(t),$$
ou seja, transladando uma curva não alteramos seu vetor tangente
(veja os exercícios da aula número 3).

\smallskip

\noindent{\tt Exemplo}: Seja $B:\R^m\times\R^n\to\R^p$ uma
aplicação bilinear e sejam $\gamma:I\subset\R\to\R^m$,
$\mu:I\subset\R\to\R^n$ duas curvas diferenciáveis num instante
$t\in I$. Temos então que a curva
$(\gamma,\mu):I\to\R^m\times\R^n$ é diferenciável em $t$ e
$(\gamma,\mu)'(t)=\big(\gamma'(t),\mu'(t)\big)$ (veja os
exercícios da aula número 2). Segue da regra da cadeia que a curva
$B\circ(\gamma,\mu):I\to\R^p$ é diferenciável em $t$ e que seu
vetor tangente no instante $t$ é dado por:
$${\dd\over\dd t}B\big(\gamma(t),\mu(t)\big)=\dd
B\big(\gamma(t),\mu(t)\big)\cdot\big(\gamma'(t),\mu'(t)\big);$$
usando a fórmula deduzida na aula número $3$ para a diferencial de
aplicações bilineares obtemos:
$${\dd\over\dd
t}B\big(\gamma(t),\mu(t)\big)=B\big(\gamma'(t),\mu(t)\big)+B\big(\gamma(t),\mu'(t)\big).$$
Interprete a fórmula acima quando substituímos $B$ por cada uma
das aplicações bilineares discutidas nos exemplos da aula número
$3$ (veja também os exercícios).

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{(1.)} Rededuza a fórmula para a diferenciação de um produto
de aplicações:
$$f,g:U\subset\R^m\longrightarrow\R$$
usando a regra da cadeia como no último exemplo da aula.
Generalize o resultado para obter a diferencial do produto interno
$U\ni x\mapsto\langle f(x),g(x)\rangle\in\R$ de aplicações
$f,g:U\subset\R^m\to\R^n$.

\vfil\eject

\centerline {\bf Complemento à Aula número $4$ (15/03)}
\bigskip
\bigskip

Faremos um exemplo adicional de aplicação da regra da cadeia. Considere o
espaço vetorial real $\Matr nn(\R)$ de todas as matrizes reais $n\times n$
(que é isomorfo a $\R^{n^2}$). O conjunto
$$\GL(n,\R)=\big\{A\in\Matr nn(\R):A\ \hbox{é inversível}\big\}$$
formado por todas as matrizes inversíveis é aberto em $\Matr nn(\R)$; isso
segue do fato que $A\in\GL(n,\R)\Leftrightarrow\det(A)\ne0$ e do fato que a
função determinante $\det$ é contínua (esse argumento será revisto na revisão
de topologia).

\smallskip

\noindent{\tt Observação}: A notação $\GL(n,\R)$ vem do fato que o conjunto
das matrizes reais inversíveis $n\times n$ é conhecido como o {\sl grupo
linear geral de $\R^n$}.

\smallskip

A {\sl função inversão\/} ${\cal I}:\GL(n,\R)\to\Matr nn(\R)$
definida por ${\cal I}(A)=A^{-1}$ é diferenciável. De fato, cada entrada de
$A^{-1}$ é uma função racional das entradas de $A$ de modo que cada coordenada
de ${\cal I}$ é um quociente de uma soma de produtos das funções coordenadas
de $\Matr nn(\R)$. O nosso objetivo é calcular a diferencial de ${\cal I}$.

Seja ${\cal C}:\Matr nn(\R)\times\Matr nn(\R)\to\Matr nn(\R)$ a {\sl função
multiplicação de matrizes\/} definida por ${\cal C}(A,B)=AB$. Sabemos que
${\cal C}$ é bilinear e portanto diferenciável. Temos:
$${\cal C}\big(A,{\cal I}(A)\big)={\rm I},$$
para toda matriz $A\in\GL(n,\R)$, onde ${\rm I}$ denota a matriz identidade
$n\times n$. Denote por $i:\GL(n,\R)\to\Matr nn(\R)$ a {\sl função inclusão\/}
(i.e., $i(A)=A$) e por
$$(i,{\cal I}):\GL(n,\R)\longrightarrow\Matr nn(\R)\times\Matr nn(\R)$$
a função $A\mapsto(A,A^{-1})$. Obviamente $i$ é diferenciável e $\dd
i(A)$ é a identidade para todo $A\in\GL(n,\R)$ (já que $i$ é a restrição da
identidade de $\Matr nn(\R)$ a um aberto -- veja os exercícios). A igualdade
${\cal C}\big(A,{\cal I}(A)\big)={\rm I}$ nos diz que a função composta ${\cal
C}\circ(i,{\cal I})$
é constante (e igual a ${\rm I}$), donde:
$$\dd\big({\cal C}\circ(i,{\cal I})\big)(A)=0,\quad A\in\GL(n,\R).$$
Usando a regra da cadeia obtemos para todos $A\in\GL(n,\R)$, $H\in\Matr
nn(\R)$:
$$\dd{\cal C}(A,A^{-1})\cdot\big(H,\dd{\cal I}(A)\cdot H\big)=0;$$
como ${\cal C}$ é bilinear obtemos:
$${\cal C}\big(A,\dd{\cal I}(A)\cdot H\big)+{\cal C}(H,A^{-1})=A\,\dd{\cal
I}(A)\cdot H+HA^{-1}=0$$
e portanto:
$$\dd{\cal I}(A)\cdot H=-A^{-1}HA^{-1}.$$

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver problemas me procura).

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{1.} Mostre que diferenciabilidade é uma {\sl propriedade local}, ou seja,
dados $U,V\subset\R^m$ abertos e $x\in\R^m$ com $x\in V\subset U$ então uma
função $f:U\to\R^n$ é diferenciável em $x$ se e somente se sua restrição
$f\vert_V$ é diferenciável em $x$; nesse caso as diferenciais são iguais,
i.e., $\dd(f\vert_V)(x)=\dd f(x)$.

\smallskip

\item{2.}Usando a fórmula deduzida para a diferencial da função ${\cal I}$,
mostre que se $t\mapsto A(t)$ é uma curva em $\Matr nn(\R)$ diferenciável em
$t=t_0$ e tal que $A(t)$ é inversível para todo $t$ então a curva $t\mapsto
A(t)^{-1}$ é diferenciável em $t=t_0$ e seu vetor tangente é dado por:
$$\left.{\dd\over \dd
t}A(t)^{-1}\right\vert_{t=t_0}=-A(t_0)^{-1}A'(t_0)A(t_0)^{-1}.$$

\vfil\eject

\centerline{\bf Aula número $5$ (20/03)}
\bigskip
\bigskip

\proclaim Definição. Seja $M$ um conjunto. Uma {\rm métrica} em
$M$ é uma função $d:M\times M\to\R$ satisfazendo as seguintes
propriedades:
\item{(EM1)} $d(x,y)\ge0$ para todos $x,y\in M$ e $d(x,y)=0$ se e somente se $x=y$;
\item{(EM2)} $d(x,y)=d(y,x)$ para todos $x,y\in M$ {\rm
(simetria)};
\item{(EM3)} $d(x,z)\le d(x,y)+d(y,z)$ para todos $x,y,z\in M$
{\rm(desigualdade triangular)}.\smallskip\noindent Um {\rm espaço
métrico} é um par $(M,d)$ onde $M$ é um conjunto e $d$ é uma
métrica em $M$.

\smallskip

\noindent{\tt Exemplo}. A aplicação $d:\R^n\times\R^n\to\R$ dada
por $d(x,y)=\Vert
x-y\Vert=\left(\sum_{i=1}^n(x_i-y_i)^2\right)^{1\over2}$ é uma
métrica em $\R^n$. As propriedades (EM1) e (EM2) são triviais;
quanto à desigualdade triangular (embora provavelmente vocês todos
já conheçam) será demonstrada mais adiante quando discutirmos
produtos internos e normas. A métrica $d$ é conhecida como a {\sl
métrica Euclideana}.

\smallskip

\noindent{\tt Exemplo}. Se $(M,d)$ é um espaço métrico e $S\subset
M$ é um subconjunto de $M$ então $d\vert_{S\times S}$ é uma
métrica em $S$ (verifique!) e portanto $\big(S,d\vert_{S\times
S}\big)$ é um espaço métrico; dizemos que $\big(S,d\vert_{S\times
S}\big)$ é um {\sl subespaço métrico\/} de $(M,d)$ ou que
$d\vert_{S\times S}$ é a métrica em $S$ {\sl induzida\/} por $d$.
Observe em particular que se $S$ é um subconjunto de $\R^n$ e $d$
é a métrica Euclideana então $\big(S,d\vert_{S\times S}\big)$ é um
espaço métrico.

\smallskip

Os dois exemplos acima são essencialmente os únicos que interessam
nesse curso; uma pequena exceção: às vezes precisaremos também
considerar o $\R^n$ (e, mais geralmente, espaços vetoriais reais
de dimensão finita) munidos de métricas induzidas por normas
diferentes da norma Euclideana (isso será discutido mais adiante).
Veremos, no entanto, que todas as normas num espaço vetorial real
de dimensão finita são {\sl equivalentes\/} num sentido que será
esclarecido depois, de modo que basicamente os dois exemplos acima
são de fato os que interessam.

\smallskip

O espaço $\R^n$ é o produto de $n$ cópias de $\R$. Mais
geralmente, existe uma maneira mais ou menos canônica de tornar o
produto de espaços métricos um espaço métrico. Decidimos
introduzir essa construção mais geral abaixo:

\proclaim Definição. Sejam $(M_i,d_i)$, $i=1,\ldots,n$, espaços
métricos. Definimos no produto cartesiano $M=M_1\times\cdots\times
M_n$ uma métrica $d$ fazendo:
$$d\big((x_1,\ldots,x_n),(y_1,\ldots,y_n)\big)=\left(\sum_{i=1}^nd_i(x_i,y_i)^2\right)^{1\over2},$$
para todos $x_i,y_i\in M_i$, $i=1,\ldots,n$. A métrica $d$ é
chamada a {\rm métrica produto} em $M$.

As propriedades (EM1) e (EM2) para $d$ são imediatas. A
desigualdade triangular pode ser mostrada facilmente usando a
desigualdade triangular para a métrica Euclideana de $\R^n$ (veja
Exercício 18). É possível também definir métricas
computacionalmente mais simples no produto $M=\prod_{i=1}^nM_i$;
tais métricas são {\sl equivalentes\/} num sentido que será
esclarecido depois. No momento, usaremos a métrica produto
introduzida acima, de modo que a métrica Euclideana de $\R^n$ é
obtida fazendo o produto de $n$ cópias de $\R$.

\smallskip

Mesmo não sendo importante para esse curso, vamos mostrar dois
exemplos de espaços métricos essencialmente diferentes de $\R^n$,
só para cultura geral.

\smallskip

\noindent{\tt Exemplo}. Se $M$ é um conjunto qualquer então a
aplicação $d:M\times M\to\R$ definida por $d(x,y)=0$ para $x=y$ e
$d(x,y)=1$ para $x\ne y$ é uma métrica em $M$ (verifique!). A
métrica $d$ é conhecida como {\sl a métrica zero-um}.

\smallskip

\noindent{\tt Exemplo}. Seja $X$ um conjunto e seja $M={\cal
B}(X,\R)$ o conjunto de todas as funções limitadas $f:X\to\R$ (uma
função é {\sl limitada\/} se existe $k\in\R$ com $\vert
f(x)\vert\le k$ para todo $x\in X$). Definimos uma métrica $d$ em
$M$ fazendo:
$$d(f,g)=\sup_{x\in X}\big\vert f(x)-g(x)\big\vert,$$
para todas $f,g\in M$. A verificação das propriedades (EM1)--(EM3)
não é difícil (para quem sabe trabalhar com $\sup$).

\medskip

\item{(1)} {\bf Terminologia Básica da Teoria dos Espaços
Métricos}.

\smallskip

Seja $(M,d)$ um espaço métrico (às vezes a gente cansa de dizer
$(M,d)$ e diz só $M$, quando $d$ está subentendido; mas cuidado,
num mesmo conjunto $M$ podemos definir muitas métricas). Vamos
listar várias definições.

\smallskip

\proclaim Definição. Dado $x\in M$ e $r>0$ então a {\rm bola
aberta} de centro $x$ e raio $r$ é definida por:
$$\Bola(x;r)=\big\{y\in M:d(x,y)<r\big\}$$
e a {\rm bola fechada} de centro $x$ e raio $r$ é definida por:
$$\Bola[x;r]=\big\{y\in M:d(x,y)\le r\big\}.$$

\smallskip

\proclaim Definição. Sejam $A\subset M$ um subconjunto e $p\in M$
um ponto. Dizemos que:
\item{(i)} $p$ é um {\rm ponto interior} de $A$ se existe $r>0$
com $\Bola(p;r)\subset A$;
\item{(ii)} $p$ é um {\rm ponto exterior} de $A$ se existe $r>0$
com $\Bola(p;r)\subset A^\compl=M\setminus A$, i.e., se existe
$r>0$ com $\Bola(p;r)\cap A=\emptyset$;
\item{(iii)} $p$ é um {\rm ponto de fronteira} de $A$ se $p$ não é
um ponto interior nem um ponto exterior de $A$, i.e., se para todo
$r>0$ temos $\Bola(p;r)\cap A\ne\emptyset$ e $\Bola(p;r)\cap
A^\compl\ne\emptyset$;
\item{(iv)} $p$ é um {\rm ponto de aderência} de $A$ se para todo
$r>0$ temos $\Bola(p;r)\cap A\ne\emptyset$;
\item{(v)} $p$ é um {\rm ponto de acumulação} de $A$ se para todo
$r>0$ a bola $\Bola(p;r)$ contém pontos de $A$ diferentes de $p$
($p$ pode ou não pertencer a $A$).

\smallskip

\proclaim Definição. Um subconjunto $A\subset M$ é dito:
\item{(i)} {\rm aberto}, se todos os pontos de $A$ são interiores;
\item{(ii)} {\rm fechado}, se todo ponto de aderência de $A$ está
em $A$;
\item{(iii)} {\rm denso}, se todo ponto de $M$ é um ponto de
aderência de $A$;
\item{(iv)} {\rm limitado}, se $A$ está contido em alguma bola
de $M$.

\smallskip

\proclaim Definição. Associados a um subconjunto $A\subset M$
estão os seguintes subconjuntos de $M$:
\item{(i)} o {\rm interior} de $A$, denotado $\Int(A)\subset M$, é o
conjunto dos pontos interiores de $A$;
\item{(ii)} o {\rm fecho} de $A$, denotado $\overline A\subset M$, é o
conjunto dos pontos de aderência de $A$;
\item{(iii)} o {\rm exterior} de $A$, denotado $\Ext(A)\subset M$,
é o conjunto dos pontos exteriores de $A$;
\item{(iv)} a {\rm fronteira} de $A$, denotada $\partial A\subset
M$, é o conjunto dos pontos de fronteira de $A$;
\item{(v)} o {\rm conjunto derivado} de $A$, denotado $A'$, é o
conjunto dos pontos de acumulação de $A$.

\smallskip

É difícil se acostumar com a terminologia acima de uma hora para
outra. Para se familiarizar com isso tudo, o jeito é fazer algumas
figuras com $M=\R^n$ ($n=1,2,3$) e fazer montes de exercícios para
entender os inter-relacionamentos que existem entre todos esses
termos. Várias sugestões legais (e fáceis) de exercícios estão
listadas no final desta aula (vide Exercícios 1--13).

\smallskip

\proclaim Teorema. Se $x\in M$ é um ponto de acumulação de
$A\subset M$ então para todo $r>0$ a bola aberta $\Bola(x;r)$
contém infinitos pontos de $A$.

\Prova Suponha por absurdo que $\Bola(x;r)\cap A$ é finito; daí
existe $s>0$ com $s<r$ e $s<d(x,y)$ para todo $y\in\Bola(x;r)\cap
A$, $y\ne x$. Segue que a interseção $\Bola(x;s)\cap A$ não contém
pontos diferentes de $x$, uma contradição.\fimprova

\smallskip

A seguinte terminologia costuma ser útil também: \proclaim
Definição. Uma {\rm vizinhança} de um ponto $x\in M$ é um
subconjunto $V\subset M$ que contém um aberto que contém $x$;
equivalentemente, $V\subset M$ é uma vizinhança de $x$ se $x$ é um
ponto interior de $V$.

\noindent{\tt Observação}: Uma vizinhança de um ponto $x$ {\sl
pode não ser aberta}, embora alguns (poucos) autores definam
vizinhança de $x$ como sendo um aberto que contém $x$ (eu detesto
isso!); no nosso curso {\sl vizinhanças não precisam ser
abertas\/} e quando eu quiser me referir a uma vizinhança que
também é aberta vou dizer explicitamente {\sl vizinhança aberta}.

\smallskip

\proclaim Definição. Seja $(M,d)$ um espaço métrico. A coleção
$\tau$ de todos os subconjuntos abertos de $M$ é chamada a {\rm
topologia} de $M$.

\proclaim Definição. Seja $X$ um conjunto. Uma {\rm topologia} no
conjunto $X$ é uma coleção $\tau$ de subconjuntos de $X$ tal que
as seguintes propriedades valem:
\item{(TP1)} $\emptyset\in\tau$ e $X\in\tau$;
\item{(TP2)} se $(U_i)_{i\in I}$ é uma família de elementos de
$\tau$ então a união $\bigcup_{i\in I}U_i$ está em $\tau$;
\item{(TP3)} se $U,V\in\tau$ então $U\cap V\in\tau$.
\smallskip\noindent Um {\rm espaço topológico} é um par $(X,\tau)$
onde $X$ é um conjunto e $\tau$ é uma topologia em $X$. Os
elementos de $\tau$ são chamados os {\rm abertos} do espaço
topológico $(X,\tau)$.

A topologia associada a um espaço métrico de fato satisfaz as
propriedades (TP1), (TP2) e (TP3) (veja o Exercício 15). Não temos
interesse em estudar a teoria geral de espaços topológicos (na
verdade, mesmo com respeito a espaços métricos só precisaremos de
subespaços de $\R^n$ na prática); mas é bom ter em mente a noção
geral de espaço topológico de qualquer forma.

Um conceito definido para espaços métricos $(M,d)$ é dito um {\sl
conceito topológico\/} quando o mesmo pode ser redefinido usando
apenas a topologia de $(M,d)$. Por exemplo, ``conjunto aberto'',
``conjunto fechado'', ``conjunto denso'' são noções topológicas
enquanto que ``conjunto limitado'' não é (isso será demonstrado
posteriormente). Quando o enunciado de um teorema envolve apenas
noções topológicas então esse enunciado faz sentido no contexto
geral de espaços topológicos; isso {\sl não\/} significa que tal
teorema continue verdadeiro para espaços topológicos quaisquer (a
maioria requer alguma hipótese adicional). Esse assunto não será
importante no nosso curso.

\medskip

\item{(2)} {\bf Seqüências}.

\smallskip

No que segue $(M,d)$ denota um espaço métrico.

\smallskip

\proclaim Definição. Seja $(x_n)_{n\ge1}$ uma seqüência em $M$.
Dizemos que $(x_n)_{n\ge1}$ {\rm converge} para um ponto $x\in M$
se dado $\varepsilon>0$ existe $n_0\in\N$ tal que
$d(x_n,x)<\varepsilon$ para todo $n\ge n_0$; escrevemos
$\lim_{n\to\infty}x_n=x$ ou, mais resumidamente, $x_n\to x$.

Nos exercícios pedimos para vocês mostrarem que o limite de uma
seqüência é único (se existir).

\smallskip

\proclaim Definição. Dizemos que $(x_n)_{n\ge1}$ é uma {\rm
seqüência de Cauchy} em $M$ se dado $\varepsilon>0$ existe
$n_0\in\N$ tal que $d(x_n,x_m)<\varepsilon$ para todos $n,m\ge
n_0$.

Nos exercícios pedimos para vocês mostrarem que toda seqüência
convergente é de Cauchy.

\smallskip

\noindent{\tt Terminologia}. Dizemos que alguma propriedade é
satisfeita para todo número natural $n$ {\sl suficientemente
grande\/} se existe $n_0\in\N$ tal que a propriedade é satisfeita
para todo $n\ge n_0$ (i.e., se a propriedade deixa de ser
satisfeita no máximo para um número finito de $n$'s). Dizemos que
a propriedade é satisfeita para todo número natural $n$ {\sl
arbitrariamente grande\/} se dado $n_0\in\N$ então existe $n\ge
n_0$ que satisfaz a propriedade (i.e., se existem infinitos $n$'s
para os quais a propriedade é satisfeita).

Na terminologia acima, uma seqüência $(x_n)_{n\ge1}$ converge para
$x$ se e somente se dado $\varepsilon>0$ então $x_n$ fica na bola
$\Bola(x;\varepsilon)$ para todo $n$ suficientemente grande.

\smallskip

\proclaim Definição. Um espaço métrico $M$ é dito {\rm completo}
quando toda seqüência de Cauchy em $M$ é convergente.

\smallskip

\proclaim Teorema. Sejam $(M_i,d_i)$, $i=1,\ldots,n$, espaços
métricos e considere $M=\prod_{i=1}^nM_i$ munido da métrica
produto $d$. Seja $(x_k)_{k\ge1}$ uma seqüência em $M$ e escreva
$x_k=(x_k^1,\ldots,x_k^n)$ para todo $k\ge1$. Então:
\itemitem{(a)} $x_k\to x\in M$ se e somente se $x_k^i\to x^i$ em
$M_i$ para todo $i=1,\ldots,n$ (``uma seqüência converge se e
somente se converge coordenada por coordenada'');
\itemitem{(b)} $(x_k)_{k\ge1}$ é de Cauchy em $M$ se e somente se
$(x_k^i)_{k\ge1}$ é de Cauchy em $M_i$ para todo $i=1,\ldots,n$
(``uma seqüência é de Cauchy se e somente se for de Cauchy
coordenada por coordenada'').

\Prova Segue facilmente observando que, para todo $\varepsilon>0$
e todos $x,y\in M$:
$$\displaylines{\left(\sum_{i=1}^nd_i(x_i,y_i)^2\right)^{1\over2}=d(x,y)<\varepsilon\Longrightarrow
d_i(x_i,y_i)<\varepsilon,\quad i=1,\ldots,n,\cr
d_i(x_i,y_i)<{\varepsilon\over\sqrt n},\
i=1,\ldots,n\Longrightarrow
d(x,y)=\left(\sum_{i=1}^nd_i(x_i,y_i)^2\right)^{1\over2}<\varepsilon.\fimprova\cr}$$

\smallskip

\proclaim Corolário. O produto de espaços métricos completos é
completo.\fimprova

Usando o fato que $\R$ é completo (vide qualquer livro de Análise
Real) obtemos também:

\proclaim Corolário. $\R^n$ é completo.

O fato que $\R^n$ é completo seguirá também quando mostrarmos mais
adiante que todo limitado e fechado em $\R^n$ é compacto.

\smallskip

Os seguintes teoremas relacionam a topologia de $M$ com a noção de
limite de seqüência.

\proclaim Teorema. Sejam $A\subset M$ e $x\in A$. O ponto $x$ está
no interior de $A$ se e somente se dada uma seqüência
$(x_n)_{n\ge1}$ convergente para $x$ então $x_n\in A$ para todo
$n$ suficientemente grande. Em particular, $A$ é aberto se e
somente se para toda seqüência $(x_n)_{n\ge1}$ que converge para
um ponto de $A$ temos $x_n\in A$ para todo $n$ suficientemente
grande.

\Prova Se $x\in\Int(A)$ então existe $\varepsilon>0$ com
$\Bola(x;\varepsilon)\subset A$. Como $x_n\to x$ então
$x_n\in\Bola(x;\varepsilon)$ e portanto $x_n\in A$ para todo $n$
suficientemente grande. Reciprocamente, suponha que toda seqüência
que converge para $x$ fica em $A$ para $n$ suficientemente grande.
Se $x$ não fosse um ponto interior de $A$ então (negue a
definição!) para todo $n\ge1$, não pode ser $\Bola\big(x;{1\over
n}\big)\subset A$ e portanto existe $x_n\in\Bola\big(x;{1\over
n}\big)$ com $x_n\not\in A$. Daí $x_n\to x$ mas $x_n\not\in A$
para {\sl todo\/} $n\ge1$, uma contradição.\fimprova

\smallskip

\proclaim Teorema. Sejam $A\subset M$ e $x\in M$. Então $x$ é um
ponto de aderência de $A$ se e somente se $x$ é o limite de uma
seqüência de pontos de $A$. Em particular, $A$ é fechado se e
somente se dada uma seqüência $(x_n)_{n\ge1}$ em $A$ que converge
para um ponto $x\in M$ então $x\in A$.

\Prova Se $x$ é o limite de uma seqüência $(x_n)_{n\ge1}$ de
pontos de $A$ então dado $r>0$ temos $x_n\in\Bola(x;r)$ para $n$
suficientemente grande e em particular para tais $n$ temos
$x_n\in\Bola(x;r)\cap A$, donde $\Bola(x;r)\cap A\ne\emptyset$.
Reciprocamente, se $x$ é um ponto de aderência de $A$ então dado
$n\ge1$ temos $\Bola\big(x;{1\over n}\big)\cap A\ne\emptyset$,
donde existe $x_n\in A$ com $d(x_n,x)<{1\over n}$. Daí $x_n\to x$
e $x_n\in A$ para todo $n\ge1$.\fimprova

\smallskip

\proclaim Corolário. $A$ é denso em $M$ se e somente se todo ponto
de $M$ é limite de uma seqüência de pontos de $A$.\fimprova

\smallskip

\proclaim Teorema. Sejam $A\subset M$ e $x\in M$. Então $x$ é
ponto de acumulação de $A$ se e somente se $x$ é o limite de uma
seqüência $(x_n)_{n\ge1}$ de pontos de $A$, {\rm todos distintos
de $x$}.

\Prova Se $x$ é o limite de uma seqüência $(x_n)_{n\ge1}$ de
pontos de $A$ distintos de $x$ então dado $r>0$ temos
$x_n\in\Bola(x;r)$ para todo $n$ suficientemente grande e em
particular existem pontos em $\Bola(x;r)\cap A$ distintos de $x$.
Reciprocamente, se $x$ é um ponto de acumulação de $A$ então dado
$n\ge1$ existe $x_n\in\Bola\big(x;{1\over n}\big)\cap A$ com
$x_n\ne x$. Daí $x_n\to x$.\fimprova

\medskip
\goodbreak

\item{(3)} {\bf Limites de funções e funções contínuas}.

\smallskip

No que segue, $(M,d)$ e $(N,d')$ denotam espaços métricos.

\proclaim Definição. Seja $A$ um subconjunto de $M$ e $f:A\subset
M\to N$ uma função. Dado um ponto de acumulação $x_0\in M$ de $A$
então dizemos que $L\in N$ é um {\rm limite} de $f(x)$ quando $x$
tende a $x_0$ se dado $\varepsilon>0$ existe $\delta>0$ tal que
$d'\big(f(x),L\big)<\varepsilon$ para todo $x\in A$ tal que
$0<d(x,x_0)<\delta$; escrevemos $\lim_{x\to x_0}f(x)=L$.

Nos exercícios pedimos para voces mostrarem que o limite
$\lim_{x\to x_0}f(x)$ é único (quando existir).

\smallskip

\proclaim Definição. Seja $f:M\to N$ uma função e $x_0\in M$ um
ponto. Dizemos que $f$ é {\rm contínua} no ponto $x_0$ se dado
$\varepsilon>0$ existe $\delta>0$ tal que
$d'\big(f(x),f(x_0)\big)<\varepsilon$ para todo $x\in M$ com
$d(x,x_0)<\delta$.

Para quem estiver usando um pouco de senso crítico enquanto lê
essas notas, uma pergunta imediatamente surge à cabeça: ``por que
será que para definir limite ele considera $f$ definida num
subconjunto de $M$ e para definir continuidade ele considera $f$
definida em todo espaço $M$? E se eu precisasse falar de
continuidade de uma função que está definida só num subconjunto de
$M$?'' Pois é, isso tudo tem uma explicação. Se eu precisar falar
em continuidade de uma função $f$ definida só num subconjunto $A$
de $M$ então eu posso simplesmente lembrar que $A$ {\sl também é
um espaço métrico\/} com a métrica induzida de $M$: daí recaímos
outra vez no caso de uma função definida num espaço métrico
tomando valores em outro espaço métrico! No caso de limite ocorre
um problema com esse raciocínio: é importante também definir
limite de $f(x)$ quando $x$ tende a pontos {\sl fora do domínio de
$f$\/}; para dar uma definição de limite que cobre essa situação
precisamos pensar no domínio $A$ de $f$ como um subconjunto de um
espaço métrico $M$ possivelmente maior.

\smallskip

Nos exercícios pedimos para vocês provarem a relação familiar
entre limite e continuidade.

\smallskip

\proclaim Definição. Uma função $f:M\to N$ é dita um {\rm
homeomorfismo} se $f$ é bijetora e tanto $f$ como $f^{-1}$ são
contínuas.

\smallskip

\proclaim Teorema. Sejam $f:A\subset M\to N$, $g:N\to P$ funções e
$x_0\in M$ um ponto de acumulação de $A$; suponha que $\lim_{x\to
x_0}f(x)=L\in N$ e que $g$ seja contínua no ponto $L$. Então:
$$\lim_{x\to x_0}g\big(f(x)\big)=g(L).$$

\Prova Dado $\varepsilon>0$, existe $\eta>0$ tal que
$g\big(\Bola(L;\eta)\big)\subset\Bola\big(g(L);\varepsilon\big)$,
pela continuidade de $g$. A partir de $\eta>0$ encontramos
$\delta>0$ tal que $f(x)\in\Bola(L;\eta)$, sempre que
$x\in\Bola(x_0;\delta)\cap A$ e $x\ne x_0$. Daí
$g\big(f(x)\big)\in\Bola\big(g(L);\varepsilon\big)$ sempre que
$x\in\Bola(x_0;\delta)\cap A$ e $x\ne x_0$.\fimprova

Compare o teorema acima com o Exercício 29.

\smallskip

\proclaim Teorema. Se $f:M\to N$ é contínua em $x_0\in M$ e
$g:N\to P$ é contínua em $f(x_0)\in N$ então $g\circ f$ é contínua
em $x_0$ (``a composta de funções contínuas é contínua'').

\Prova Dado $\varepsilon>0$, existe $\eta>0$ tal que
$g\big(\Bola(f(x_0);\eta)\big)\subset\Bola\big(g(f(x_0));\varepsilon\big)$,
pela continuidade de $g$. Pela continuidade de $f$, achamos
$\delta>0$ com
$f\big(\Bola(x_0;\delta)\big)\subset\Bola\big(f(x_0);\eta\big)$.
Daí $(g\circ
f)\big(\Bola(x_0;\delta)\big)\subset\Bola\big(g(f(x_0));\varepsilon\big)$.\fimprova

\smallskip

\proclaim Teorema. Sejam $(N_i,d_i)$, $i=1,\ldots,n$, espaços
métricos e considere $N=\prod_{i=1}^nN_i$ munido da métrica
produto $d$. Seja $f:A\subset M\to N$ uma função e seja $x_0\in
M$. Escreva $f=(f^1,\ldots,f^n)$ com cada $f^i:M\to N_i$. Então:
\itemitem{(a)} se $x_0$ é um ponto de acumulação de $A$ temos
$\lim_{x\to x_0}f(x)=L\in N$ se e somente se $\lim_{x\to
x_0}f^i(x)=L^i\in N_i$ para todo $i=1,\ldots,n$ (``limites são
feitos coordenada por coordenada'');
\itemitem{(b)} se $x_0\in A$ então $f$ é contínua em $x_0$ se e
somente se $f^i$ é contínua em $x_0$ para $i=1,\ldots,n$
(``continuidade é feita coordenada por coordenada'').

\Prova Segue facilmente observando que, para todo $\varepsilon>0$
e todo $x\in A$:
$$\displaylines{\left(\sum_{i=1}^nd_i\big(f^i(x),L^i\big)^2\right)^{1\over2}=d\big(f(x),L\big)<\varepsilon\Longrightarrow
d_i(f^i(x),L^i)<\varepsilon,\quad i=1,\ldots,n,\cr
d_i\big(f^i(x),L^i\big)<{\varepsilon\over\sqrt n},\
i=1,\ldots,n\Longrightarrow
d\big(f(x),L\big)=\left(\sum_{i=1}^nd_i\big(f^i(x),L^i\big)^2\right)^{1\over2}<\varepsilon.\fimprova\cr}$$

\smallskip

\proclaim Corolário. (propriedades operatórias dos limites) Sejam
$f:A\subset M\to\R^n$, $g:A\subset M\to\R^n$ e $c:A\subset M\to\R$
funções. Temos:
\item{(a)} se $x_0\in M$ é um ponto de acumulação de $A$ e $f$,
$g$, $c$ admitem limite quando $x\to x_0$ então:
$$\lim_{x\to x_0}\big[f(x)+g(x)\big]=\lim_{x\to
x_0}f(x)+\lim_{x\to x_0}g(x),\quad\lim_{x\to
x_0}\big[c(x)f(x)\big]=\lim_{x\to x_0}c(x)\lim_{x\to x_0}f(x);$$
\item{(b)} se $f$, $g$ e $c$ são contínuas num ponto $x_0\in A$
então $f+g$ e $cf$ são contínuas no ponto $x_0$.

\Prova Segue dos teoremas anteriores, observando que as funções
soma e produto: $$\R^n\times\R^n\ni(v,w)\longmapsto
v+w\in\R^n,\quad\R\times\R^n\ni(c,v)\longmapsto cv\in\R^n,$$ são
contínuas (voltaremos a esse assunto depois).\fimprova

\smallskip

Vamos agora relacionar limite e continuidade de funções com
limites de seqüências.

\proclaim Teorema. Sejam $f:A\subset M\to N$ uma função e $x_0\in
M$ um ponto de acumulação de $A$. Temos $\lim_{x\to x_0}f(x)=L\in
N$ se e somente se vale a seguinte propriedade: para toda
seqüência $(x_n)_{n\ge1}$ em $A$ com $x_n\to x_0$ e $x_n\ne x_0$
para todo $n$ temos $f(x_n)\to L$.

\Prova Suponha que $\lim_{x\to x_0}f(x)=L$ e seja $(x_n)_{n\ge1}$
uma seqüência em $A$ com $x_n\to x_0$ e $x_n\ne x_0$ para todo
$n$. Dado $\varepsilon>0$, existe $\delta>0$ tal que
$d'\big(f(x),L\big)<\varepsilon$ para todo $x\in A$ com
$0<d(x,x_0)<\delta$; a partir de $\delta>0$ achamos $n_0\in\N$ com
$d(x_n,x_0)<\delta$ para todo $n\ge n_0$. Concluímos que
$d'\big(f(x_n),L\big)<\varepsilon$ para todo $n\ge n_0$.

Reciprocamente, se a propriedade envolvendo seqüências que aparece
no enunciado é satisfeita, vamos mostrar que $\lim_{x\to
x_0}f(x)=L$. Se não fosse, existiria um $\varepsilon>0$ tal que
para todo $n\ge1$, tomando $\delta={1\over n}$, existe $x_n\in A$
com $0<d(x_n,x_0)<{1\over n}$ mas
$d'\big(f(x_n),L\big)\ge\varepsilon$. Daí $(x_n)_{n\ge1}$ é uma
seqüência em $A$ com $x_n\to x_0$, $x_n\ne x_0$ para todo $n$, mas
$f(x_n)\not\to L$.\fimprova

\smallskip

\proclaim Teorema. Seja $f:M\to N$ uma função e $x_0\in M$ um
ponto. Temos que $f$ é contínua no ponto $x_0$ se e somente se
vale a seguinte propriedade: dada uma seqüência $(x_n)_{n\ge1}$ em
$M$ com $x_n\to x_0$ então $f(x_n)\to f(x_0)$.

\Prova Suponha que $f$ é contínua no ponto $x_0$ e seja
$(x_n)_{n\ge1}$ uma seqüência em $M$ com $x_n\to x_0$. Dado
$\varepsilon>0$, existe $\delta>0$ tal que
$d\big(f(x),f(x_0)\big)<\varepsilon$ sempre que $d(x,x_0)<\delta$;
a partir de $\delta>0$ encontramos $n_0\in\N$ com
$d(x_n,x_0)<\delta$ sempre que $n\ge n_0$. Daí
$d\big(f(x_n),f(x_0)\big)<\varepsilon$ sempre que $n\ge n_0$.

Reciprocamente, se a propriedade envolvendo seqüências que aparece
no enunciado é satisfeita, vamos mostrar que $f$ é contínua no
ponto $x_0$. Se não fosse, existiria $\varepsilon>0$ tal que para
todo $n\ge1$, tomando $\delta={1\over n}$, existiria $x_n\in M$
com $d(x_n,x_0)<{1\over n}$ mas
$d'\big(f(x_n),f(x_0)\big)\ge\varepsilon$. Daí $(x_n)_{n\ge1}$ é
uma seqüência em $M$ com $x_n\to x_0$ e $f(x_n)\not\to
f(x_0)$.\fimprova

\smallskip

\proclaim Corolário. Se $x_k\to x$, $y_k\to y$ em $\R^n$ e se
$c_k\to c$ em $\R$ então $x_k+y_k\to x+y$ e $c_kx_k\to cx$.

\Prova Segue do fato que as funções soma
$\R^n\times\R^n\ni(x,y)\mapsto x+y\in\R^n$ e produto
$\R\times\R^n\ni(c,x)\mapsto cx\in\R^n$ são contínuas (isso será
revisto depois), do fato que $(x_k,y_k)\to(x,y)$ se e somente se
$x_k\to x$ e $y_k\to y$ e do fato que $(c_k,x_k)\to(c,x)$ se e
somente se $c_k\to c$ e $x_k\to x$.\fimprova

\proclaim Teorema. As seguintes propriedades são equivalentes
sobre uma função $f:M\to N$:
\smallskip
\itemitem{(a)} $f$ é contínua;
\itemitem{(b)} para todo $U\subset N$ aberto, $f^{-1}(U)\subset M$
é aberto;
\itemitem{(c)} para todo $F\subset N$ fechado, $f^{-1}(F)\subset
M$ é fechado.

\Prova A equivalência entre (b) e (c) é um exercício usando o fato
que $U$ é aberto se e somente se $U^\compl$ é fechado, juntamente
com o fato que $f^{-1}(U^\compl)=f^{-1}(U)^\compl$.

Provemos ${\rm (a)}\Rightarrow{\rm (b)}$. Suponha que $f$ é
contínua; dado $U\subset N$ aberto, mostremos que
$f^{-1}(U)\subset M$ é aberto. Seja $x\in f^{-1}(U)$. Daí $f(x)\in
U$ e como $U$ é aberto, existe $\varepsilon>0$ com
$\Bola\big(f(x);\varepsilon\big)\subset U$. Como $f$ é contínua,
existe $\delta>0$ com
$f\big(\Bola(x;\delta)\big)\subset\Bola\big(f(x);\varepsilon\big)\subset
U$. Daí $\Bola(x;\delta)\subset f^{-1}(U)$.

Provemos ${\rm (b)}\Rightarrow{\rm (a)}$. Suponha (b) e mostremos
que $f$ é contínua num certo ponto $x\in M$. Dado $\varepsilon>0$,
então $U=\Bola\big(f(x);\varepsilon\big)$ é aberto em $N$ e
portanto $f^{-1}(U)$ é aberto em $M$. Como $x\in U$, existe
$\delta>0$ com $\Bola(x;\delta)\subset f^{-1}(U)$. Daí
$f\big(\Bola(x;\delta)\big)\subset\Bola\big(f(x);\varepsilon\big)$.\fimprova

\smallskip

\proclaim Corolário. Uma função bijetora $f:M\to N$ é um
homeomorfismo se e somente se valem uma das seguintes propriedades
equivalentes:
\item{(i)}para todo $U\subset M$, $U$ é aberto em $M$ $\Leftrightarrow$ $f(U)$
é aberto em $N$;
\item{(ii)}para todo $F\subset M$, $F$ é fechado em $M$ $\Leftrightarrow$ $f(F)$ é fechado em $N$.\fimprova

\smallskip

\proclaim Corolário. Sejam $f,g:M\to\R$ funções contínuas. Os
conjuntos:
$$\big\{x\in M:f(x)<g(x)\big\},\qquad\big\{x\in M:f(x)\ne
g(x)\big\}$$ são abertos e os conjuntos:
$$\big\{x\in M:f(x)\le g(x)\big\},\qquad\big\{x\in
M:f(x)=g(x)\big\}$$ são fechados.

\Prova A função $h=f-g:M\to\R$ é contínua. Os dois primeiros
conjuntos são $h^{-1}\big((-\infty,0)\big)$ e
$h^{-1}\big(\R\setminus\{0\}\big)$ e os dois últimos são
$h^{-1}\big((-\infty,0]\big)$ e $h^{-1}(0)$.\fimprova

\smallskip

\proclaim Definição. Uma função $f:M\to N$ é dita {\rm
uniformemente contínua} quando dado $\varepsilon>0$, existe
$\delta>0$ tal que, para todos $x,y\in M$, se $d(x,y)<\delta$
então $d'\big(f(x),f(y)\big)<\varepsilon$.

Obviamente toda função uniformemente contínua é contínua.

\smallskip

\proclaim Teorema. Se $f:(M,d)\to(N,d')$ e $g:(N,d')\to(P,d'')$
são uniformemente contínuas então $g\circ f$ é uniformemente
contínua.

\Prova Dado $\varepsilon>0$, existe $\eta>0$ tal que
$d'(u,v)<\eta$ implica $d''\big(g(u),g(v)\big)<\varepsilon$; a
partir de $\eta>0$ encontramos $\delta>0$ tal que $d(x,y)<\delta$
implica $d'\big(f(x),f(y)\big)<\eta$. Daí $d(x,y)<\delta$ implica
$d''\big((g\circ f)(x),(g\circ f)(y)\big)<\varepsilon$.\fimprova

\smallskip

\proclaim Teorema. Funções uniformemente contínuas levam
seqüências de Cauchy em seqüências de Cauchy, i.e., se $f:M\to N$
é uniformemente contínua e $(x_n)_{n\ge1}$ é uma seqüência de
Cauchy em $M$ então $\big(f(x_n)\big)_{n\ge1}$ é uma seqüência de
Cauchy em $N$.

\Prova Dado $\varepsilon>0$, existe $\delta>0$ tal que
$d(x,y)<\delta$ implica $d'\big(f(x),f(y)\big)<\varepsilon$; a
partir de $\delta>0$ encontramos $n_0\in\N$ tal que $n,m\ge n_0$
implica $d(x_n,x_m)<\delta$. Daí
$d'\big(f(x_n),f(y_n)\big)<\varepsilon$ sempre que $n,m\ge
n_0$.\fimprova

\smallskip

\proclaim Definição. Uma função $f:M\to N$ é dita {\rm
Lipschitziana} quando existe $k\ge0$ tal que
$d'\big(f(x),f(y)\big)\le k\,d(x,y)$ para todos $x,y\in M$;
dizemos então que $k$ é uma {\rm constante de Lipschitz} para $f$.
Quando $f$ é Lipschitziana com constante $k=1$ dizemos que $f$ é
uma {\rm contração fraca} e quando $f$ for Lipschitziana com
constante $k<1$ dizemos que $f$ é uma {\rm contração}.

\smallskip

\proclaim Teorema. Toda função Lipschitziana é uniformemente
contínua.

\Prova Se $k>0$ é uma constante de Lipschitz para $f$ então, dado
$\varepsilon>0$, tome $\delta={\varepsilon\over k}$
(complete!).\fimprova

\smallskip

\noindent{\tt Exemplo}. Uma função afim $f:\R\to\R$, $f(x)=ax+b$ é
sempre uniformemente contínua, pois é Lipschitziana (com constante
$\vert a\vert$). A função $f:\R\to\R$, $f(x)=x^2$ é contínua mas
não é uniformemente contínua; de fato, dado $\delta>0$ então
$\vert f(x+\delta)-f(x)\vert=\vert2\delta x+\delta^2\vert$ e essa
quantidade torna-se arbitrariamente grande quando $x\to+\infty$.

\smallskip

\noindent{\tt Exemplo}. As projeções $\pi_j:\prod_{i=1}^nM_i\to
M_j$, $j=1,\ldots,n$, de um produto cartesiano são contrações
fracas e portanto uniformemente contínuas.

\smallskip

\proclaim Teorema. Sejam $(N_i,d_i)$, $i=1,\ldots,n$, espaços
métricos e considere $N=\prod_{i=1}^nN_i$ munido da métrica
produto. Uma função $f:M\to N$ é uniformemente contínua se e
somente se cada coordenada $f^i:M\to N_i$ é uniformemente
contínua, $i=1,\ldots,n$.

\Prova É similar à demonstração de que $f$ é contínua se e somente
se cada $f^i$ é contínua.\fimprova

\smallskip

\proclaim Definição. Uma aplicação $f:(M,d)\to(N,d')$ tal que
$d'\big(f(x),f(y)\big)=d(x,y)$ para todos $x,y\in M$ é chamada uma
{\rm imersão isométrica}; quando $f$ é uma imersão isométrica
bijetora dizemos então que $f$ é uma {\rm isometria}.

\smallskip

\noindent{\tt Observação}. É fácil ver que toda imersão isométrica é injetora,
de modo que {\sl uma isometria é o mesmo que uma imersão isométrica sobrejetora}.

\smallskip

\noindent{\tt Exemplo}. Se $f:(M,d)\to(N,d')$ é uma isometria
então $f$ é um homeomorfismo; na verdade, $f$ e $f^{-1}$ são ambas
{\sl uniformemente contínuas}. De fato, se $f$ é uma isometria
então também $f^{-1}$ é uma isometria e isometrias são
Lipschitzianas.

\smallskip

\proclaim Definição. Uma função $f:M\to N$ é dita {\rm limitada}
quando sua imagem $f(M)$ é um subconjunto limitado de $N$, i.e.,
quando existe $c\ge0$ tal que $d\big(f(x),f(y)\big)\le c$ para
todos $x,y\in M$.

\medskip

\item{(4)} {\bf Métricas equivalentes}.

\smallskip

\proclaim Definição. Duas métricas $d_1$ e $d_2$ num conjunto $M$
são ditas {\rm equivalentes} quando induzem a mesma topologia em
$M$, i.e., quando $U$ é aberto em $(M,d_1)$ se e somente se $U$ é
aberto em $(M,d_2)$.

\proclaim Teorema. Duas métricas $d_1$ e $d_2$ em $M$ são
equivalentes se e somente se a aplicação identidade
$\Id:(M,d_1)\to(M,d_2)$ é um homeomorfismo.

\Prova A afirmação ``a identidade $\Id:(M,d_1)\to(M,d_2)$ é um
homeomorfismo'' é equivalente à afirmação ``$U$ é aberto em
$(M,d_1)$ se e somente se $\Id(U)=U$ é aberto em
$(M,d_2)$''.\fimprova

\smallskip

\noindent{\tt Exemplo}. Se $h:(M,d)\to(N,d')$ é um homeomorfismo
então a métrica $\tilde d$ em $M$ definida por $\tilde
d(x,y)=d'\big(h(x),h(y)\big)$ é equivalente a $d$. De fato, temos
um diagrama comutativo:
$$\xymatrix{%
&(N,d')\\
(M,d)\ar[ur]^h\ar[r]_{\Id}&(M,\tilde d)\ar[u]_h}$$ onde a flecha
inclinada é um homeomorfismo e a flecha vertical é uma isometria (veja o
Exercício 31); segue que a flecha horizontal é um homeomorfismo.

\smallskip

Observe que trocando uma métrica por outra equivalente não
prejudicamos noções topológicas do espaço, i.e., noções que podem
ser definidas usando apenas a topologia. Temos então o seguinte:

\proclaim Teorema. Sejam $d_1$, $d_2$ métricas equivalentes em
$M$. Então, para todo $A\subset M$ temos:
\item{(i)} $A$ é aberto em $(M,d_1)$ se e somente se $A$ é aberto
em $(M,d_2)$;
\item{(ii)} $A$ é fechado em $(M,d_1)$ se e somente se $A$ é
fechado em $(M,d_2)$;
\item{(iii)} os pontos interiores, de fronteira, de aderência e de
acumulação de $A$ no espaço $(M,d_1)$ coincidem respectivamente
com os pontos interiores, de fronteira, de aderência e de
acumulação de $A$ no espaço $(M,d_2)$;
\item{(iv)} $A$ é denso em $(M,d_1)$ se e somente se $A$ é denso
em $(M,d_2)$.\fimprova

Limites de funções, continuidade de funções e limites de seqüência
também são noções topológicas: afirmações como $\lim_{x\to
x_0}f(x)=L$, $x_n\to x$ e ``$f$ é contínua no ponto $x_0$''
continuam verdadeiras quando trocamos as métricas nos espaços
envolvidos por métricas equivalentes. Noções como completude,
seqüências de Cauchy, conjuntos limitados, continuidade uniforme
{\sl não\/} são preservadas quando trocamos uma métrica por outra
equivalente (vide exemplo a seguir).

\smallskip

\noindent{\tt Exemplo}. A {\sl função arco--tangente\/}
$\atg:\R\to\big({-{\pi\over2}},{\pi\over2}\big)$ é um
homeomorfismo. Segue então que a métrica $d$ em $\R$ definida por
$d(x,y)=|\atg x-\atg y|$ é equivalente à métrica Euclideana. Observe
que $\R$ é limitado (com diâmetro $\pi$) com a métrica $d$, mas
não é limitado com a métrica Euclideana. Além do mais, $\R$ é
completo com a métrica Euclideana, mas não é completo com a
métrica $d$; de fato, se $(x_n)_{n\ge1}$ é uma seqüência em $\R$
tal que $\atg x_n\to{\pi\over2}$ então $(x_n)_{n\ge1}$ é de Cauchy
(pois $(\atg x_n)_{n\ge1}$ é de Cauchy em
$\big({-{\pi\over2}},{\pi\over2}\big)$), mas $(x_n)_{n\ge1}$ não é
convergente em $\R$.

\smallskip

A noção de equivalência mais forte introduzida a seguir garante a
invariância de noções como completude e seqüências de Cauchy.

\proclaim Definição. Duas métricas $d_1$ e $d_2$ são ditas {\rm
uniformemente equivalentes} quando as a\-pli\-ca\-ções identidade
$\Id:(M,d_1)\to(M,d_2)$ e $\Id:(M,d_2)\to(M,d_1)$ são ambas
uniformemente contínuas.

\noindent{\tt Observação}. Uma condição suficiente para que $d_1$
e $d_2$ sejam uniformemente equivalentes é que as aplicações
identidade $\Id:(M,d_1)\to(M,d_2)$ e $\Id:(M,d_2)\to(M,d_1)$ sejam
Lipschitzianas; isso equivale à existência de constantes
$\alpha,\beta>0$ tais que:
$$\alpha\,d_1(x,y)\le d_2(x,y)\le\beta\,d_1(x,y),$$
para todos $x,y\in M$.

\smallskip

\proclaim Teorema. Sejam $d_1$ e $d_2$ métricas uniformemente
equivalentes em $M$. Então:
\item{(i)} uma seqüência $(x_n)_{n\ge1}$ é de Cauchy em $(M,d_1)$
se e somente se $(x_n)_{n\ge1}$ for de Cauchy em $(M,d_2)$;
\item{(ii)} $(M,d_1)$ é completo se e somente se $(M,d_2)$ é
completo.

\Prova Segue do fato que funções uniformemente contínuas levam
seqüências de Cauchy em seqüências de Cauchy.\fimprova

É fácil também mostrar que uma função uniformemente contínua
mantêm-se uniformemente contínua quando trocamos as métricas do
domínio ou do contra-domínio por métricas uniformemente
equivalentes (isso segue do fato que a composta de funções
uniformemente contínuas é uniformemente contínua).

\medskip

\item{(5)} {\bf Subespaços e topologia induzida}.

\smallskip

Como foi mencionado no início da aula, se $(M,d)$ é um espaço
métrico e $S\subset M$ é um subconjunto então
$\big(S,d\vert_{S\times S}\big)$ também é um espaço métrico;
podemos então de maneira natural pensar num subconjunto de um
espaço métrico como sendo também um espaço métrico. O nosso
objetivo agora é estabelecer algumas relações entre $M$ e $S$
(principalmente de natureza topológica).

\smallskip

\proclaim Teorema. Se $U\subset M$ é aberto em $M$ então $U\cap S$
é aberto em $S$. Além do mais, dado $Z\subset S$ aberto em $S$
então existe $U\subset M$ aberto em $M$ com $Z=U\cap S$ (``os
abertos de $S$ são os abertos de $M$ interceptados com $S$'').

\smallskip

Antes de provar o teorema acima, fazemos uma convenção: para $x\in
S$ e $r>0$ denotamos por $\Bola(x;r)$ e $\Bola[x;r]$
respectivamente a bola aberta e a bola fechada de centro $x$ e
raio $r$ {\sl no espaço métrico $(M,d)$}; nesta seção,
precisaremos também nos referir às bolas aberta e fechada de
centro $x$ e raio $r>0$ {\sl no espaço métrico
$\big(S,d\vert_{S\times S}\big)$} --- as últimas serão denotadas
por:
$$\eqalign{\Bola_S(x;r)&=\big\{y\in S:d(x,y)<r\big\},\cr
\Bola_S[x;r]&=\big\{y\in S:d(x,y)\le r\big\},\cr}.$$ A seguinte
observação é trivial:
$$\Bola_S(x;r)=\Bola(x;r)\cap S,\qquad\Bola_S[x;r]=\Bola[x;r]\cap
S.$$

\noindent{\bf Demonstração do Teorema}.\enspace Se $U\subset M$ é
aberto, mostremos que $U\cap S$ é aberto. Seja $x\in U\cap S$;
como $U$ é aberto em $M$, existe $r>0$ com $\Bola(x;r)\subset U$.
Daí $\Bola_S(x;r)\subset U\cap S$. Reciprocamente, suponha que
$Z\subset S$ é aberto em $S$. Para cada $x\in Z$ existe então
$r_x>0$ tal que $\Bola_S(x;r_x)=\Bola(x;r_x)\cap S\subset Z$. Daí
$U=\bigcup_{x\in Z}\Bola(x;r_x)$ é aberto em $M$ e $Z=U\cap
S$.\fimprova

\smallskip

\proclaim Corolário. Se $S$ é aberto em $M$ então os abertos de
$S$ são os abertos de $M$ que estão contidos em $S$.\fimprova

\smallskip

\proclaim Teorema. Se $F\subset M$ é fechado em $M$ então $F\cap
S$ é fechado em $S$. Além do mais, dado $H\subset S$ fechado em
$S$ então existe $F\subset M$ fechado em $M$ com $H=F\cap S$ (``os
fechados de $S$ são os fechados de $M$ interceptados com $S$'').

\Prova Se $F\subset M$ é fechado em $M$ então $F^\compl$ é aberto
em $M$ e $F^\compl\cap S$ é aberto em $S$ pelo teorema anterior.
Daí $S\setminus(F^\compl\cap S)=F\cap S$ é fechado em $S$.
Reciprocamente, suponha que $H\subset S$ é fechado em $S$. Daí
$S\setminus H$ é aberto em $S$ e portanto existe $U\subset M$
aberto em $M$ com $S\setminus H=U\cap S$, pelo teorema anterior.
Temos então que $F=U^\compl$ é fechado em $M$ e $H=F\cap
S$.\fimprova

\smallskip

\proclaim Corolário. Se $S$ é fechado em $M$ então os fechados de
$S$ são os fechados de $M$ que estão contidos em $S$.\fimprova

\medskip

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Espaços métricos (terminologia básica)}.

\medskip

\item{1.} Seja $(M,d)$ um espaço métrico. Mostre que se $r_1,r_2>0$ e
$x_1,x_2\in M$ são tais que $r_1+r_2<d(x_1,x_2)$ então as bolas
fechadas $\Bola[x_1;r_1]$ e $\Bola[x_2;r_2]$ são disjuntas. Se
$r_1+r_2\le d(x_1,x_2)$ então mostre que as bolas abertas
$\Bola(x_1;r_1)$ e $\Bola(x_2;r_2)$ são disjuntas.

\smallskip

\item{2.} Para um espaço métrico arbitrário $(M,d)$ e um
subconjunto $A\subset M$ mostre que:
\smallskip
\itemitem{(a)} $M=\Int(A)\cup\partial A\cup\Ext(A)$ e essa união é
disjunta;

\itemitem{(b)} $\Int(A)\subset A$ e $\Ext(A)\subset A^\compl$;

\itemitem{(c)} $A\cap\partial A=A\setminus\Int(A)$;

\itemitem{(d)} $\overline A=A\cup\partial A=A\cup A'$;

\itemitem{(e)} $\Ext(A)={\overline A}^{\,\compl}$;

\itemitem{(f)} $\Int(A)^\compl=\overline{A^\compl}$.

\smallskip

\item{3.} Mostre que as seguintes condições são equivalentes sobre
um subconjunto $A$ de um espaço métrico $M$:
\smallskip
\itemitem{(a)} $A$ é aberto;

\itemitem{(b)} $A=\Int(A)$;

\itemitem{(c)} para todo $x\in A$ existe $r>0$ com $B(x;r)\subset
A$;

\itemitem{(d)} para todo $x\in A$ existe $r>0$ com $B[x;r]\subset
A$.

\smallskip

\item{4.} Mostre que as seguintes condições são equivalentes sobre
um subconjunto $A$ de um espaço métrico $M$:
\smallskip
\itemitem{(a)} $A$ é fechado;

\itemitem{(b)} $A=\overline A$;

\itemitem{(c)} para todo $x\in M$, se $x\not\in A$ então existe
$r>0$ com $B(x;r)\cap A=\emptyset$;

\itemitem{(d)} para todo $x\in M$, se $x\not\in A$ então existe
$r>0$ com $B[x;r]\cap A=\emptyset$;

\itemitem{(e)} o complementar de $A$ é aberto (logo ``ser fechado'' é uma
noção topológica);

\itemitem{(f)} $\partial A\subset A$;

\itemitem{(g)} $A'\subset A$;

\smallskip

\item{5.} Mostre que as seguintes condições são equivalentes sobre
um subconjunto $A$ de um espaço métrico $M$:
\smallskip
\itemitem{(a)} $A$ é denso;
\itemitem{(b)} $\overline A=M$;
\itemitem{(c)} todo aberto não vazio de $M$ intercepta $A$ (logo ``ser denso''
é uma noção topológica);
\itemitem{(d)} toda bola aberta de $M$ intercepta $A$;
\itemitem{(e)} toda bola fechada de $M$ intercepta $A$;
\itemitem{(f)} dado $x\in M$ e $\varepsilon>0$ existe $y\in A$ com
$d(x,y)<\varepsilon$;
\itemitem{(g)} $\Int(A^\compl)=\emptyset$.

\eject

\item{6.} Sejam $(M,d)$ um espaço métrico e $A\subset M$ um
subconjunto. O {\sl diâmetro\/} de $A$ é definido por:
$$\diam(A)=\sup_{x,y\in A}d(x,y)\in[0,+\infty].$$
Mostre que $A$ é limitado se e somente se $\diam(A)<+\infty$.

\smallskip

\item{7.} Mostre que toda bola aberta é um conjunto aberto e que
toda bola fechada é um conjunto fechado (ninguém acha que apenas
os nomes ``bola aberta'' e ``bola fechada'' implicam
automaticamente que esses conjuntos sejam abertos ou fechados,
certo?).

\smallskip

\item{8.} Se $A$ é um subconjunto de um espaço métrico $M$ mostre
que:
\smallskip
\itemitem{(a)} $\Int(A)$ e $\Ext(A)$ são abertos;
\itemitem{(b)} $\partial A$ e $\overline A$ são fechados;
\itemitem{(c)} $\Int\big(\Int(A)\big)=\Int(A)$ e
$\overline{\overline A}=\overline A$.

\smallskip

\item{9.} Seja $A$ um subconjunto de um espaço métrico $M$. Mostre
que $\Int(A)$ é o {\sl maior aberto contido em $A$}, i.e., que
$\Int(A)$ é aberto e que se $U\subset M$ é um aberto contido em
$A$ então $U\subset\Int(A)$.

\smallskip

\item{10.} Seja $A$ um subconjunto de um espaço métrico $M$. Mostre
que $\overline A$ é o {\sl menor fechado que contém $A$}, i.e.,
que $\overline A$ é fechado e que se $F\subset M$ é um fechado que
contém $A$ então $\overline A\subset F$.

\smallskip

\item{11.} Dados subconjuntos $A$ e $B$ de um espaço métrico $M$,
mostre que $A\subset B\Rightarrow\overline A\subset\overline B$.

\smallskip

\item{12.} Dado um espaço métrico $M$, um subconjunto $A\subset M$
e um aberto $U\subset M$ mostre que $U\cap A\ne\emptyset$ se e
somente se $U\cap\overline A\ne\emptyset$.

\smallskip

\item{13.} Sobre um espaço métrico $M$, um subconjunto $A\subset M$
e um ponto $x\in M$ mostre que:
\smallskip
\itemitem{(a)} $x$ é um ponto interior de $A$ se e somente se $x$
pertence a um aberto contido em $A$ (logo ``ponto interior'' é uma
noção topológica);
\itemitem{(b)} $x$ é um ponto de aderência de $A$ se e somente se
todo aberto contendo $x$ intercepta $A$ (logo ``ponto de aderência
é uma noção topológica'');
\itemitem{(c)} $x$ é um ponto de fronteira de $A$ se e somente se
todo aberto que contém $x$ possui pontos de $A$ e pontos fora de
$A$ (logo ``ponto de fronteira'' é uma noção topológica);
\itemitem{(d)} $x$ é um ponto de acumulação de $A$ se e somente se
todo aberto que contém $x$ intercepta $A$ num ponto diferente de
$x$ (logo ``ponto de acumulação'' é uma noção topológica).

\eject

\vbox{\rightskip=50pt
\item{14.} Sobre um espaço métrico $M$, um subconjunto $A\subset M$
e um ponto $x\in M$ mostre que:
\smallskip
\itemitem{(a)} $x$ é um ponto interior de $A$ se e somente se $x$
possui uma vizinhança contida em $A$;
\itemitem{(b)} $x$ é um ponto de aderência de $A$ se e somente se
toda vizinhança de $x$ intercepta $A$;
\itemitem{(c)} $x$ é um ponto de fronteira de $A$ se e somente se
toda vizinhaça de $x$ possui pontos de $A$ e pontos fora de $A$;
\itemitem{(d)} $x$ é um ponto de acumulação de $A$ se e somente se
toda vizinhança de $x$ intercepta $A$ num ponto diferente de $x$;
\itemitem{(e)} $A$ é aberto se e somente se todo ponto de $A$
possui uma vizinhança contida em $A$;
\itemitem{(f)} $A$ é fechado se e somente se todo ponto fora de
$A$ possui uma vizinhança disjunta de $A$.\par}

\smallskip

\item{15.} Sobre um espaço métrico $M$ mostre que:
\smallskip
\itemitem{(a)} O conjunto vazio e o próprio espaço $M$ são abertos
e fechados;
\itemitem{(b)} A união de uma família arbitrária de abertos é um
aberto;
\itemitem{(c)} A interseção de dois (ou, mais geralmente, de uma
família {\sl finita}) de abertos é aberto;
\itemitem{(d)} A interseção de uma família arbitrária (não vazia) de fechados é
um fechado;
\itemitem{(e)} A união de dois (ou, mais geralmente, de uma
família {\sl finita}) de fechados é fechado.

\smallskip

\item{16.} Encontre em $\R^2$ uma família enumerável de abertos
cuja interseção não é aberta e uma família enumerável de fechados
cuja união não é fechada.

\smallskip

\item{17.} Sobre um espaço métrico $M$ e subconjuntos $A,B\subset
M$ mostre que:
\smallskip
\itemitem{(a)} $\overline{A\cup B}=\overline A\cup\overline B$;
\itemitem{(b)} $\overline{A\cap B}\subset\overline A\cap\overline
B$ (dê um contra-exemplo para a igualdade com $M=\R^2$);
\itemitem{(c)} $\Int(A\cap B)=\Int(A)\cap\Int(B)$;
\itemitem{(d)} $\Int(A)\cup\Int(B)\subset\Int(A\cup B)$ (dê um
contra-exemplo para a igualdade com $M=\R^2$).

\smallskip

\item{18.} Sejam $(M_i,d_i)$, $i=1,\ldots,n$, espaços métricos e
seja $M=\prod_{i=1}^nM_i$. Defina uma aplicação $d:M\times M\to\R$
fazendo:
$$d(x,y)=\left(\sum_{i=1}^nd_i(x_i,y_i)^2\right)^{1\over2},$$
para $x=(x_i)_{i=1}^n$, $y=(y_i)_{i=1}^n\in M$. Mostre que $d$
satisfaz a desigualdade triangular ({\sl dica}: para $x,y\in M$
denote por $\alpha_{xy}\in\R^n$ o vetor cuja $i$-ésima coordenada
é $d_i(x_i,y_i)$; observe que $d(x,y)=\Vert\alpha_{xy}\Vert$, onde
$\Vert\cdot\Vert$ denota a norma Euclideana em $\R^n$. Use a
desigualdade triangular para a norma Euclideana).

\eject

\item{19.} Seja $(M,d)$ um espaço métrico. Um {\sl sistema
fundamental de vizinhanças\/} para um ponto $x\in M$ é um conjunto
${\cal V}$ de vizinhanças de $x$ tal que toda vizinhança de $x$
contém uma vizinhança de $x$ pertencente a ${\cal V}$. Quando
todos os elementos de ${\cal V}$ são abertos, dizemos que ${\cal
V}$ é um {\sl sistema fundamental de vizinhanças abertas\/} de
$x$. Mostre que:
\smallskip
\itemitem{(a)} ${\cal V}=\big\{\Bola[x;r]:r>0\big\}$ é um sistema
fundamental de vizinhanças de $x$ e ${\cal
V}=\big\{\Bola(x;r):r>0\big\}$ é um sistema fundamental de
vizinhanças abertas de $x$;
\itemitem{(b)} ${\cal V}=\big\{\Bola\big[x;{1\over n}\big]:n\in\N\big\}$ é um sistema
fundamental de vizinhanças de $x$ e ${\cal
V}=\big\{\Bola\big(x;{1\over n}\big):n\in\N\big\}$ é um sistema
fundamental de vizinhanças abertas de $x$;
\itemitem{(c)} para cada $x\in M$ seja ${\cal V}_x$ um sistema
fundamental de vizinhanças de $x$. Reprove os itens (a)--(f) do
Exercício 14 trocando o termo ``vizinhança de $x$'' por
``vizinhança de $x$ pertencente a ${\cal V}_x$''.

\smallskip

\item{20.} Sejam $(M_i,d_i)$, $i=1,\ldots,n$, espaços métricos e
considere o produto $M=\prod_{i=1}^nM_i$ munida da métrica produto
$d$. Mostre que:
\smallskip
\itemitem{(a)} se $U_i$ é aberto em $M_i$ para cada $i=1,\ldots,n$
então $\prod_{i=1}^nU_i$ é aberto em $M$ (``o produto de abertos é
aberto'');
\itemitem{(b)} se $F_i$ é fechado em $M_i$ para cada
$i=1,\ldots,n$ então $\prod_{i=1}^nF_i$ é fechado em $M$ (``o
produto de fechados é fechado'');
\itemitem{(c)} para $x=(x_i)_{i=1}^n\in M$, mostre que:
$${\cal
V}=\left\{\prod_{i=1}^n\Bola(x_i;r_i):r_1,\ldots,r_n\in(0,+\infty)\right\}$$
é um sistema fundamental de vizinhanças para $x$ em $M$;
\itemitem{(d)} o fecho do produto é o produto dos fechos, i.e.:
$$\overline{A_1\times\cdots\times
A_n}=\overline{A_1}\times\cdots\times\overline{A_n},$$ para
quaisquer $A_i\subset M_i$, $i=1,\ldots,n$.

\medskip

\noindent{\tt Espaços métricos (seqüências)}.

\smallskip

\item{21.} Seja $M$ um espaço métrico. Mostre que se $y_1$ e $y_2$
são ambos limites de uma seqüência $(x_n)_{n\ge1}$ em $M$ então
$y_1=y_2$.

\smallskip

\item{22.} Mostre que toda seqüência convergente num espaço
métrico é uma seqüência de Cauchy.

\smallskip

\item{23.} Seja $(M,d)$ um espaço métrico e $(x_n)_{n\ge1}$ uma
seqüência em $M$. Mostre que $x_n$ converge para um certo $x\in M$
se e somente se $d(x_n,x)\to0$ em $\R$.

\smallskip

\item{24.} Se $(x_n)_{n\ge1}$ é uma seqüência e $\phi:\N\to\N$ é
uma função estritamente crescente então dizemos que
$(x_{\phi(k)})_{k\ge1}$ é uma {\sl subseqüência\/} de
$(x_n)_{n\ge1}$; tipicamente escrevemos $n_k=\phi(k)$ e dizemos
que $(x_{n_k})_{k\ge1}$ é uma subseqüência de $(x_n)_{n\ge1}$.
Mostre que se uma seqüência $(x_n)_{n\ge1}$ converge para $x$ num
espaço métrico $M$ então toda subseqüência $(x_{n_k})_{k\ge1}$
converge para $x$.

\smallskip

\item{25.} ({\sl o teorema do sanduíche}) Mostre que se $y_n\to L$
e $z_n\to L$ em $\R$ e se $y_n\le x_n\le z_n$ para todo $n\in\N$
então $x_n\to L$ em $\R$.

\medskip

\noindent{\tt Espaços métricos (limite e continuidade)}.

\smallskip

\item{26.} Seja $f:A\subset M\to N$ uma função, onde $(M,d)$ e
$(N,d')$ são espaços métricos. Dado um ponto de acumulação $x_0\in
M$ de $A$, mostre que se $L$ e $L'\in N$ são ambos limites de
$f(x)$ quando $x$ tende a $x_0$ então $L=L'$.

\noindent{\tt Observação}. Para quem gosta de pensar em coisas
vaziamente satisfeitas: mostre que se $x_0\in M$ não é um ponto de
acumulação de $A$ então seria verdade que {\sl todo\/} $L\in N$ é
um limite de $f(x)$ quando $x$ tende a $x_0$.

\smallskip

\item{27.} Sejam $(M,d)$, $(N,d')$ espaços métricos e $f:M\to N$
uma função. Dado $x_0\in M$ mostre que:
\smallskip
\itemitem{(a)} se $x_0$ é um ponto de acumulação de $M$ então $f$
é contínua no ponto $x_0$ se e somente se o limite $\lim_{x\to
x_0}f(x)$ existe e é igual a $f(x_0)$.
\itemitem{(b)} se $x_0$ não é um ponto de acumulação de $M$ (i.e.,
se $x_0$ é um {\sl ponto isolado\/} de $M$) então $f$ é {\sl
sempre\/} contínua no ponto $x_0$.

\smallskip

\item{28.} Sejam $(M,d)$, $(N,d')$ espaços métricos e $f:M\to N$
uma função. Mostre que $f$ é contínua num ponto $x\in M$ se e
somente se dado $\varepsilon>0$, existe $\delta>0$ com:
$$f\big(\Bola(x;\delta)\big)\subset\Bola\big(f(x);\varepsilon\big).$$

\smallskip

\item{29.} Sejam $(M,d)$, $(N,d')$ e $(P,d'')$ espaços métricos,
$A\subset M$, $B\subset N$ subconjuntos, $x_0\in M$ um ponto de
acumulação de $A$ e $f:A\to N$, $g:B\to P$ funções com
$f(A)\subset B$. Suponha que $\lim_{x\to x_0}f(x)=L\in N$ e que
$L$ {\sl não\/} pertence à imagem de $f$. Mostre que:
\smallskip
\itemitem{(a)} $L$ é um ponto de acumulação de $B$;
\itemitem{(b)} se $\lim_{y\to L}g(y)=K\in P$ então $\lim_{x\to
x_0}g\big(f(x)\big)=K$;
\itemitem{(c)} ache um contra-exemplo para os itens (a) e (b)
quando removemos a hipótese que $L$ não pertence à imagem de $f$.

\smallskip

\item{30.} ({\sl teorema do sanduíche para funções}) Sejam
$f,g,h:A\subset M\to\R$ funções, onde $M$ é um espaço métrico.
Suponha que $x_0\in M$ é um ponto de acumulação de $A$, que
$\lim_{x\to x_0}f(x)=\lim_{x\to x_0}h(x)=L\in\R$ e que $f(x)\le
g(x)\le h(x)$ para todos $x\in A$. Mostre que $\lim_{x\to
x_0}g(x)=L$.

\smallskip

\item{31.} ({\sl métricas induzidas por funções}) Seja $f:M\to N$
uma função bijetora, onde $M$ é um conjunto e $(N,d')$ é um espaço
métrico. Mostre que $d(x,y)=d'\big(f(x),f(y)\big)$ é uma métrica
em $M$; mostre também que $d$ é a {\sl única\/} métrica em $M$ que
torna $f$ uma isometria.

\eject

\noindent{\tt Espaços métricos (métricas equivalentes)}.

\smallskip

\item{32.} Para $x\in\R^n$ denote por $\Vert x\Vert$ a norma
Euclideana de $x$ e defina:
$$\Vert x\Vert_1=\sum_{i=1}^n\vert x_i\vert,\quad\Vert
x\Vert_\infty=\max\big\{\vert x_1\vert,\ldots,\vert
x_n\vert\big\}.$$ Mostre que:
$$\Vert x\Vert_\infty\le\Vert x\Vert,\quad\Vert x\Vert\le\sqrt
n\,\Vert x\Vert_\infty,\quad\Vert x\Vert_\infty\le\Vert
x\Vert_1,\quad\Vert x\Vert_1\le n\Vert x\Vert_\infty,$$ e conclua
que:
$$\Vert x\Vert\le\sqrt n\,\Vert x\Vert_1,\quad\Vert x\Vert_1\le n\Vert x\Vert.$$

\smallskip

\item{33.} Sejam $(M_i,d_i)$, $i=1,\ldots,n$, espaços métricos e
considere o produto $M=\prod_{i=1}^nM_i$. Mostre que as fórmulas:
$$d_1(x,y)=\sum_{i=1}^nd_i(x_i,y_i),\quad
d_\infty(x,y)=\max\big\{d_1(x_1,y_1),\ldots,d_n(x_n,y_n)\big\},$$
definem métricas em $M$, onde $x=(x_i)_{i=1}^n$,
$y=(y_i)_{i=1}^n\in M$. Usando o Exercício anterior, mostre que as
métricas $d_1$ e $d_\infty$ são uniformemente equivalentes entre
si e também são uniformemente equivalentes à métrica produto $d$.

\smallskip

\item{34.} Sejam $d$ e $d'$ métricas em $M$ de modo que as
aplicações identidade:
$$\Id:(M,d)\to(M,d'),\quad\Id:(M,d')\to(M,d)$$
sejam Lipschitzianas. Mostre que $A\subset M$ é limitado em
$(M,d)$ se e somente se $A$ é limitado em $(M,d')$.

\medskip

\noindent{\tt Espaços métricos (subespaços e topologia induzida)}.

\smallskip

\item{35.} Seja $(M,d)$ um espaço métrico e considere $S\subset M$
munido da métrica induzida por $d$. Mostre que:
\smallskip
\itemitem{(a)} uma seqüência $(x_n)_{n\ge1}$ em $S$ é de Cauchy em
$S$ se e somente se $(x_n)_{n\ge1}$ for de Cauchy em $M$;
\itemitem{(b)} uma seqüência $(x_n)_{n\ge1}$ em $S$ converge para
$x\in S$ no espaço $S$ se e somente se $(x_n)_{n\ge1}$ converge
para $x\in S$ no espaço $M$;
\itemitem{(c)} para $A\subset S$, o fecho
de $A$ no espaço métrico $S$ coincide com $\overline A\cap S$;
\itemitem{(d)} a função inclusão $i:S\to M$ é uniformemente
contínua;
\smallskip
se $(N,d')$ é um outro espaço métrico então:
\smallskip
\itemitem{(e)} se uma função $f:M\to N$ é (uniformemente) contínua
então sua restrição $f\vert_S:S\to N$ é (uniformemente) contínua;
\itemitem{(f)} se uma função $f:N\to M$ tem imagem contida em $S$
então $f:N\to M$ é (uniformemente) contínua se e somente se
$f:N\to S$ é (uniformemente) contínua.

\smallskip

\item{36.} Seja $(M,d)$ um espaço métrico e considere $S\subset M$
munido da métrica induzida por $d$. Mostre que:
\smallskip
\itemitem{(a)} se $S$ é um espaço métrico completo então $S$ é
fechado em $M$;
\itemitem{(b)} se $M$ é um espaço métrico completo então $S$ é
completo se e somente se $S$ é fechado em $M$.

\smallskip

\item{37.} Mostre que {\sl continuidade é uma propriedade local},
i.e., que dada uma função $f:M\to N$ entre espaços métricos, um
ponto $x\in M$ e uma vizinhança $V\subset M$ de $x$ então $f$ é
contínua no ponto $x$ se e somente se a restrição $f\vert_V$ é
contínua no ponto $x$.

\smallskip

\item{38.} Sejam $(M,d)$, $(N,d')$ espaços métricos e suponha que
$M=\bigcup_{i=1}^nF_i$ com cada $F_i$ fechado em $M$. Seja $f:M\to
N$ uma função tal que $f\vert_{F_i}:F_i\to N$ é contínua para todo
$i=1,\ldots,n$. Mostre que $f$ é contínua ({\sl dica}: use que $f$
é contínua se e somente se $f^{-1}(F)$ é fechado em $M$ para todo
$F$ fechado em $N$).

\smallskip

\item{39.} Sejam $(M,d)$ um espaço métrico e $S\subset M$ um subconjunto. Um
ponto $x\in S$ é dito {\sl isolado\/} de $S$ quando $x\not\in S'$, i.e.,
quando existe $r>0$ tal que $\Bola(x;r)\cap S=\{x\}$. Mostre que $x\in S$ é
isolado de $S$ se e somente se $\{x\}$ é um aberto do espaço métrico
$\big(S,d\vert_{S\times S}\big)$.

\smallskip

\item{40.} Um espaço métrico $(M,d)$ é dito {\sl discreto\/} quando todo
subconjunto de $M$ é aberto. Mostre que:
\smallskip
\itemitem{(a)} $M$ é discreto se e somente se $\{x\}$ é aberto em $M$ para
todo $x\in M$;
\itemitem{(b)} $M$ é discreto se e somente se todo subconjunto de $M$ é
fechado em $M$;
\itemitem{(c)} um conjunto $M$ munido da métrica zero-um é sempre um espaço
métrico discreto;
\itemitem{(d)} considerando $S\subset M$ com a métrica induzida de $M$ então
$S$ é um espaço discreto se e somente se todo ponto de $S$ é isolado;
\itemitem{(e)} $\Z$ é discreto com a métrica induzida de $\R$.

\vfil\eject

\centerline{\bf Aula número $6$ (22/03)}
\bigskip
\bigskip

A aula número $6$ cobriu o material das Seções (3) e (4)
originalmente destinado à aula número $5$.

\bigskip\bigskip\bigskip\bigskip

\centerline{\bf Aula número $7$ (27/03)}
\bigskip
\bigskip

\item{(1)} {\bf Espaços conexos}.

\smallskip

\noindent{\tt Observação}. Esta aula deve iniciar cobrindo a seção
(5) da aula número $5$ (20/03) sobre o tema ``Subespaços e
topologia induzida''; para compreensão dos resultados sobre
espaços conexos tal assunto é fundamental.

\smallskip

Seja $(M,d)$ um espaço métrico. Se $A$, $B$ são abertos disjuntos em $M$ tais
que $M=A\cup B$ então dizemos que $M=A\cup B$ é uma {\sl cisão\/} de $M$;
quando $A=\emptyset$, $B=M$ ou $A=M$, $B=\emptyset$ então dizemos que $M=A\cup
B$ é uma {\sl cisão trivial\/} de $M$.

\proclaim Definição. Dizemos que o espaço métrico $M$ é {\rm conexo} quando
$M$ só admite a cisão trivial, i.e., se dados abertos disjuntos $A$, $B\subset
M$ com $M=A\cup B$ então $A=\emptyset$ ou $B=\emptyset$.

Algumas maneiras equivalentes de definir espaço conexo estão
listadas no Exercício~6. Particularmente importante é a observação
que $M$ é conexo se e somente se os únicos subconjuntos de $M$ que
são ao mesmo tempo abertos e fechados são $\emptyset$ e $M$.

\smallskip

\noindent{\tt Observação}. Conexidade é uma {\sl noção
intrínseca}, i.e., não depende de espaço ambiente (ao contrário de
noções como aberto, fechado e denso). Por exemplo, se $(M,d)$ é um
espaço métrico e $S\subset M$ é um subconjunto então afirmações
como ``$S$ é aberto'', ``$S$ é fechado'', ``$S$ é denso'' são {\sl
relativas a $M$}; se $M$ for um subespaço métrico de um espaço
métrico maior $N$ então é bem possível que $S$ seja aberto (ou
fechado, ou denso) em $M$, mas {\sl não\/} em $N$. Por outro lado,
a definição de conexidade {\sl não faz referência a um ambiente
externo}; dizemos que um subconjunto $S$ de um espaço métrico $M$
é conexo quando o {\sl espaço métrico $S$ com a métrica
$d\vert_{S\times S}$ induzida de $M$ for conexo\/} no sentido da
definição anterior. Note que se $M$ é um subespaço métrico de $N$
e $S\subset M$ então $M$ e $N$ {\sl induzem a mesma métrica em
$S$\/} (isso é meio óbvio!) e portanto $S$ é conexo visto como
subconjunto de $M$ se e somente se $S$ é conexo visto como
subconjunto de $N$. Observamos que também as noções de {\sl espaço
limitado\/} e {\sl espaço completo\/} mencionadas anteriormente
são intrínsecas.

\smallskip

\noindent{\tt Observação}. Conexidade é uma noção topológica,
i.e., pode ser definida apenas em termos de abertos (sem
referência direta à métrica). Segue que dois espaços homeomorfos
são ambos conexos ou ambos desconexos.

\smallskip

\noindent{\tt Exemplo bobo}. Se $M=\emptyset$ ou se $M$ tem só um
ponto então $M$ é conexo.

\smallskip

\proclaim Teorema. Sejam $(M,d)$, $(N,d')$ espaços métricos e seja
$f:M\to N$ uma função contínua. Se $M$ é conexo então a imagem de
$f$ é conexa (``a imagem de um conexo por uma função contínua
é conexa'').

\Prova Podemos substituir $N$ por $f(M)$ (isso não afeta a continuidade de
$f$) e portanto podemos supor sem perda de generalidade que $f$ seja
sobrejetora. Seja $A\subset N$ aberto e fechado. Daí $f^{-1}(A)$ é aberto e
fechado em $M$ e portanto $f^{-1}(A)=\emptyset$ ou $f^{-1}(A)=M$; no primeiro
caso temos $A=\emptyset$ e no segundo $A=N$.\fimprova

\smallskip

\proclaim Teorema. Seja $(M,d)$ um espaço métrico e seja $(C_i)_{i\in I}$ uma
família de subconjuntos conexos de $M$ tal que existe $p\in\bigcap_{i\in
I}C_i$. Então a união $C=\bigcup_{i\in I}C_i$ é conexa (``a união de conexos
com ponto em comum é conexa'').

\Prova Seja $A\subset C$ aberto e fechado em $C$. Para cada $i\in I$ temos que
$A\cap C_i$ é aberto e fechado em $C_i$ e portanto $A\cap C_i=\emptyset$ ou
$A\cap C_i=C_i$, pela conexidade de $C_i$. Se $p\in A$ então deve ser $A\cap
C_i=C_i$ para todo $i\in I$ e portanto $A=C$; se $p\not\in A$ então deve ser
$A\cap C_i=\emptyset$ para todo $i\in I$ e portanto $A=\emptyset$.\fimprova

\smallskip

\proclaim Teorema. Sejam $(M,d)$ um espaço métrico e $X\subset M$
um subconjunto conexo. Se $C$ é tal que $X\subset
C\subset\overline X$ então $C$ é conexo (em particular, o fecho de
um conexo é conexo).

\Prova Seja $A\subset C$ aberto e fechado em $C$. Temos que $A\cap X$ é aberto
e fechado em $X$, donde $A\cap X=\emptyset$ ou $A\cap X=X$, já que $X$ é
conexo. Como $X$ é denso em $C$ e $A$ é aberto em $C$ temos que $A\cap
X=\emptyset$ implica $A=\emptyset$. Se for $A\cap X=X$ (i.e., $X\subset A$)
então $A=C$ pois $A$ é fechado em $C$ e $X$ é denso em $C$.\fimprova

\smallskip

\proclaim Teorema. A reta $\R$ (com a métrica Euclideana, lógico)
é conexa.

\Prova Suponha por absurdo que existam abertos disjuntos não
vazios $A$, $B$ em $\R$ com $\R=A\cup B$. Escolha $a\in A$ e $b\in
B$; como $A$ e $B$ são disjuntos temos $a\ne b$ e podemos por
exemplo supor que $a<b$ (senão é só trocar $A$ por $B$). Considere
o conjunto $X=\big\{x\in A:x<b\}\subset\R$. Temos que $X$ é
limitado superiormente (por $b$) e não vazio ($a\in X$), donde
existe $c=\sup X\in\R$; obviamente $c\le b$. Para todo
$\varepsilon>0$, existem elementos de $X$ (e portanto de $A$) no
intervalo $(c-\varepsilon,c]$, donde $c$ é um ponto de aderência
de $A$. Como $A$ é fechado, temos $c\in A$; como $A\cap
B=\emptyset$, temos $c\ne b$ (e portanto $c<b$). Como $A$ é
aberto, existe $\varepsilon>0$ tal que
$(c-\varepsilon,c+\varepsilon)\subset A$ e podemos supor também
que $c+\varepsilon<b$; daí $[c,c+\varepsilon)\subset X$,
contradizendo $c=\sup X$.\fimprova

\smallskip

\proclaim Corolário. Todo intervalo de $\R$ é conexo.

\Prova Intervalos abertos são homeomorfos a $\R$ (mostre!) e
portanto conexos; se o intervalo $I$ contém alguma de suas
extremidades então existe um intervalo aberto $J$ com $J\subset
I\subset\overline J$, donde $I$ é conexo.\fimprova

\smallskip

\proclaim Teorema. Se $I\subset\R$ é conexo então ou
$I=\emptyset$, ou $I$ tem um único ponto ou $I$ é um intervalo.

\Prova Sejam $a,b\in I$ com $a<b$. Afirmamos que $[a,b]\subset I$;
de fato, se existisse $c\in(a,b)$ com $c\not\in I$ então
$I=\big(I\cap(-\infty,c)\big)\cup\big(I\cap(c,+\infty)\big)$ seria
uma cisão não trivial de $I$, contradizendo o fato que $I$ é
conexo. Suponha que $I$ tem mais de um ponto; é fácil ver então
que $(\inf I,\sup I)\subset I\subset[\inf I,\sup I]$, donde $I$ é
um intervalo (é possível que $\inf I=-\infty$ ou $\sup
I=+\infty$).\fimprova

\smallskip

\proclaim Corolário. (o teorema do valor intermediário) Se $f:I\subset\R\to\R$
é uma função contínua definida num intervalo $I\subset\R$ e se $f$ assume os
valores $x,y\in\R$ então $f$ assume todos os valores entre $x$ e $y$.

\Prova Observe que $f(I)\subset\R$ é conexo e portanto é um intervalo (ou um
único ponto).\fimprova

\smallskip

\proclaim Teorema. O produto $M=\prod_{i=1}^nM_i$ de espaços
métricos conexos $(M_i,d_i)$ é conexo.

\Prova É suficiente mostrar o caso $n=2$, sendo que o caso geral
segue por indução (observe que o produto cartesiano de espaços
métricos é associativo). Vamos supor $M_1$ e $M_2$ não vazios,
caso contrário o resultado é trivial. Fixe $x\in M_1$; para cada
$y\in M_2$, os espaços $\{x\}\times M_2$ e $M_1\times\{y\}$ são
conexos (pois são homeomorfos respectivamente à $M_2$ e à $M_1$) e
possuem o ponto $(x,y)$ em comum. Segue que:
$$C_y=\big(\{x\}\times
M_2\big)\cup\big(M_1\times\{y\}\big)$$ é conexo para todo $y\in
M_2$. Observe agora que $M_1\times M_2=\bigcup_{y\in M_2}C_y$,
onde cada $C_y$ é conexo e $\{x\}\times M_2\subset\bigcap_{y\in
M_2}C_y\ne\emptyset$.\fimprova

\smallskip

\proclaim Corolário. $\R^n$ é conexo.\fimprova

\smallskip

\proclaim Definição. Um espaço métrico $(M,d)$ é dito {\rm conexo
por caminhos} (dizemos também {\rm conexo por arcos}) quando dados
$x,y\in M$ existe uma aplicação contínua (dizemos também uma {\rm
curva} contínua) $\gamma:[0,1]\to M$ tal que $\gamma(0)=x$ e
$\gamma(1)=y$ (dizemos que $\gamma$ é um {\rm caminho ligando $x$
a $y$}).

\smallskip

\proclaim Teorema. Se $M$ é conexo por caminhos então $M$ é conexo.

\Prova Escolha $x\in M$ (o caso $M=\emptyset$ é trivial). Para cada $y\in M$
podemos escolher uma curva contínua $\gamma_y:[0,1]\to M$ com $\gamma_y(0)=x$
e $\gamma_y(1)=y$. Como $\gamma_y$ é contínua e $[0,1]\subset\R$ é conexo
temos que a imagem de $\gamma_y$ é conexa; daí $M=\bigcup_{y\in
M}\Img(\gamma_y)$ e $x\in\Img(\gamma_y)$ para todo $y\in M$, i.e., $M$ é união
de conexos com ponto em comum.\fimprova

\smallskip

\noindent{\tt Exemplo}. Dados $x,y\in\R^n$ então o {\sl segmento
de reta\/} ligando $x$ a $y$ é definido por:
$$[x,y]=\big\{(1-t)x+ty:t\in[0,1]\big\};$$
um subconjunto $S\subset\R^n$ é dito {\sl convexo\/} quando dados
$x,y\in S$ então $[x,y]\subset S$. Todo subconjunto convexo $S$ de
$\R^n$ é conexo por caminhos ($[0,1]\ni t\mapsto(1-t)x+ty\in S$ é
um caminho ligando $x,y\in S$) e portanto é conexo. Bolas abertas
e bolas fechadas em $\R^n$ são convexas e portanto conexas por
caminhos (e conexas); de fato, vamos verificar por exemplo que a
bola aberta $\Bola(a;r)$, $a\in\R^n$, $r>0$ é convexa. Dados
$x,y\in\R^n$ com $\Vert x-a\Vert<r$, $\Vert y-a\Vert<r$ então para
todo $t\in[0,1]$:
$$\big\Vert(1-t)x+ty-a\big\Vert=\big\Vert(1-t)x-(1-t)a+ty-ta\big\Vert\le(1-t)\Vert
x-a\Vert+t\Vert y-a\Vert<r.$$

\smallskip

\proclaim Teorema. Seja $M$ um espaço métrico conexo e suponha que
todo ponto de $M$ possui uma vizinhança conexa por caminhos. Então
$M$ é conexo por caminhos.

\Prova Defina uma relação de equivalência $\sim$ em $M$ fazendo:
$$x\sim y\quad\Longleftrightarrow\quad\hbox{existe um caminho em $M$ ligando
$x$ a $y$},$$ para todos $x,y\in M$ (veja o Exercício 8). Seja $C$
uma classe de equivalência; vamos mostrar que $C$ é aberta. Dado
$x\in C$ então $x$ possui uma vizinhança $V$ em $M$ que é conexa
por caminhos, donde $y\sim x$ para todo $y\in V$; daí $V\subset C$
e $x$ é um ponto interior de $C$. Mostramos então que $C$ é
aberta; mas $C$ também é fechada, pois seu complementar é a união
das outras classes de equivalência (que também são abertas). Como
$C\ne\emptyset$ (classes de equivalência nunca são vazias, por
definição; no caso trivial $M=\emptyset$ não temos nenhuma classe
de equivalência), temos $C=M$ e portanto $M$ é conexo por
caminhos.\fimprova

\smallskip

\proclaim Corolário. Se um aberto $U\subset\R^n$ é conexo então
ele é conexo por caminhos.\fimprova

\smallskip

\noindent{\tt Exemplo}. Existem espaços conexos que não são
conexos por caminhos. Um exemplo famoso é a {\sl senóide dos
topólogos\/} definida da seguinte maneira: seja $S\subset\R^2$ o
gráfico da função $\left(0,1\right]\ni t\mapsto\sen{1\over t}$,
i.e.:
$$S=\big\{\big(t,\sen{\textstyle{1\over
t}}\big):t\in\left(0,1\right]\big\}.$$ Temos que $S$ é conexo pois
é a imagem do conexo $\left(0,1\right]$ pela função contínua
$t\mapsto\big(t,\sen{1\over t}\big)$. A senóide dos topólogos é
por definição o fecho de $S$, ou seja:
$$\overline
S=S\cup\big(\{0\}\times[-1,1]\big).\qquad\hbox{(verifique!)}$$
Temos que $\overline S$ é conexo pois é o fecho de um conexo.
Ocorre que não existe um caminho contínuo em $\overline S$ ligando
um ponto de $S$ a um ponto do segmento vertical
$\{0\}\times[-1,1]$ (veja o Exercício 10).

\medskip

\item{(2)} {\bf Espaços compactos}.

\smallskip

Seja $(M,d)$ um espaço métrico. Uma {\sl cobertura\/} de $M$ é uma
família $(U_i)_{i\in I}$ de subconjuntos de $M$ com
$M=\bigcup_{i\in I}U_i$; dizemos que $(U_i)_{i\in I}$ é uma {\sl
cobertura aberta\/} de $M$ se cada $U_i$ for aberto em $M$. Uma
{\sl subcobertura\/} de $(U_i)_{i\in I}$ é uma família da forma
$(U_i)_{i\in J}$ com $J\subset I$ tal que $M=\bigcup_{i\in J}U_i$.

\proclaim Definição. Um espaço métrico $M$ é dito {\rm compacto}
quando toda cobertura aberta de $M$ admite uma subcobertura
finita, i.e., se $M=\bigcup_{i\in I}U_i$ com cada $U_i\subset M$
aberto então existem $i_1,\ldots,i_n\in I$ com
$M=\bigcup_{k=1}^nU_{i_k}$.

\smallskip

\noindent{\tt Observação}. Assim como a noção de espaço conexo, a
noção de espaço compacto é intrínseca, i.e., não faz referência a
um ambiente aonde o espaço está imerso -- um subconjunto $K$ de um
espaço métrico $M$ é dito {\sl compacto\/} quando $K$ é um espaço
métrico compacto munido da métrica induzida de $M$. Assim,
$K\subset M$ é compacto significa que toda cobertura aberta de $K$
{\sl por abertos relativos a $K$\/} admite uma subcobertura
finita. No entanto, a noção intrínseca de compacto é {\sl
equivalente\/} à noção {\sl extrínseca\/} de compacto dada no
seguinte: \proclaim Teorema. Seja $M$ um espaço métrico e
$K\subset M$ um subconjunto. Então $K$ é um espaço compacto (com a
métrica induzida de $M$) se e somente se toda cobertura aberta de
$K$ por abertos de $M$ admite uma subcobertura finita, i.e., dada
uma família $(U_i)_{i\in I}$ de abertos de $M$ com
$K\subset\bigcup_{i\in I}U_i$ então existem $i_1,\ldots,i_n\in I$
com $K\subset\bigcup_{k=1}^nU_{i_k}$.

\Prova Suponha que $K$ é compacto (na métrica induzida) e seja
$K\subset\bigcup_{i\in I}U_i$ uma cobertura de $K$ por abertos de
$M$. Daí $K=\bigcup_{i\in I}(U_i\cap K)$ é uma cobertura aberta de
$K$ (por abertos de $K$) e portanto existem $i_1,\ldots,i_n$ com
$K=\bigcup_{k=1}^n(U_{i_k}\cap K)$; daí
$K\subset\bigcup_{k=1}^nU_{i_k}$. Reciprocamente, suponha que toda
cobertura aberta de $K$ por abertos de $M$ admite uma subcobertura
finita; seja $K=\bigcup_{i\in I}Z_i$ uma cobertura de $K$ por
abertos de $K$. Cada $Z_i$ é da forma $U_i\cap K$ com $U_i$ aberto
em $M$; daí $K\subset\bigcup_{i\in I}U_i$ é uma cobertura de $K$
por abertos de $M$ e portanto existem $i_1,\ldots,i_n$ com
$K\subset\bigcup_{k=1}^nU_{i_k}$. Daí
$K=\bigcup_{k=1}^n(U_{i_k}\cap K)=\bigcup_{k=1}^nZ_{i_k}$ e
portanto $K$ é compacto.\fimprova

\smallskip

\noindent{\tt Observação}. Compacidade é uma noção topológica.

\smallskip

\proclaim Teorema. Sejam $M$, $N$ espaços métricos. Se $f:M\to N$
é uma função contínua e $M$ é compacto então a imagem de $f$ é
compacta (``a imagem de um compacto por uma função contínua é
compacta'').

\Prova Trocando $N$ por $f(M)$ (o que não afeta a continuidade de
$f$) podemos supor sem perda de generalidade que $f$ é
sobrejetora. Seja $N=\bigcup_{i\in I}U_i$ uma cobertura aberta de
$N$; daí $M=\bigcup_{i\in I}f^{-1}(U_i)$ é uma cobertura aberta de
$M$, da qual extraímos uma subcobertura finita
$M=\bigcup_{k=1}^nf^{-1}(U_{i_k})$. Daí
$N=\bigcup_{k=1}^nU_{i_k}$.\fimprova

\smallskip

\proclaim Teorema. Se $M$ é compacto então $M$ é limitado.

\Prova Fixe $x\in M$ (o caso $M=\emptyset$ é trivial); temos que
$M=\bigcup_{n\in\N}\Bola(x;n)$ é uma cobertura aberta de $M$ e
portanto existem $n_1,\ldots,n_k\in\N$ com
$M=\bigcup_{i=1}^k\Bola(x;n_i)$. Tomando
$n=\max\{n_1,\ldots,n_k\}$ então $M=\Bola(x;n)$.\fimprova

\smallskip

\proclaim Corolário. Se $f:M\to N$ é contínua e $M$ é compacto
então $f$ é limitada.\fimprova

\smallskip

\proclaim Teorema. Se $(M,d)$ é um espaço métrico e $K\subset M$ é
compacto então $K$ é fechado em $M$.

\Prova Seja $x\in M$ com $x\not\in K$. Para cada $y\in K$, como
$x\ne y$, existem abertos disjuntos $U_y,V_y\subset M$ com $y\in
U_y$ e $x\in V_y$ (por exemplo, tome $U_y=\Bola(y;r)$ e
$V_y=\Bola(x;r)$ com $r={1\over2}d(x,y)>0$). Daí
$K\subset\bigcup_{y\in K}U_y$ é uma cobertura aberta (por abertos
de $M$!) e portanto podemos extrair uma subcobertura finita
$K\subset\bigcup_{i=1}^nU_{y_i}$. Seja $V=\bigcap_{i=1}^nV_{y_i}$;
obviamente $V$ é uma vizinhança aberta de $x$. Além do mais, $V$ é
disjunto de $K$ (na verdade $V$ é disjunto até de
$\bigcup_{i=1}^nU_{y_i}\supset K$). Daí $x$ não é um ponto de
aderência de $K$ e $K$ é fechado em $M$.\fimprova

\smallskip

\proclaim Corolário. Se $M$ é compacto não vazio e $f:M\to\R$ é
contínua então $f$ assume máximo e mínimo em $M$.

\Prova Como $f(M)\subset\R$ é fechado e limitado temos $\sup
f(M)\in f(M)$ e $\inf f(M)\in f(M)$.\fimprova

\smallskip

\proclaim Teorema. Se $(M,d)$ é compacto e $K\subset M$ é fechado então $K$ é
compacto.

\Prova Seja $K\subset\bigcup_{i\in I}U_i$ uma cobertura aberta de
$K$ por abertos de $M$; daí $M=(M\setminus K)\cup\bigcup_{i\in
I}U_i$ é uma cobertura aberta de $M$, já que $K$ é fechado.
Obtemos então uma subcobertura finita dessa última que pode ou não
envolver $M\setminus K$; de qualquer modo, $M\setminus K$ não
cobre nenhum ponto de $K$ e portanto obtivemos na verdade uma
cobertura finita de $K$ por alguns $U_i$'s.\fimprova

\smallskip

\proclaim Teorema. (Heine--Borel) Um subconjunto de $\R$ é
compacto se e somente se for fechado e limitado.

\Prova Em vista dos teoremas anteriores, basta provar que um
intervalo fechado $[a,b]$ é compacto. Seja
$[a,b]\subset\bigcup_{i\in I}U_i$ uma cobertura de $[a,b]$ por
abertos de $\R$; suponha por absurdo que ela não possui uma
subcobertura finita. Divida o intervalo fechado $I_0=[a,b]$ no
meio; uma das duas metades não pode ser coberta por um número
finito de $U_i$'s (senão $I_0$ poderia) --- denote essa metade por
$I_1$. Agora divida $I_1$ no meio e denote por $I_2$ uma metade
que não pode ser coberta por um número finito de $U_i$'s. Seguindo
com esse processo, obteremos uma seqüência decrescente $I_0\supset
I_1\supset I_2\supset I_3\supset\cdots$ de intervalos fechados com
$\diam(I_k)={1\over2^k}\diam(I_0)$, onde cada $I_k$ não pode ser
coberto por um número finito de $U_i$'s. Pelo princípio dos
intervalos encaixantes, existe $c\in\bigcap_{k\ge1}I_k$; como
$c\in I_0$, existe $U_i$ com $c\in U_i$. Como $\diam(I_k)\to0$,
existe $I_k$ com $c\in I_k\subset U_i$, contradizendo o fato que
$I_k$ não pode ser coberto por um número finito de
$U_i$'s.\fimprova

\smallskip

\proclaim Corolário. (o teorema de Weierstrass) Uma função
contínua $f:[a,b]\to\R$ é limitada e assume máximo e
mínimo.\fimprova

\smallskip

A demonstração do teorema de Heine--Borel motiva a introdução da
seguinte:

\proclaim Definição. Um espaço métrico $M$ é dito {\rm totalmente
limitado} quando dado $\varepsilon>0$ então $M$ admite uma
cobertura finita por subconjuntos de diâmetro menor que
$\varepsilon$, i.e., existem $M_1,\ldots,M_n\subset M$ com cada
$\diam(M_i)<\varepsilon$ e $M=\bigcup_{i=1}^nM_i$. Um subconjunto
$S$ de um espaço métrico é dito {\rm totalmente limitado} quando
$S$ for um espaço métrico totalmente limitado munido da métrica
induzida de $M$ (``totalmente limitado'' é uma noção intrínseca).

\smallskip

\noindent{\tt Exemplo}. Todo subconjunto limitado de $\R$ é
totalmente limitado. De fato, todo subconjunto limitado de $\R$ está contido
num intervalo $[a,b]$ e para todo $\varepsilon>0$, o
intervalo $[a,b]$ pode ser particionado em um número finito de
intervalos de comprimento menor que $\varepsilon$.

\smallskip

\noindent{\tt Observação}. Obviamente todo espaço totalmente limitado é
limitado, de modo que espaços não limitados fornecem exemplos de espaços que
não são totalmente limitados. Qual seria um exemplo de um espaço limitado que
não é totalmente limitado? Bom, generalizando o exemplo acima não é difícil
ver que todo subconjunto limitado de $\R^n$ é totalmente limitado. Para
encontrar um exemplo de espaço limitado que não é totalmente limitado
precisamos então procurar fora de $\R^n$ (o que na verdade foge um pouco do
espírito do curso) --- o exemplo é o seguinte: considere um conjunto infinito
$M$ munido da métrica zero-um (um outro exemplo seria uma bola de um espaço
normado de dimensão infinita, mas isso é outra história).

\smallskip

\proclaim Teorema. Seja $M$ um espaço métrico. As seguintes
condições são equivalentes:
\item{(i)} $M$ é compacto;
\item{(ii)} toda seqüência em $M$ possui uma subseqüência
convergente (em $M$);
\item{(iii)} $M$ é completo e totalmente limitado.

\Prova

\noindent (i)$\Rightarrow$(ii).\enspace Seja $(x_n)_{n\ge1}$ uma
seqüência em $M$. Suponha por absurdo que $(x_n)_{n\ge1}$ não
possui subseqüência convergente, i.e., que $(x_n)_{n\ge1}$ não
possui pontos aderentes (veja o Exercício~13). Daí, para todo
$y\in M$ temos que $y$ não é um ponto aderente de $(x_n)_{n\ge1}$
e portanto (negue a definição!) existe um aberto $U_y$ em $M$
contendo $y$ tal que $x_n\not\in U_y$ para todo $n$
suficientemente grande (i.e., o conjunto $\big\{n\in\N:x_n\in
U_y\big\}$ é finito). A cobertura aberta $M=\bigcup_{y\in M}U_y$
admite uma subcobertura finita $M=\bigcup_{i=1}^kU_{y_i}$; daí:
$$\N=\big\{n\in\N:x_n\in
M\big\}=\bigcup_{i=1}^k\big\{n\in\N:x_n\in U_{y_i}\big\}$$ é
finito, uma contradição.

\noindent (ii)$\Rightarrow$(iii).\enspace É fácil ver que se uma seqüência de
Cauchy possui uma subseqüência convergente então ela mesmo é convergente;
portanto $M$ é completo. Seja agora $\varepsilon>0$ e vamos mostrar que $M$
pode ser coberto por um número finito de subconjuntos de diâmetro menor que
$\varepsilon$. Suponha que não. Daí $M\ne\emptyset$ e podemos escolher $x_1\in
M$; a bola $\Bola\big(x_1;{\varepsilon\over3}\big)$ tem diâmetro menor que
$\varepsilon$ e portanto não pode cobrir $M$ -- podemos então encontrar
$x_2\in M$ fora de $\Bola\big(x_1;{\varepsilon\over3}\big)$. Analogamente, as
bolas $\Bola\big(x_1;{\varepsilon\over3}\big)$ e
$\Bola\big(x_2;{\varepsilon\over3}\big)$ possuem diâmetro menor que
$\varepsilon$ e portanto não podem cobrir $M$; podemos encontrar então $x_3\in
M$ fora dessas bolas. Prosseguindo a construção indutivamente, supondo
$x_1,\ldots,x_k$ construídos então como
$\bigcup_{i=1}^k\Bola\big(x_i;{\varepsilon\over3}\big)$ não pode ser $M$,
podemos encontrar $x_{k+1}$ fora da união dessas $k$ bolas. Obtemos assim uma
seqüência $(x_n)_{n\ge1}$ em $M$ com $d(x_n,x_m)\ge{\varepsilon\over3}$ para
todo $n\ne m$ e tal seqüência não pode ter subseqüência convergente, uma
contradição.

\noindent (iii)$\Rightarrow$(i).\enspace (essa parte imita a demonstração do
teorema de Heine-Borel) Seja $M=\bigcup_{i\in
I}U_i$ uma cobertura aberta de $M$. Suponha por absurdo que tal
cobertura não admite uma subcobertura finita. Dado $\varepsilon=1$
então $M$ pode ser coberto por um número finito de subconjuntos de
diâmetro menor que $1$; podemos supor que tais subconjuntos são
fechados (pois $\diam(S)=\diam(\overline S)$; veja o Exercício
12). Se todos esses conjuntos pudessem ser cobertos por um número
finito de $U_i$'s então também $M$ poderia --- logo algum desses
fechados, digamos $F_1$, não admite uma subcobertura finita (em
particular $F_1\ne\emptyset$). Temos que também $F_1$ é totalmente
limitado, já que $F_1$ é um subespaço de $M$; logo, dado
$\varepsilon={1\over2}$ podemos cobrir $F_1$ por um número finito
de fechados de diâmetro menor que ${1\over2}$. Novamente, um
desses fechados, digamos $F_2\subset F_1$, não pode ser coberto
por um número finito de $U_i$'s. Seguindo o raciocício
indutivamente, construímos uma seqüência decrescente de fechados
não vazios $F_1\supset F_2\supset F_3\supset\cdots$ onde
$\diam(F_k)<{1\over k}$ para todo $k\ge1$ e nenhum $F_k$ pode ser
coberto por um número finito de $U_i$'s. Como $M$ é completo (veja
o Exercício 14) existe $x\in\bigcap_{k\in\N}F_k$; tal $x$ pertence
a algum $U_i$ e como $\diam(F_k)\to0$ devemos ter $F_k\subset U_i$
para algum $k$, contradizendo o fato que $F_k$ não pode ser
coberto por um número finito de $U_i$'s.\fimprova

\smallskip

\proclaim Corolário. O produto $M_1\times\cdots\times M_n$ de
espaços métricos compactos $(M_i,d_i)$ é compacto.

\Prova Podemos supor que $n=2$; o caso geral segue indutivamente
(já que o produto cartesiano de espaços métricos é associativo).
Suponha que $M_1$ e $M_2$ são compactos; vamos mostrar que toda
seqüência em $M_1\times M_2$ admite uma subseqüência convergente.
Seja $\big((x_m,y_m)\big)_{m\ge1}$ uma seqüência com $x_m\in M_1$
e $y_m\in M_2$ para todo $m$. Como $M_1$ é compacto, podemos
extrair uma subseqüência convergente $(x_{m_k})_{k\ge1}$ de
$(x_m)_{m\ge1}$; da seqüência $(y_{m_k})_{k\ge1}$ em $M_2$ podemos
também extrair uma subseqüência convergente
$(y_{m_{k_i}})_{i\ge1}$, já que $M_2$ também é compacto. Logo,
$\big((x_{m_{k_i}},y_{m_{k_i}})\big)_{i\ge1}$ é uma subseqüência
convergente de $\big((x_m,y_m)\big)_{m\ge1}$ em $M_1\times
M_2$.\fimprova

\smallskip

\proclaim Corolário. Um subconjunto de $\R^n$ é compacto se e
somente se é limitado e fechado.

\Prova Basta ver que blocos retangulares fechados
$\prod_{i=1}^n[a_i,b_i]$ são compactos; isso segue do Corolário
anterior e do teorema de Heine-Borel.\fimprova

\smallskip

\noindent{\tt Observação}. Como já havíamos prometido, a teoria
desta seção produz uma demonstração que $\R$ é completo. De fato,
se $(x_n)_{n\ge1}$ é uma seqüência de Cauchy em $\R$ então
$(x_n)_{n\ge1}$ fica em algum intervalo $[a,b]$, o qual é completo
(pois é compacto).

\smallskip

\noindent{\tt Observação}. Para espaços métricos quaisquer, nem de
longe é verdade que
$\hbox{limitado}+\hbox{fechado}\Rightarrow\hbox{compacto}$. Por
exemplo, se $M\subset\R^n$ é limitado mas não fechado então $M$ é
limitado e fechado em si próprio, mas não é compacto (para os que
conhecem um pouco de Análise Funcional: num espaço de Banach de
dimensão infinita, {\sl nenhuma\/} bola fechada é compacta, apesar do espaço
ser completo).

\smallskip

\proclaim Teorema. Sejam $M$, $N$ espaços métricos e $f:M\to N$
uma função contínua. Se $M$ é compacto então $f$ é uniformemente
contínua (``uma função contínua num compacto é uniformemente
contínua'').

\Prova Se não fosse, existiria $\varepsilon>0$ tal que para todo
$\delta={1\over n}$, poderíamos encontrar $x_n,y_n\in M$ com
$d(x_n,y_n)<{1\over n}$ mas
$d\big(f(x_n),f(y_n)\big)\ge\varepsilon$. Como $M$ é compacto, a
seqüência $(x_n)_{n\ge1}$ possui uma subseqüência convergente
$(x_{n_k})_{k\ge1}$, digamos $x_{n_k}\to x$. Como
$d(x_{n_k},y_{n_k})<{1\over n_k}\to0$, temos também $y_{n_k}\to
x$. Daí, pela continuidade de $f$, $f(x_{n_k})\to f(x)$ e
$f(y_{n_k})\to f(x)$, contradizendo
$d\big(f(x_{n_k}),f(y_{n_k})\big)\ge\varepsilon$ para todo
$k$.\fimprova

\smallskip

\proclaim Definição. Sejam $M$ um espaço métrico e
$M=\bigcup_{i\in I}U_i$ uma cobertura de $M$ (não necessariamente
aberta). Um número positivo $\delta>0$ é chamado um {\rm número de
Lebesgue} para a cobertura $M=\bigcup_{i\in I}U_i$ se todo
subconjunto $S\subset M$ com diâmetro menor que $\delta$ está
contido em algum $U_i$.

\smallskip

A importância da definição acima está no seguinte:

\proclaim Teorema. Se $M$ é um espaço métrico compacto então toda
cobertura aberta $M=\bigcup_{i\in I}U_i$ $(I\ne\emptyset$) admite
um número de Lebesgue.

\Prova Para cada $x\in M$ existe $i\in I$ com $x\in U_i$; como
$U_i$ é aberto podemos encontrar $r_x>0$ com $\Bola(x;r_x)\subset
U_i$. Da cobertura aberta $M=\bigcup_{x\in
M}\Bola\big(x;{r_x\over2}\big)$ podemos extrair uma subcobertura
finita $M=\bigcup_{k=1}^n\Bola\big(x_k;{r_{x_k}\over2}\big)$; tome
$\delta={1\over2}\min\{r_{x_1},\ldots,r_{x_n}\}>0$. Vamos mostrar
que $\delta$ é um número de Lebesgue para $M=\bigcup_{i\in I}U_i$.
Seja $S\subset M$ com $\diam(S)<\delta$; escolha $x\in S$ (o caso
$S=\emptyset$ é trivial). Temos
$x\in\Bola\big(x_k;{r_{x_k}\over2}\big)$ para algum $k=1,\ldots,n$
e $\Bola(x_k;r_{x_k})\subset U_i$ para algum $i\in
I$. Daí, para todo $y\in S$ temos:
$$d(x_k,y)\le d(x_k,x)+d(x,y)<{r_{x_k}\over2}+\delta\le r_{x_k},$$
donde $y\in\Bola(x_k;r_{x_k})\subset U_i$. Daí $S\subset
U_i$.\fimprova

\medskip

\item{(3)} {\bf Base enumerável e separabilidade}.

\smallskip

\noindent{\tt Observação}. Espero que todo mundo esteja
familiarizado com o conceito de conjunto enumerável (um {\sl
conjunto enumerável\/} é um conjunto que é finito ou que admite
uma bijeção com $\N$) --- espero também que todo mundo saiba
coisas como ``a união enumerável de enumeráveis é enumerável'',
``o produto finito de enumeráveis é enumerável'', ``$\Q$ é
enumerável'' e ``$\R$ não é enumerável''.

\smallskip

\proclaim Definição. Seja $M$ um espaço métrico. Uma {\rm base de abertos}
para $M$ é uma coleção $\Bfrak$ de abertos de $M$ tal que todo aberto de $M$
pode ser escrito como uma reunião de abertos de $M$ pertencentes à coleção
$\Bfrak$. Quando $M$ admite uma base de abertos enumerável então diz-se que
$M$ é um {\rm espaço com base enumerável} ou um {\rm espaço que satisfaz o
segundo axioma da enumerabilidade} (em inglês é mais curto: diz-se {\rm second
countable space}).

No Exercício 24 apresentamos uma maneira equivalente (e talvez
mais simples) de definir base de abertos.

\smallskip

\noindent{\tt Observação}. Como vocês podem imaginar, o nome
``segundo axioma da enumerabilidade'' vem do fato que existe um
``primeiro axioma da enumerabilidade''; esse axioma só aparece em
cursos de topologia geral, já que o mesmo é automaticamente
satisfeito em todo espaço métrico (para os curiosos: o primeiro
axioma da enumerabilidade diz que todo ponto possui um sistema
fundamental de vizinhanças enumerável).

\smallskip

\noindent{\tt Exemplo}. Os intervalos abertos $(a,b)\subset\R$ com $a$ e $b$
racionais formam uma base de abertos enumerável para o espaço métrico $\R$
(verifique!).

\smallskip

\proclaim Teorema. Sejam $M$ um espaço métrico, $S\subset M$ um
subconjunto e $\Bfrak$ uma base de abertos para $M$. Então:
$$\Bfrak_S=\big\{B\cap S:B\in\Bfrak\big\}$$
é uma base de abertos para $S$. Em particular, se $M$ admite uma
base enumerável então também $S$ admite uma base enumerável
(diz-se que a propriedade de existência de base enumerável é {\rm
hereditária para subespaços}).

\Prova É claro que $\Bfrak_S$ é um conjunto de abertos relativos a
$S$. Seja $Z$ um aberto relativo a $S$; temos $Z=U\cap S$ com $U$
aberto em $M$. Podemos escrever $U=\bigcup_{i\in I}B_i$ com cada
$B_i\in\Bfrak$; daí $Z=U\cap S=\bigcup_{i\in I}(B_i\cap S)$ e cada
$B_i\cap S$ está em $\Bfrak_S$.\fimprova

\smallskip

\proclaim Definição. Um espaço métrico $M$ é dito {\rm separável} quando o
mesmo admite um subconjunto enumerável denso (diz-se também que $M$ satisfaz o
{\rm terceiro axioma da enumerabilidade}).

\smallskip

\noindent{\tt Observação}. Existência de base enumerável e
separabilidade são propriedades intrínsecas e topológicas.

\smallskip

\noindent{\tt Exemplo}. $\R$ é separável pois $\Q$ é um subconjunto enumerável
denso de $\R$. Também $\R^n$ é separável pois $\Q^n$ é um subconjunto
enumerável denso de $\R^n$.

\smallskip

\proclaim Teorema. Um espaço métrico $M$ admite base enumerável de abertos se
e somente se for separável.

\Prova Suponha que $M$ admite uma base enumerável $\Bfrak$ de
abertos, digamos $\Bfrak=\{B_n\}_{n\ge1}$ (para facilitar a
notação vamos fazer de conta que $\Bfrak$ é infinito, mas na
realidade $\Bfrak$ poderia ser finito, tanto faz). Podemos supor
que todos os $B_n$'s são não vazios (se algum $B_n$ for
vazio, jogue ele fora); podemos então escolher $x_n\in B_n$
para todo $n$. Daí $D=\{x_n\}_{n\ge1}$ é um subconjunto enumerável
denso de $M$; de fato, se $U\subset M$ é um aberto não vazio então
existe $n\in\N$ com $B_n\subset U$ e daí $x_n\in D\cap U$.
Logo $M$ é separável.

Reciprocamente, suponha que $M$ é separável. Seja $D=\{x_n\}_{n\ge1}$ um
subconjunto enumerável denso de $M$ (de novo vamos fazer de conta que $D$ é
infinito para facilitar a notação, mas não tem problema se $D$ for
finito). Defina:
$$\Bfrak=\big\{\Bola(x_n;q):n\in\N,\ q\in\Q,\ q>0\big\};$$
é claro que $\Bfrak$ é um conjunto enumerável de abertos de $M$. Vamos mostrar
que $\Bfrak$ é uma base. Seja $U\subset M$ aberto e seja $x\in U$; devemos
encontrar um elemento de $\Bfrak$ que contém $x$ e que esteja contido em $U$,
i.e., devemos encontrar $n\in\N$ e $q$ racional positivo com
$x\in\Bola(x_n;q)\subset U$. Como $U$ é aberto, existe $r>0$
com $\Bola(x;r)\subset U$. Como $D$ é denso, existe $n\in\N$ com
$x_n\in\Bola\big(x;{r\over3}\big)$; obviamente existe $q\in\Q$ com
${r\over3}<q<{r\over2}$ e daí $x\in\Bola(x_n;q)$ (pois $d(x,x_n)<{r\over3}<q$)
e $\Bola(x_n;q)\subset\Bola(x;r)\subset U$, pois
$d(x_n,x)+q<{r\over3}+{r\over2}<r$.\fimprova

\smallskip

\proclaim Corolário. $\R^n$ (e todos os seus subespaços) admitem base
enumerável de abertos.\fimprova

\smallskip

\noindent{\tt Observação}. Na teoria geral dos espaços
topológicos, não é verdade que separável implica existência de
base enumerável; é por isso que os dois nomes diferentes existem.
Como mostramos, no contexto de espaços métricos (que é o que nos
interessa), existência de base enumerável é o mesmo que
separabilidade.

\smallskip

\proclaim Teorema de Lindelöf. Seja $M$ um espaço métrico separável (ou que
admita base enumerável, tanto faz). Então
toda cobertura aberta $M=\bigcup_{i\in I}U_i$ de $M$ admite uma subcobertura
enumerável, i.e., existe $J\subset I$ enumerável com $M=\bigcup_{j\in J}U_j$.

\Prova Seja $\Bfrak$ uma base enumerável de abertos, digamos
$\Bfrak=\{B_n\}_{n\ge1}$; seja $M=\bigcup_{i\in I}U_i$ uma cobertura aberta de
$M$. Para cada $n\in\N$ escolha, se existir, $i_n\in I$ com $B_n\subset
U_{i_n}$; se tal $i_n$ não existir, não escolha nada. Seja $J\subset I$ o
conjunto dos $i_n$'s que foram escolhidos nesse processo; obviamente $J$ é
enumerável. Vamos mostrar que $M=\bigcup_{j\in J}U_j$. Seja $x\in M$; sabemos
que existe $i\in I$ com $x\in U_i$. Como $\Bfrak$ é uma base de abertos e
$U_i$ é aberto, existe $n\in\N$ com $x\in B_n\subset U_i$. Daí esse $n$ é tal
que $B_n$ está contido em algum dos $U_i$'s; portanto, na
parte inicial onde fizemos as escolhas dos $i_n$'s, necessariamente escolhemos
um $i_n$ com $B_n\subset U_{i_n}$ (pode ser que $i_n=i$ ou não!). Em qualquer
caso, temos $i_n\in J$ e $x\in B_n\subset U_{i_n}$; em particular
$x\in\bigcup_{j\in J}U_j$.\fimprova

\smallskip

\noindent{\tt Observação}. De modo análogo ao que foi discutido na
seção sobre espaços compactos, temos que para um subconjunto $S$
de um espaço métrico $M$ são equivalentes:
\item{(i)} toda cobertura de $S$ por abertos de $S$ admite uma
subcobertura enumerável;
\item{(ii)} toda cobertura de $S$ por abertos de $M$ admite uma
subcobertura enumerável.

\smallskip

\proclaim Corolário. Dada uma cobertura aberta de um subconjunto
$S\subset\R^n$ (por abertos relativos a $S$ ou por abertos de $\R^n$) então
tal cobertura possui uma subcobertura enumerável.\fimprova

\medskip

\item{(4)} {\bf Espaços normados e espaços com produto interno}.

\smallskip

Nesta seção $V$ denota um espaço vetorial real.

\proclaim Definição. Uma {\rm norma} em $V$ é uma aplicação $V\ni
x\mapsto\Vert x\Vert\in\R$ satisfazendo as seguites propriedades:
\item{(N1)}$\Vert x\Vert\ge0$ para todo $x\in V$ e $\Vert
x\Vert=0$ se e somente se $x=0$;
\item{(N2)}$\Vert\lambda x\Vert=\vert\lambda\vert\Vert x\Vert$
para todo $x\in V$ e todo $\lambda\in\R$;
\item{(N3)}$\Vert x+y\Vert\le\Vert x\Vert+\Vert y\Vert$ para todos
$x,y\in V$ {\rm (desigualdade triangular)}.

\smallskip

É fácil ver que se $\norma$ é uma norma em $V$ então a fórmula:
$$d(x,y)=\Vert x-y\Vert,\quad x,y\in V,$$
define uma métrica em $V$; dizemos que $d$ é a {\sl métrica
induzida\/} pela norma $\norma$. Todo espaço vetorial normado será
pensado sempre como um espaço métrico munido da métrica acima.

\smallskip

\proclaim Definição. Duas normas $\norma$ e $\norma'$ são ditas
{\rm equivalentes} quando as métricas correspondentes são
equivalentes.

\smallskip

\proclaim Teorema. Sejam $V$, $W$ espaços normados e $T:V\to W$
uma aplicação linear. As seguintes afirmações são equivalentes:
\item{(i)} $T$ é contínua;
\item{(ii)} $T$ é contínua na origem;
\item{(iii)} $T$ é limitada na bola unitária $\Bola[0;1]$ de $V$;
\item{(iv)} existe uma constante $c\ge0$ tal que $\Vert T(x)\Vert\le c\Vert
x\Vert$ para todo $x\in V$;
\item{(v)} $T$ é Lipschitziana.

\Prova

\noindent (i)$\Rightarrow$(ii).\enspace É óbvio.

\noindent (ii)$\Rightarrow$(iii).\enspace Dado $\varepsilon=1$
existe $\delta>0$ tal que $\Vert x\Vert<\delta$ implica $\Vert
T(x)\Vert<1$. Daí para todo $x\in\Bola[0;1]\subset V$ temos
$\big\Vert{\delta\over2}x\big\Vert<\delta$ e portanto $\big\Vert
T\big({\delta\over2}x\big)\big\Vert<1$; daí $\Vert
T(x)\Vert<{2\over\delta}$ para todo $x\in\Bola[0;1]$.

\noindent (iii)$\Rightarrow$(iv).\enspace Tome $c=\sup_{\Vert
x\Vert\le1}\Vert T(x)\Vert<+\infty$. Daí $\big\Vert
T\big({x\over\Vert x\Vert}\big)\big\Vert\le c$ para todo $x\ne0$ e
portanto $\Vert T(x)\Vert\le c\Vert x\Vert$ para todo $x\in V$ (o
caso $x=0$ é trivial).

\noindent (iv)$\Rightarrow$(v).\enspace $\Vert
T(x)-T(y)\Vert=\Vert T(x-y)\Vert\le c\Vert x-y\Vert$.

\noindent (v)$\Rightarrow$(i).\enspace É óbvio.\fimprova

\smallskip

\proclaim Corolário. Duas normas $\norma$ e $\norma'$ são
equivalentes se e somente se existem constantes $\alpha,\beta>0$
tais que:
$$\alpha\Vert x\Vert\le\Vert x\Vert'\le\beta\Vert x\Vert,$$
para todo $x\in V$. Em particular, as métricas associadas a duas
normas equivalentes são também uniformemente equivalentes (até
mesmo ``Lipschitz equivalentes'').

\Prova As aplicações identidade
$$\Id:(V,\norma)\longrightarrow(V,\norma'),\quad\Id:(V,\norma')\longrightarrow(V,\norma)$$
são lineares e portanto são
contínuas se e somente se são Lipschitzianas.\fimprova

\smallskip

\noindent{\tt Exemplo}. A norma Euclideana $\Vert
x\Vert=\big(\sum_{i=1}^nx_i^2\big)^{1\over2}$ é uma norma em
$\R^n$; a demonstração da desigualdade triangular será feita logo
adiante. As fórmulas:
$$\Vert x\Vert_1=\sum_{i=1}^n\vert x_i\vert,\quad\Vert
x\Vert_\infty=\max\big\{\vert x_1\vert,\ldots,\vert
x_n\vert\big\},$$ definem normas em $\R^n$, ambas equivalentes à
norma Euclideana (recorde Exercício 32 da aula número 5).

\smallskip

\proclaim Teorema. Seja $V$ um espaço vetorial normado. A norma de $V$ é uma
contração fraca de $(V,\norma)$ em $\R$. Em particular a norma de $V$ é
uniformemente contínua.

\Prova É fácil ver que a desigualdade:
$$\big\vert\Vert x\Vert-\Vert y\Vert\big\vert\le\Vert x-y\Vert=d(x,y),\quad
x,y\in V,$$
segue da desigualdade triangular para $\norma$.\fimprova

\smallskip

\proclaim Teorema. Todas as normas num espaço vetorial $V$ de
dimensão finita são equivalentes.

\Prova Podemos supor sem perda de generalidade que $V=\R^n$ (veja
também a observação a seguir). Seja $\norma'$ uma norma arbitrária
em $\R^n$; por transitividade, basta mostrar que $\norma'$ é equivalente à norma
$\Vert\cdot\Vert_1$ definida acima. Se $(e_1,\ldots,e_n)$ denota a
base canônica de $\R^n$ então:
$$\Vert
x\Vert'=\Big\Vert{\textstyle\sum_{i=1}^n}x_ie_i\Big\Vert'\le\sum_{i=1}^n\vert
x_i\vert\,\Vert e_i\Vert'\le k\Vert x\Vert_1,$$ para todo $x\in\R^n$, onde
$k=\max\big\{\Vert e_1\Vert',\ldots,\Vert e_n\Vert'\big\}$. Isso
mostra que a aplicação identidade
$$\Id:(\R^n,\Vert\cdot\Vert_1)\longrightarrow(\R^n,\norma')$$ é contínua; segue
então também que a aplicação $(\R^n,\Vert\cdot\Vert_1)\ni
x\mapsto\Vert x\Vert'\in\R$ é contínua (pois é uma composta de funções
contínuas). Como a esfera unitária
$\big\{x\in\R^n:\Vert x\Vert_1=1\big\}$ de
$(\R^n,\Vert\cdot\Vert_1)$ é compacta em
$(\R^n,\Vert\cdot\Vert_1)$, segue que $x\mapsto\Vert x\Vert'$
assume um valor mínimo $\alpha>0$ em $\big\{x\in\R^n:\Vert
x\Vert_1=1\big\}$. Daí, para todo $x\in\R^n$ não nulo temos:
$$\left\Vert{x\over\Vert
x\Vert_1}\right\Vert'\ge\alpha\Longrightarrow\Vert
x\Vert_1\le{1\over\alpha}\Vert x\Vert',$$ donde
$\Id:(V,\norma')\to(V,\Vert\cdot\Vert_1)$ é contínua.\fimprova

\smallskip

\proclaim Corolário. Dados espaços normados $V$ e $W$, se $V$ tem
dimensão finita, então todo operador linear $T:V\to W$ é contínuo.

\Prova Trocando $W$ por $T(V)$ (o que não afeta a continuidade de
$T$), podemos supor sem perda de generalidade que $T$ é
sobrejetora; daí $W$ também tem dimensão finita. Pelo Teorema
acima, podemos supor também que $V=\R^n$, $W=\R^m$ e que ambos
possuem a métrica Euclidena. Daí cada coordenada de $T$ é uma
combinação linear de projeções de $\R^n$ e portanto $T$ é
contínua.\fimprova

\smallskip

\noindent{\tt Observação}. Vamos entender essa estória de ``supor
sem perda de generalidade''. Quando queremos provar um teorema do
tipo $h\Rightarrow t$ e dizemos que ``é possível supor sem perda
de generalidade que vale $h'$'', isso significa que podemos provar
o teorema desejado $h\Rightarrow t$ a partir do teorema mais fraco
$h+h'\Rightarrow t$. Por exemplo, acima mostramos que duas normas
quaisquer em $\R^n$ são equivalentes; como podemos de fato
concluir a partir daí que duas normas num espaço vetorial de
dimensão finita qualquer são equivalentes? Fazemos o seguinte:
seja $V$ um espaço vetorial de dimensão $n<+\infty$ e sejam
$\norma$ e $\norma'$ normas em $V$. Sabemos da álgebra linear que
existe um isomorfismo linear $\phi:\R^n\to V$; tal isomorfismo
induz normas $\norma_\phi$ e $\norma'_\phi$ em $\R^n$ fazendo:
$$\Vert x\Vert_\phi=\Vert\phi(x)\Vert,\quad\Vert
x\Vert'_\phi=\Vert\phi(x)\Vert',\qquad x\in\R^n,
$$(veja o Exercício 30). Temos então um diagrama comutativo:
$$\xymatrix{%
(V,\norma)\ar[r]^{\Id}&(V,\norma')\\
(\R^n,\norma_\phi)\ar[u]^{\phi}\ar[r]_{\Id}&(\R^n,\norma'_\phi)\ar[u]_{\phi}}$$
no diagrama acima as flechas verticais são isometrias e a flecha
horizontal de baixo é um homeomorfismo, pois as normas
$\norma_\phi$ e $\norma'_\phi$ em $\R^n$ são equivalentes. Segue
então que a flecha horizontal de cima é um homeomorfismo e
portanto $\norma$ e $\norma'$ são de fato equivalentes em $V$.
\smallskip

\noindent{\tt Exemplo}. Se $V$, $W$ são espaços vetoriais normados
então o conjunto $\Lin(V,W)$ de todas as aplicações lineares
contínuas $T:V\to W$ é um subespaço do espaço vetorial de todas as
aplicações lineares de $V$ em $W$; observe que quando $V$ tem
dimensão finita então $\Lin(V,W)$ denota simplesmente o espaço de
todas as aplicações lineares de $V$ em $W$. Para $T\in\Lin(V,W)$ a
quantidade:
$$\Vert T\Vert=\sup_{\scriptstyle x\in V\atop\scriptstyle\Vert
x\Vert=1}\Vert T(x)\Vert=\sup_{\scriptstyle x\in
V\atop\scriptstyle\Vert x\Vert\le1}\Vert T(x)\Vert$$ é finita. É
um exercício simples de $\sup$ mostrar que $T\mapsto\Vert T\Vert$
é de fato uma norma no espaço vetorial $\Lin(V,W)$; tal norma é
conhecida como a {\sl norma de operadores}. As seguintes
desigualdades são fáceis de verificar:
$$\Vert T(x)\Vert\le\Vert T\Vert\,\Vert x\Vert,\quad\Vert T\circ
S\Vert\le\Vert T\Vert\,\Vert S\Vert,$$ para todos $x\in V$,
$T\in\Lin(V,W)$, $S\in\Lin(Z,V)$ (onde $Z$ é um outro espaço
normado qualquer). Observe que em $\Lin(\R^m,\R^n)$ temos a norma
de operadores e a norma Euclideana bem definidas (podemos
identificar $\Lin(\R^m,\R^n)$ com $\R^{mn}$ da maneira óbvia)
--- tais normas {\sl são diferentes}, mas são equivalentes, pelo que já mostramos.

\smallskip

\noindent{\tt Observação}. Uma outra norma famosa em $\R^n$ é a
{\sl norma $p$}; para $p\in\R$, $p\ge1$ definimos:
$$\Vert x\Vert_p=\left(\sum_{i=1}^n\vert x_i\vert^p\right)^{1\over
p},\quad x\in\R^n.$$ Quando $p=2$ obtemos a norma Euclideana. A
desigualdade triangular para $\Vert\cdot\Vert_p$ é conhecida como
a {\sl desigualdade de Minkowski\/} e sua demonstração é um tanto
chata. Não teremos nenhum uso para a norma $p$ com
$p\not\in\{1,2,\infty\}$.

\smallskip

\proclaim Definição. Um {\rm produto interno} em $V$ é uma
aplicação bilinear $$V\times V\ni(x,y)\longmapsto\langle
x,y\rangle\in\R$$ satisfazendo as seguintes propriedades:
\item{(PI1)} $\langle x,y\rangle=\langle y,x\rangle$, para todos
$x,y\in V$ {\rm (simetria)};
\item{(PI2)} $\langle x,x\rangle>0$ para todo $x\in V$ não nulo
{\rm (positividade)}.

\smallskip

Se $\pint$ é um produto interno num espaço vetorial $V$ então
definimos:
$$\Vert x\Vert=\langle x,x\rangle^{1\over2};$$
a menos da desigualdade triangular, é fácil ver que $\norma$
satisfaz todas as propriedades requeridas para uma norma. Veremos
logo adiante que $\norma$ de fato satisfaz a desigualdade
triangular e portanto {\sl é uma norma\/} em $V$; dizemos que
$\norma$ é a {\sl norma induzida pelo produto interno $\pint$}.
Todo espaço vetorial com produto interno será sempre pensado como
um espaço vetorial normado através da norma induzida pelo produto
interno.

\smallskip

\proclaim Teorema. (desigualdade de Cauchy-Schwarz) Se $V$ é um
espaço vetorial com produto interno $\pint$ então para todos
$x,y\in V$ temos:
$$\big\vert\langle x,y\rangle\big\vert\le\Vert x\Vert\,\Vert y\Vert;$$
a igualdade vale se e somente se $x$ e $y$ são linearmente
dependentes.

\Prova Seja $f:\R\to\R$ definida por $f(t)=\langle
x+ty,x+ty\rangle$; temos:
$$f(t)=\Vert y\Vert^2t^2+2\langle x,y\rangle t+\Vert x\Vert^2,$$
donde $f$ é um polinômio de grau $2$ em $t$ (vamos excluir o caso
$y=0$ que é trivial). Como $f$ é não negativa, temos que o
discriminante $\Delta$ de $f$ é menor ou igual a zero, ou seja:
$$\Delta=4\langle x,y\rangle^2-4\Vert x\Vert^2\Vert
y\Vert^2\le0;$$ a igualdade vale se e somente se existe $t\in\R$
com $f(t)=0$ e nesse caso $x+ty=0$ e $x$, $y$ são linearmente
dependentes.\fimprova

\smallskip

\proclaim Corolário. $\Vert x\Vert=\langle x,x\rangle^{1\over2}$
satisfaz a desigualdade triangular e portanto realmente define uma
norma em $V$.

\Prova Temos:
$$\Vert x+y\Vert^2=\Vert x\Vert^2+2\langle x,y\rangle+\Vert
y\Vert^2\le\Vert x\Vert^2+2\Vert x\Vert\,\Vert y\Vert+\Vert
y\Vert^2=\big(\Vert x\Vert+\Vert y\Vert\big)^2.\fimprova$$

\smallskip

\noindent{\tt Exemplo}. Em $\R^n$, o produto interno canônico
$\langle x,y\rangle=\sum_{i=1}^nx_iy_i$ é de fato um produto
interno; a norma correspondente é a norma Euclideana (e finalmente
provamos a desigualdade triangular para essa norma!). {\sl Para
quem sabe mais álgebra linear}: se $A$ é uma matriz real simétrica
$n\times n$ que possui apenas autovalores positivos então $\langle
x,y\rangle'=\sum_{i,j=1}^nA_{ij}x_iy_j$ é um produto interno em
$\R^n$ (e na verdade todo produto interno em $\R^n$ é desse tipo).

\smallskip

\noindent{\tt Observação}. O conceito de métrica e topologia faz
sentido em conjuntos quaisquer; normas e produtos internos só
fazem sentido em espaços vetoriais reais (e na verdade em
complexos também, mas não nos interessamos por isso aqui). Num
espaço vetorial real $V$, existem métricas que não vem de normas
(como a métrica zero-um) e existem normas que não vem de produto
interno (como a norma $\norma_p$ com $p\ne2$). Note que {\sl
métricas que não vem de normas podem não ser equivalentes à
métricas que vem de norma}, mesmo quando $V$ tem dimensão finita
(a métrica zero-um não é equivalente à métrica Euclideana em
$\R^n$).

\smallskip

\noindent{\tt Observação}. Para cultura geral: um espaço normado
completo chama-se um {\sl espaço de Banach\/} e um espaço com
produto interno completo chama-se um {\sl espaço de Hilbert}.
Esses são os objetos básicos de estudo da análise funcional
abstrata. Nesse caso o interesse maior é pelos espaços de dimensão
infinita; para tais espaços não é verdade que todas as normas são
equivalentes.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Adendo aos exercícios da aula número 5}.

\smallskip

\item{-1.} No Exercício 2 da aula número 5 (20/03)
acrescentar o seguinte item:
\itemitem{(g)} $\Ext(A)=\Int(A^\compl)$.

\smallskip

\item{0.} No Exercício 3 da aula número 5 (20/03) acrescentar o
seguinte item:
\itemitem{(e)} $\partial A\cap A=\emptyset$.

\medskip

\noindent{\tt Mais espaços métricos}.

\smallskip

\item{1.} Mostre que todo subconjunto finito de um espaço métrico
é fechado.

\smallskip

\item{2.} Seja $(M,d)$ um espaço métrico. Dados $x,y,a\in M$
mostre que:
$$\big\vert d(x,a)-d(y,a)\big\vert\le d(x,y).$$
({\sl dica}: mostre que $d(x,a)-d(y,a)\le d(x,y)$ e que
$d(y,a)-d(x,a)\le d(x,y)$). Conclua que a função {\sl distância
até o ponto $a$\/} definida por $d_a:M\to\R$, $d_a(x)=d(x,a)$, é
uma contração fraca e portanto é uniformemente contínua.

\smallskip

\item{3.} Seja $(M,d)$ um espaço métrico.
\smallskip
\itemitem{(a)} Mostre que para todos $x,x',y,y'\in M$ vale a
desigualdade:
$$\big\vert d(x,y)-d(x',y')\big\vert\le d(x,x')+d(y,y').$$
({\sl dica}: $d(x,y)-d(x',y')=d(x,y)-d(x',y)+d(x',y)-d(x',y')$;
use o Exercício anterior).
\itemitem{(b)} Considere o produto
cartesiano $M\times M$ munido da seguinte métrica $D$:
$$D\big((x,y),(x',y')\big)=d(x,x')+d(y,y'),\quad (x,y),(x',y')\in M\times M;$$
(recorde do Exercício 33 da aula número 5 que $D$ é uniformemente
equivalente à métrica produto em $M\times M$). Conclua do item (a)
que a própria métrica de $M$:
$$d:M\times M\longrightarrow\R$$
é uma contração fraca e portanto é uma função uniformemente
contínua.
\itemitem{(c)} Mostre que se $f,g:N\to M$ são funções contínuas
(onde $N$ é um outro espaço métrico qualquer) então a função
$h:N\to\R$ definida por $h(x)=d\big(f(x),g(x)\big)$, é contínua
({\sl dica}: use o item (b)).
\itemitem{(d)} Refaça o item (c) trocando ``função contínua'' por
``função uniformemente con\-tí\-nua'' (i.e., suponha $f$, $g$
uniformemente contínuas e conclua que $h$ também o é).
\itemitem{(e)} Mostre que se $f,g:N\to M$ são funções contínuas
então o conjunto
$$\big\{x\in N:f(x)=g(x)\big\}$$
dos pontos aonde elas coincidem é fechado em $N$ --- cuidado pois
$h=f-g$ {\sl não faz sentido\/} se $M$ não é um espaço vetorial!
({\sl dica}: mas $h(x)=d\big(f(x),g(x)\big)$ faz sentido).
\itemitem{(f)} Mostre que se $(x_n)_{n\ge1}$ e $(y_n)_{n\ge1}$ são seqüências
de Cauchy em $M$ então a seqüência $\big(d(x_n,y_n)\big)_{n\ge1}$
é convergente em $\R$ ({\sl dica}: funções uniformemente
con\-tí\-nuas levam seqüências de Cauchy em seqüências de Cauchy).

\smallskip

\item{4.} ({\sl distância de um ponto a um conjunto}) Sejam $(M,d)$
um espaço métrico, $x\in M$ um ponto e $A\subset M$ um
subconjunto. A {\sl distância de $x$ até $A$\/} é definida por:
$$d(x,A)=\inf_{a\in A}d(x,a).$$
Mostre que:
\smallskip
\itemitem{(a)} $d(x,A)=0$ se e somente se $x\in\overline A$;
conclua que se $A$ é fechado e $x\not\in A$ então $d(x,A)>0$.
\itemitem{(b)} $d(x,A)=d(x,\overline A)$.
\itemitem{(c)} ({\sl desigualdade triangular generalizada}) para
$x,y\in M$:
$$d(x,A)\le d(x,y)+d(y,A).$$
({\sl dica}: se $\phi(a)\le\psi(a)$ para todo $a$ então
$\inf_a\phi(a)\le\inf_a\psi(a)$).
\itemitem{(d)} Para $x,y\in M$:
$$\big\vert d(x,A)-d(y,A)\big\vert\le d(x,y);$$
conclua que a função {\sl distância até o conjunto $A$\/} definida
por $d_A:M\to\R$, $d_A(x)=d(x,A)$, é uma contração fraca e
portanto é uniformemente contínua.
\itemitem{(e)} ({\sl o Lema de Urisohn para espaços métricos})
Sejam $F,G\subset M$ fechados disjuntos. Mostre que a expressão:
$$f(x)={d(x,F)\over d(x,F)+d(x,G)}$$
define uma função contínua $f:M\to[0,1]$ tal que $f\vert_F\equiv0$
e $f\vert_G\equiv1$.
\itemitem{(f)} ({\sl a propriedade T4 dos espaços métricos})
Conclua do item (d) que se $F,G\subset M$ são fechados disjuntos
então existem abertos disjuntos $U,V\subset M$ com $F\subset U$ e
$G\subset V$ ({\sl dica}: considere
$f^{-1}\big((-\infty,{1\over2})\big)$ e
$f^{-1}\big(({1\over2},+\infty)\big)$).

\medskip

\noindent{\tt Espaços conexos}.

\smallskip

\item{5.} Sejam $M$ um espaço métrico e $A\subset M$ um subconjunto. Mostre
que as seguintes afirmações são equivalentes:
\smallskip
\itemitem{(a)} $A$ é aberto e fechado em $M$;
\itemitem{(b)} $A$ e $A^\compl$ são abertos em $M$;
\itemitem{(c)} $A$ e $A^\compl$ são fechados em $M$;
\itemitem{(d)} a fronteira de $A$ é vazia.

\eject

\item{6.} Seja $M$ um espaço métrico. Mostre que são equivalentes:
\smallskip
\itemitem{(a)} $M$ é conexo;
\itemitem{(b)} se $M=A\cup B$ com $A$, $B$ fechados disjuntos em $M$ então
$A=\emptyset$ ou $B=\emptyset$;
\itemitem{(c)} se $A$ é aberto e fechado em $M$ então $A=\emptyset$ ou $A=M$;
\itemitem{(d)} se $A\subset M$ tem fronteira vazia então $A=\emptyset$ ou $A=M$.

\item{7.} ({\sl componentes conexas}) Seja $M$ um espaço métrico. Defina uma relação $\sim$ em
$M$ fazendo:
$$x\sim y\quad\Longleftrightarrow\quad\hbox{existe $C\subset M$
conexo com $x,y\in C$},$$ para todos $x,y\in M$.
\smallskip
\itemitem{(a)} Mostre que $\sim$ é uma relação de equivalência ({\sl dica}:
se $C,C'\subset M$ são conexos tais que $x,y\in C$ e $y,z\in C'$,
olhe para $C\cup C'$). As classes de equivalência correspondentes
a $\sim$ são chamadas as {\sl componentes conexas\/} de $M$ (mostre que elas
são de fato conexas).
\itemitem{(b)} Mostre que se $C\subset M$ é uma componente conexa,
$A\subset M$ é conexo e $A\cap C\ne\emptyset$ então $A\subset C$;
conclua que a componente conexa $C$ de $M$ que contém um certo
ponto $x$ é o maior subconjunto conexo de $M$ que contém $x$,
i.e., se $A\subset M$ é conexo e $x\in A$ então $A\subset C$.
\itemitem{(c)} Mostre que se $A\subset M$ é aberto, fechado,
conexo e não vazio então $A$ é uma componente conexa de $M$ ({\sl
dica}: se $C\subset M$ é uma componente conexa então $A\cap C$ é
aberto e fechado em $C$).
\itemitem{(d)} Mostre que as componentes conexas de um espaço
métrico são fechadas ({\sl dica}: o fecho de um conjunto conexo é
conexo).
\itemitem{(e)} Determine as componentes conexas de $\Q$ e mostre
que elas {\sl não são abertas\/} em $\Q$.
\itemitem{(f)} Se $U\subset\R^n$ é um aberto (munido da métrica
induzida da métrica Euclideana de $\R^n$), mostre que as
componentes conexas de $U$ são abertas em $U$ --- e portanto
abertas em $\R^n$ ({\sl dica}: uma bola aberta em $\R^n$ é
conexa).
\itemitem{(g)} Vamos generalizar o item (f). Um espaço métrico $M$
é dito {\sl localmente conexo\/} quando todo ponto de $M$ possui
um sistema fundamental de vizinhanças conexas (recorde Exercício
19 da aula número 5), i.e., se toda vizinhança de um ponto $x\in
M$ contém uma vizinhança conexa de $x$. Mostre que se $M$ é
localmente conexo então as componentes conexas de um aberto $U$ de
$M$ são abertas em $U$ --- e portanto abertas em $M$.

\smallskip

\item{8.} ({\sl componentes conexas por arcos}) Seja $M$ um espaço
métrico. Dadas curvas contínuas $\gamma,\mu:[0,1]\to M$ com
$\gamma(1)=\mu(0)$ então a {\sl concatenação\/} de $\gamma$ e
$\mu$ é a curva $\sigma=\gamma\cdot\mu$, $\sigma:[0,1]\to M$,
definida por:
$$\sigma(t)=\cases{\gamma(2t),&$t\in\big[0,{\textstyle{1\over2}}\big],$\cr
\noalign{\smallskip}
\mu(2t-1),&$t\in\big[{\textstyle{1\over2}},1\big].$\cr}$$
(intuitivamente, $\sigma$ consiste de $\gamma$ percorrida com o
dobro da velocidade na primeira metade de $[0,1]$, seguida de
$\mu$ percorrida com o dobro da velocidade na segunda metade de
$[0,1]$).
\smallskip
\itemitem{(a)} Mostre que $\sigma=\gamma\cdot\mu$ é contínua ({\sl
dica}: use o resultado do Exercício 38, aula número 5).
\itemitem{(b)} Defina uma relação $\sim$ em $M$ fazendo:
$$x\sim y\Longleftrightarrow\hbox{existe $\gamma:[0,1]\to M$
contínua com $\gamma(0)=x$ e $\gamma(1)=y$},$$ para todos $x,y\in
M$. Mostre que $\sim$ é uma relação de equivalência; as classes de
equivalência correspondentes a $\sim$ são chamadas as {\sl
componentes conexas por arcos\/} de $M$ (mostre que elas são de fato conexas
por arcos).
\itemitem{(c)} Mostre que toda componente conexa de $M$ é uma
união de componentes conexas por arcos de $M$.

\smallskip

\item{9.} Um subconjunto $S\subset\R^n$ é dito {\sl estrelado\/}
com respeito a $x\in S$ se para todo $y\in S$ o segmento de reta
$[x,y]$ está contido em $S$ (observe que $S$ é convexo se e
somente se $S$ é estrelado com respeito a todos os seus pontos).
Mostre que todo subconjunto estrelado de $\R^n$ é conexo por
caminhos (e portanto conexo).

\smallskip

\item{10.} Seja $S\subset\R^2$ o gráfico de $\left(0,1\right]\ni
t\mapsto\sen{1\over t}$, de modo que o fecho de $S$ é a senóide
dos topólogos. Vamos mostrar neste exercício que não existe uma
curva contínua $\gamma:[0,1]\to\overline S$ com $\gamma(0)\in S$ e
$\gamma(1)\in\{0\}\times[0,1]$. Suponha por absurdo que tal curva
existe e escreva $\gamma(t)=\big(x(t),y(t)\big)$; considere o
conjunto:
$$A=\big\{t\in[0,1]:x(t)>0\big\}.$$
\itemitem{(a)}Mostre que $A$ é limitado superiormente e não vazio,
de modo que existe $c=\sup A$.
\itemitem{(b)}Mostre que $x(c)=0$.
\itemitem{(c)}Mostre que para todo $\varepsilon>0$ a função
$x:[0,1]\to\R$ assume valores da forma ${1\over k\pi}$ e da forma
${1\over{\pi\over2}+k\pi}$ ($k\in\N$) no intervalo
$[c-\varepsilon,c]$ ({\sl dica}: use o teorema do valor
intermediário).
\itemitem{(d)}Conclua do item (c) que $\lim_{t\to c^-}y(t)$ não
existe, contradizendo o fato que $y$ é contínua.

\smallskip

\item{11.} Mostre que um intervalo fechado $[a,b]\subset\R$ não
pode ser homeomorfo ao círculo
$S^1=\big\{(x,y)\in\R^2:x^2+y^2=1\big\}$ ({\sl dica}: é possível
tirar um ponto de $[a,b]$ e torná-lo desconexo, mas tirando um
ponto qualquer de $S^1$ ele continua conexo).

\medskip

\noindent{\tt Espaços compactos}.

\smallskip

\item{12.} Seja $M$ um espaço métrico. Mostre que:
\smallskip
\itemitem{(a)} se $M$ é totalmente limitado então todo subconjunto
$S\subset M$ é totalmente limitado;
\itemitem{(b)} para $S\subset M$ temos $\diam(S)=\diam(\overline
S)$; conclua que $S$ é totalmente limitado se e somente se
$\overline S$ é totalmente limitado.

\smallskip

\item{13.} Seja $M$ um espaço métrico e seja $(x_n)_{n\ge1}$ uma
seqüência em $M$. Dizemos que $x\in M$ é um {\sl ponto aderente\/}
(ou um {\sl ponto de aderência}) à seqüência $(x_n)_{n\ge1}$ se
dado $\varepsilon>0$ então temos $x_n\in\Bola(x;\varepsilon)$ para
$n$ {\sl arbitrariamente grande}, i.e., se o conjunto
$$\big\{n\in\N:x_n\in\Bola(x;\varepsilon)\big\}$$ é infinito. Mostre
que as seguintes condições são equivalentes:
\smallskip
\itemitem{(a)}$x$ é um ponto de aderência de $(x_n)_{n\ge1}$;
\itemitem{(b)}dado um aberto $U\subset M$ contendo $x$ então
$x_n\in U$ para $n$ arbitrariamente grande ({\sl ``ponto aderente
a uma seqüência'' é uma noção topológica});
\itemitem{(c)}existe uma subseqüência $(x_{n_k})_{k\ge1}$ de
$(x_n)_{n\ge1}$ convergindo para $x$ (recorde que para que
$(x_{n_k})_{k\ge1}$ seja chamada uma {\sl subseqüência\/} de
$(x_n)_{n\ge1}$ é necessário que $n_1<n_2<n_3<\cdots$);
\itemitem{(d)}$x$ é um ponto de acumulação do conjunto
$\big\{x_n:n\in\N\big\}$ {\sl ou\/} a seqüência $(x_n)_{n\ge1}$
possui infinitos termos iguais a $x$ (i.e., o conjunto
$\big\{n\in\N:x_n=x\big\}$ é infinito).

\smallskip

\noindent{\tt Observação}. Não confundir ``$x$ é aderente à
seqüência $(x_n)_{n\ge1}$'' com ``$x$ é aderente ao conjunto
$\big\{x_n:n\in\N\big\}$''. Por exemplo, ${1\over n}$ é aderente
ao conjunto $\big\{{1\over n}:n\in\N\big\}$ para todo $n\in\N$ mas
{\sl nenhum\/} número da forma ${1\over n}$ é aderente à seqüência
$\big({1\over n}\big)_{n\ge1}$.

\smallskip

\noindent{\tt Observação}. ``existem infinitos $n$'s com
$x_n\in\Bola(x;\varepsilon)$'' e ``existem infinitos $x_n$'s em
$\Bola(x;\varepsilon)$'' são afirmações diferentes! O conjunto
$\big\{n\in\N:x_n\in\Bola(x;\varepsilon)\big\}$ possui
cardinalidade maior ou igual a do conjunto
$\big\{x_n:n\in\N\big\}\cap\Bola(x;\varepsilon)$. Por exemplo, se
$x_n=x$ para todo $n\in\N$ então existe um único $x_n$ em
$\Bola(x;\varepsilon)$ mas existem infinitos $n$'s com
$x_n\in\Bola(x;\varepsilon)$.

\smallskip

\item{14.} Seja $M$ um espaço métrico completo. Seja
$(F_n)_{n\ge1}$ uma seqüência de subconjuntos fechados não vazios
$F_n\subset M$ com $F_1\supset F_2\supset F_3\supset\cdots$ e
$\diam(F_n)\to0$. Mostre que a interseção $\bigcap_{n\ge1}F_n$
possui exatamente um ponto ({\sl dica}: escolha $x_n\in F_n$ para
cada $n$ e mostre que $(x_n)_{n\ge1}$ é uma seqüência de Cauchy).

\smallskip

\noindent{\tt Observação}. Sem a hipótese $\diam(F_n)\to0$ no
Exercício 14 é possível que a interseção $\bigcap_{n\ge1}F_n$ seja
vazia! Por exemplo, tome $M=\R$ e $F_n=\left[n,+\infty\right)$.

\smallskip

\item{15.} Seja $M$ um espaço métrico e seja $(K_n)_{n\ge1}$ uma
seqüência de compactos não vazios $K_n\subset M$ com $K_1\supset
K_2\supset K_3\supset\cdots$. Mostre que a interseção
$\bigcap_{n\ge1}K_n$ é não vazia ({\sl dica}: supondo por absurdo
que $\bigcap_{n\ge1}K_n=\emptyset$, a família $(M\setminus
K_n)_{n\ge1}$ seria uma cobertura aberta de $K_1$).

\smallskip

\item{16.} Mostre que todo espaço métrico finito é compacto. Mostre também
que a união finita de subespaços compactos de um espaço métrico
ainda é compacta.

\smallskip

\item{17.} ({\sl produto por um compacto}) Seja $K$ um espaço
métrico compacto e $M$ um espaço métrico qualquer. Dado um ponto
$p\in M$ e um aberto $Z\subset K\times M$ que contém
$K\times\{p\}$, mostre que existe uma vizinhança aberta $V$ de $p$
em $M$ tal que $K\times V\subset Z$ (``se um aberto contém uma
linha compacta então ele contém uma faixa em torno dessa linha'')
--- {\sl dica}: para todo $x\in K$, existem abertos $U_x\subset K$
e $V_x\subset M$ com $x\in U_x$, $p\in V_x$ e $U_x\times
V_x\subset Z$ (veja o Exercício 20(c) da aula número 5); adivinhe
o que fazer com a cobertura aberta $K=\bigcup_{x\in K}U_x$!

\smallskip

\item{18.} ({\sl mais uniformidades}) Sejam $K$ um espaço métrico
compacto e $M$, $N$ espaços métricos quaisquer. Seja $f:K\times
M\to N$ uma função contínua; mostre que a continuidade de $f$ é
{\sl uniforme com respeito à variável em $K$}, i.e., que dados
$y_0\in M$ e $\varepsilon>0$ então existe $\delta>0$ tal que para
todo $x\in K$ e todo $y\in\Bola(y_0;\delta)\subset M$ temos
$d\big(f(x,y),f(x,y_0)\big)<\varepsilon$ ($\delta>0$ não depende
de $x\in K$) --- {\sl dica}: o conjunto
$$Z=\big\{(x,y)\in K\times
M:d\big(f(x,y),f(x,y_0)\big)<\varepsilon\big\}$$ é aberto em
$K\times M$ e contém a linha $K\times\{y_0\}$; use o Exercício anterior.

\smallskip

\item{19.} ({\sl distância entre conjuntos}) Seja $M$
um espaço métrico. Dados subconjuntos $A,B\subset M$ definimos a
{\sl distância entre $A$ e $B$\/} por:
$$d(A,B)=\inf_{\scriptstyle a\in A\atop\scriptstyle b\in B}d(a,b)=\inf_{a\in
A}d(a,B)=\inf_{b\in B}d(A,b).$$ Mostre que:
\smallskip
\itemitem{(a)} se $K\subset M$ é compacto e $F\subset M$ é fechado
com $K\ne\emptyset$ então existe $x\in K$ com $d(x,F)=d(K,F)$
({\sl dica}: $K\ni x\mapsto d(x,F)$ é uma função contínua num
compacto). Conclua que se $K\cap F=\emptyset$ então $d(K,F)>0$.
\itemitem{(b)} Se $K_1,K_2\subset M$ são compactos não vazios
então existem $x\in K_1$, $y\in K_2$ com $d(x,y)=d(K_1,K_2)$,
i.e., {\sl a distância entre compactos é efetivamente assumida\/}
({\sl dica}: $d\vert_{K_1\times K_2}$ é uma função contínua num
compacto).
\itemitem{(c)} Se $K\subset\R^n$ é compacto e $F\subset\R^n$ é
fechado, com $K$ e $F$ não vazios, então existem $x\in K$, $y\in
F$ com $d(x,y)=d(K,F)$, i.e., {\sl a distância entre um compacto e
um fechado em $\R^n$ é efetivamente assumida\/} ({\sl dica}:
escolha $k>0$ grande tal que $F_0=\big\{y\in F:d(y,K)\le k\big\}$
é não vazio; daí $F_0$ é compacto e $d(K,F)=d(K,F_0)$).
\itemitem{(d)} Ache subconjuntos fechados disjuntos
$F_1,F_2\subset\R^2$ tais que $d(F_1,F_2)=0$.

\smallskip

\noindent{\tt Observação}. O item (c) do Exercício 19 {\sl não
vale\/} se o espaço métrico ambiente não é o $\R^n$; por exemplo,
se $M=\R\setminus\{0\}$, $K=\{-1\}$ e $F=(0,+\infty)$ então $F$ é
fechado em $M$, $K$ é compacto, mas a distância entre $K$ e $F$
não é efetivamente assumida. É possível também obter
contra-exemplos onde o espaço ambiente $M$ é completo.

\smallskip

\item{20.} Mostre que se $(x_n)_{n\ge1}$ é uma seqüência convergente num
espaço métrico $M$, digamos $x_n\to x\in M$, então o conjunto
$\big\{x_n:n\ge1\big\}\cup\{x\}$ é compacto.

\smallskip

\item{21.} Sejam $M$ um espaço métrico e $S\subset M$ um
subconjunto. Dizemos que $S$ é {\sl relativamente compacto\/} em
$M$ se o fecho de $S$ em $M$ é compacto (``relativamente
compacto'' {\sl não\/} é uma noção intrínseca). Mostre que as
seguintes condições são equivalentes: \smallskip
\itemitem{(a)} $S$ é relativamente compacto em $M$;
\itemitem{(b)} toda seqüência em $S$ possui uma subseqüência
convergente em $M$.

\smallskip

\item{22.} Seja $M$ um espaço métrico discreto (recorde Exercício
40 da aula número 5). Mostre que $M$ é compacto se e somente se
$M$ é finito.

\smallskip

\item{23.} Mostre que a esfera
$S^2=\big\{(x,y,z)\in\R^3:x^2+y^2+z^2=1\big\}$ não é homeomorfa a
$\R^2$, mas que a esfera $S^2$ menos um ponto é homeomorfa a
$\R^2$ (lembram da projeção estereográfica?).

\medskip

\noindent{\tt Base enumerável e separabilidade}.

\smallskip

\item{24.} Sejam $M$ um espaço métrico e $\Bfrak$ uma coleção de
abertos de $M$. Mostre que as seguintes propriedades são
equivalentes:
\smallskip
\itemitem{(i)} $\Bfrak$ é uma base de abertos para $M$;
\itemitem{(ii)} dado $U\subset M$ aberto e $x\in U$ então existe
$V\in\Bfrak$ com $x\in V\subset U$.

\eject

\item{25.} ({\sl para acompanhar este exercício, você deve recordar os
Exercícios 19 e 20 da aula número 5}) Seja $M$ um espaço métrico.
Mostre que:
\smallskip
\itemitem{(a)} se $\Bfrak$ é uma base de abertos para $M$ então
para todo $x\in M$ a coleção $${\cal V}_x=\big\{V\in\Bfrak:x\in
V\big\}$$ é um sistema fundamental de vizinhanças abertas para
$x\in M$.
\itemitem{(b)} Se para cada $x\in M$ escolhemos um sistema
fundamental de vizinhanças abertas ${\cal V}_x$ de $x\in M$ então
a coleção $\Bfrak=\bigcup_{x\in M}{\cal V}_x$ é uma base de
abertos para $M$.
\itemitem{(c)} Se $(M_i,d_i)$, $i=1,\ldots,n$, são espaços
métricos então:
$$\Bfrak=\Big\{{\textstyle\prod_{i=1}^nU_i}:U_i\subset M_i\
\hbox{aberto},\ i=1,\ldots,n\Big\}$$ é uma base de abertos para o
produto $\prod_{i=1}^nM_i$. Mais geralmente, se $\Bfrak_i$ é uma
base de abertos para $M_i$ então:
$$\Bfrak=\Big\{{\textstyle\prod_{i=1}^nU_i}:U_i\in\Bfrak_i,\ i=1,\ldots,n\Big\}$$
é uma base de abertos para $\prod_{i=1}^nM_i$.
\itemitem{(d)} Conclua do item (c) que o produto de espaços
métricos com base enumerável ainda é um espaço métrico com base
enumerável.

\smallskip

\item{26.} Mostre que todo espaço métrico totalmente limitado $M$ é
separável ({\sl dica}: para todo $n\ge1$ escolha $S_n\subset M$
finito tal que $M=\bigcup_{x\in S_n}\Bola\big(x;{1\over n}\big)$
--- defina $D=\bigcup_{n\ge1}S_n$).

\medskip

\noindent{\tt Espaços normados e espaços com produto interno}.

\smallskip

Vamos generalizar alguns resultados da seção para operadores
bilineares e multi-lineares:

\item{27.} Sejam $V$, $W$ e $Z$ espaços vetoriais normados. Sobre
uma aplicação bilinear $B:V\times W\to Z$ mostre que são
equivalentes:
\itemitem{(i)} $B$ é contínua;
\itemitem{(ii)} $B$ é contínua em $(0,0)$;
\itemitem{(iii)} existe $c\ge0$ com $\Vert B(x,y)\Vert\le c$ para
todos $x\in V$, $y\in W$ com $\Vert x\Vert=\Vert y\Vert=1$;
\itemitem{(iv)} existe $c\ge0$ com $\Vert B(x,y)\Vert\le c\Vert
x\Vert\,\Vert y\Vert$ para todo $x\in V$, $y\in W$.

Generalize o
exercício para aplicações multi-lineares.

\smallskip

\noindent{\tt Observação}. Segue do Exercício acima e da desigualdade de
Cauchy-Schwarz que um produto interno é contínuo (com respeito à métrica
definida por ele mesmo).

\smallskip

\noindent{\tt Observação}. Aplicações bilineares em geral não são
Lipschitzianas (e nem mesmo uniformemente contínuas).

\smallskip

\item{28.} Sejam $V$, $W$ e $Z$ espaços vetoriais normados. Mostre
que se $V$ e $W$ tem dimensão finita então toda aplicação bilinear
$B:V\times W\to Z$ é contínua ({\sl dica:} troque $Z$ por um
subespaço de dimensão finita que contém a imagem de $B$; podemos
supor então que $B$ é uma aplicação bilinear
$\R^m\times\R^n\to\R^p$ onde $\R^m$, $\R^n$ e $\R^p$ possuem a
norma Euclideana). Generalize o resultado para aplicações
multi-lineares.

\smallskip

\item{29.} Sejam $V$, $W$ e $Z$ espaços vetoriais normados. Denote
por $\Bil(V,W;Z)$ o conjunto de todas as aplicações bilineares
contínuas $B:V\times W\to Z$. Mostre que:
\smallskip
\itemitem{(a)} $\Bil(V,W;Z)$ é um subespaço do espaço vetorial de
todas as aplicações bilineares $V\times W\to Z$;
\itemitem{(b)} a fórmula:
$$\Vert B\Vert=\sup_{\scriptstyle x\in V,\ \Vert
x\Vert=1\atop\scriptstyle y\in W,\ \Vert y\Vert=1}\Vert
B(x,y)\Vert=\sup_{\scriptstyle x\in V,\ \Vert
x\Vert\le1\atop\scriptstyle y\in W,\ \Vert y\Vert\le1}\Vert
B(x,y)\Vert,$$ define uma norma em $\Bil(V,W;Z)$;
\itemitem{(c)} generalize os itens (a) e (b) para o caso de
aplicações multi-lineares.

\smallskip

\item{30.} ({\sl norma induzida por um isomorfismo}) Sejam $V$, $W$ espaços vetoriais, $\phi:V\to W$ um isomorfismo
e $\norma$ uma norma em $W$. Defina:
$$\Vert x\Vert_\phi=\Vert\phi(x)\Vert,$$
para todo $x\in V$. Mostre que $\norma_\phi$ é uma norma em $V$ e
que $\phi:(V,\norma_\phi)\to(W,\norma)$ é uma isometria. Dizemos
que $\norma_\phi$ é a norma em $V$ {\sl induzida\/} pelo
isomorfismo $\phi$ e pela norma de $W$.

\smallskip

\item{31.} ({\sl norma num subespaço}) Sejam $V$ um espaço vetorial
e $S\subset V$ um subespaço. Mostre que se $\norma$ é uma norma em
$V$ então $\norma$ restringe-se a uma norma $\norma\vert_S$ em
$S$. Mostre que a métrica induzida em $S$ por $\norma\vert_S$
coincide com a restrição da métrica induzida por $\norma$ em $V$.

\smallskip

\item{32.} ({\sl produto interno induzido por uma aplicação}) Seja
$\phi:V\to W$ um isomorfismo entre espaços vetoriais $V$, $W$ e
seja $\pint$ um produto interno em $W$. Mostre que:
$$\langle x,y\rangle_\phi=\langle\phi(x),\phi(y)\rangle,\quad
x,y\in V,$$ define um produto interno em $V$; dizemos que
$\pint_\phi$ é o produto interno {\sl induzido\/} por $\phi$ e por
$\pint$. Mostre que $\phi:(V,\pint_\phi)\to(W,\pint)$ é uma
isometria.

\smallskip

\item{33.} ({\sl produto interno induzido num subespaço}) Sejam
$V$ um espaço vetorial e $S\subset V$ um subespaço. Seja $\pint$
um produto interno em $V$. Mostre que $\pint$ restringe-se a um
produto interno $\pint\vert_{S\times S}$ em $S$. Mostre que a
norma induzida em $S$ por $\pint\vert_{S\times S}$ coincide com a
restrição da norma induzida em $V$ por $\pint$.

\smallskip

\noindent{\tt Observação}. É possível unificar os exercícios 30 e
31 (assim como os exercícios 32 e 33) trocando a hipótese que
$\phi$ seja um isomorfismo pela hipótese que $\phi$ seja linear
injetora (obviamente nesse caso vamos concluir apenas que $\phi$ é
uma imersão isométrica em vez de concluir que $\phi$ é uma
isometria).

\eject

\item{34.} ({\sl generalizando os Exercícios 18 e 33 da aula 5}) Seja $\norma$
uma norma em $\R^n$ satisfazendo a
seguinte propriedade:
$$\hbox{se $x,y\in\left[0,+\infty\right)^n$ são tais que $x_i\le y_i$,
$i=1,\ldots,n$ então $\Vert x\Vert\le\Vert y\Vert$}\eqno{(*)}$$
(observe que a norma
Euclideana e as normas $\norma_p$ satisfazem a propriedade ($*$)).
\smallskip
\itemitem{(a)} Sejam $V_i$, $i=1,\ldots,n$ espaços vetoriais; o
produto $V=\prod_{i=1}^nV_i$ também é um espaço vetorial com as
operações definidas coordenada por coordenada --- tal espaço é
usualmente denotado por $\bigoplus_{i=1}^nV_i$ (ou
$V_1\oplus\cdots\oplus V_n$) e é chamado a {\sl soma direta\/} ou
a {\sl soma direta externa\/} dos espaços $V_i$. Se $\norma^i$ é
uma norma em $V_i$, defina:
$$\Vert x\Vert^{\rm prod}=\Big\Vert\big(\Vert x_1\Vert^1,\ldots,\Vert
x_n\Vert^n\big)\Big\Vert,$$ para todo $x=(x_i)_{i=1}^n\in V$.
Mostre que $\norma^{\rm prod}$ é de fato uma norma em $V$; mostre
também que trocando $\norma$ por uma outra norma $\norma'$ em
$\R^n$ (satisfazendo ($*$)) então obtemos uma norma equivalente a $\norma^{\rm
prod}$
em $V$.
\itemitem{(b)} Sejam $(M_i,d_i)$, $i=1,\ldots,n$ espaços métricos.
Seja $M=\prod_{i=1}^nM_i$ e defina:
$$d^{\rm prod}(x,y)=\Big\Vert\big(d_1(x_1,y_1),\ldots,d_n(x_n,y_n)\big)\Big\Vert,$$
para todos $x=(x_i)_{i=1}^n$, $y=(y_i)_{i=1}^n$ em $M$. Mostre que
$d^{\rm prod}$ é de fato uma métrica em $M$; mostre também que
$d^{\rm prod}$ é uniformemente equivalente à métrica produto usual
em $M$ (a equivalência é até mesmo ``Lipschitz'').
\itemitem{(c)} Se $d_i$ é a métrica induzida pela norma $\norma^i$
no espaço $V_i$, mostre que $d^{\rm prod}$ é a métrica induzida
pela norma $\norma^{\rm prod}$ em $V=\bigoplus_{i=1}^nV_i$.

\vfil\eject

\centerline{\bf Aula número $8$ (29/03)}
\bigskip
\bigskip

A aula número $8$ cobriu o material das Seções (2) e (4)
originalmente destinado à aula número $7$ --- os seguintes
assuntos foram omitidos:
\item{(i)} a noção de número de Lebesgue de uma cobertura;

\item{(ii)} a seção (3) sobre base enumerável e separabilidade;

\item{(iii)} a noção de normas equivalentes e os resultados
relativos à continuidade de operadores lineares.

\bigskip\bigskip\bigskip\bigskip

\centerline{\bf Aula número $9$ (03/04)}
\bigskip
\bigskip

A aula começa cobrindo a parte que faltou da seção (4) da aula número $7$
(equivalência de normas, continuidade de operadores lineares cujo domínio tem
dimensão finita e norma de operadores).

\medskip

\item{(1)} {\bf A prova da regra da cadeia}.

\smallskip

Sejam $U\subset\R^m$, $V\subset\R^n$ abertos e $f:U\to\R^n$,
$g:V\to\R^p$ funções com $f(U)\subset V$. Suponha que $f$ é
diferenciável num ponto $x\in U$ e que $g$ é diferenciável no
ponto $f(x)\in V$. Nós devemos mostrar que $g\circ f$ é
diferenciável no ponto $x\in U$ e que sua diferencial é dada por:
$$\dd(g\circ f)(x)=\dd g\big(f(x)\big)\circ\dd f(x).$$
Escrevemos:
$$\eqalign{f(x+h)&=f(x)+\dd f(x)\cdot h+\rho(h)\Vert h\Vert,\cr
g\big(f(x)+k\big)&=g\big(f(x)\big)+\dd g\big(f(x)\big)\cdot
k+\sigma(k)\Vert k\Vert,\cr}$$ com $\lim_{h\to0}\rho(h)=0$ e
$\lim_{k\to0}\sigma(k)=0$. Aplicamos $g$ dos dois lados da
primeira identidade acima e utilizamos a segunda com $k=\dd
f(x)\cdot h+\rho(h)\Vert h\Vert$; obtemos:
$$(g\circ f)(x+h)=(g\circ f)(x)+\big[\dd g\big(f(x)\big)\circ\dd
f(x)\big]\cdot h+\Vert h\Vert\big[\dd
g\big(f(x)\big)\cdot\rho(h)\big]+\sigma(k)\Vert k\Vert.$$ Como
$\dd g\big(f(x)\big)\circ\dd f(x)$ é linear, a conclusão segue se
mostrarmos que:
$$\lim_{h\to0}\dd g\big(f(x)\big)\cdot\rho(h)+\sigma(k){\Vert
k\Vert\over\Vert h\Vert}=0.$$ Sabemos que $\lim_{h\to0}\rho(h)=0$
e pela continuidade da transformação linear $\dd g\big(f(x)\big)$
obtemos $\lim_{h\to0}\dd g\big(f(x)\big)\cdot\rho(h)=0$. Além do
mais, temos:
$$\Vert k\Vert\le\big[\Vert\dd f(x)\Vert+\Vert\rho(h)\Vert\big]\Vert
h\Vert,$$ donde segue que $\lim_{h\to0}k=0$ e que ${\Vert
k\Vert\over\Vert h\Vert}$ é uma quantidade limitada para $h$ numa
vizinhança da origem. A conclusão segue do fato que
$\lim_{k\to0}\sigma(k)=0$.\fimprova

\goodbreak
\medskip

\item{(2)} {\bf A igualdade e a desigualdade do valor médio}.

\smallskip

Recordem do Cálculo 1 o seguinte:

\proclaim Teorema. (do valor médio) Se $f:[a,b]\to\R$ é uma função
contínua, derivável no intervalo aberto $(a,b)$, então existe
$c\in(a,b)$ tal que:
$$f(b)-f(a)=f'(c)(b-a).$$

\Prova Fazemos primeiro o caso $f(a)=f(b)$ (conhecido como o {\sl
Teorema de Rolle}). A função contínua $f$ assume um máximo e um
mínimo no compacto $[a,b]$; se ambos fossem assumidos nas
extremidades do intervalo então $f$ seria constante (pois
$f(a)=f(b)$) --- daí $f'\equiv0$. Se $f$ assume um máximo ou um
mínimo num ponto $c\in(a,b)$ então (como todo mundo deve saber do
Cálculo I) temos $f'(c)=0$.

O caso geral segue diretamente aplicando o Teorema de Rolle para a
função $g(t)=f(t)-\big(f(b)-f(a)\big){t-a\over
b-a}-f(a)$.\fimprova

\smallskip

É muito fácil generalizar o teorema acima para o caso de funções
definidas em abertos de $\R^m$ tomando valores em $\R$; para isso,
recorde que o {\sl segmento de reta\/} ligando dois pontos
$x,y\in\R^m$ é definido por:
$$[x,y]=\big\{(1-t)x+ty:t\in[0,1]\big\}.$$
Denotamos também por $(x,y)$ o {\sl segmento de reta aberto\/}
definido por:
$$(x,y)=\big\{(1-t)x+ty:t\in(0,1)\big\}.$$
(espero que não ocorra confusão com o par ordenado $(x,y)$).

\proclaim Teorema. Sejam $f:U\subset\R^m\to\R$ uma função definida
num aberto $U\subset\R^m$ e $[x,y]\subset U$ um segmento de reta
{\rm contido} em $U$. Suponha que $f$ é contínua nos pontos de
$[x,y]$ e diferenciável nos pontos de $(x,y)$; então existe
$z\in(x,y)$ tal que:
$$f(y)-f(x)=\dd f(z)\cdot(y-x).$$

\Prova Seja $\phi:[0,1]\to\R$ a aplicação definida por
$\phi(t)=f\big((1-t)x+ty\big)$; note que $f$ é bem definida pois
$[x,y]\subset U$. Como $f$ é contínua em $[x,y]$, temos que $\phi$
é contínua; como $f$ é diferenciável em $(x,y)$, segue da regra da
cadeia que $\phi$ é diferenciável em $(0,1)$ e que sua derivada é
dada por:
$$\phi'(t)=\dd f\big((1-t)x+ty\big)\cdot(y-x),$$
para todo $t\in(0,1)$. A conclusão segue diretamente aplicando o
teorema do valor médio do Cálculo I para $\phi$.\fimprova

\smallskip

\noindent{\tt Exemplo}. O teorema do valor médio não vale em geral
para funções a valores vetoriais, i.e., para funções a valores em
$\R^n$ com $n>1$. Por exemplo, a curva diferenciável
$\gamma:[0,2\pi]\to\R^2$ definida por $\gamma(t)=(\cos t,\sen t)$
satisfaz $\gamma(0)=\gamma(2\pi)$ mas não existe $t\in[0,2\pi]$
com $\gamma'(t)=0$ ({\sl para refletir}: por que a idéia óbvia de
aplicar o teorema do valor médio em cada coordenada de $\gamma$
não funciona?).

\smallskip

A generalização correta do teorema do valor médio para funções a
valores vetoriais é dada no seguinte:

\proclaim Teorema. (desigualdade do valor médio) Sejam
$f:U\subset\R^m\to\R^n$ uma função definida num aberto
$U\subset\R^m$ e $[x,y]\subset U$ um segmento de reta {\rm
contido} em $U$. Suponha que $f$ é contínua nos pontos de $[x,y]$
e diferenciável nos pontos de $(x,y)$; vale a desigualdade:
$$\big\Vert f(y)-f(x)\big\Vert\le\sup_{z\in(x,y)}\big\Vert\dd
f(z)\big\Vert\,\Vert y-x\Vert,$$ onde utilizamos normas {\rm
arbitrárias} em $\R^m$ e $\R^n$ e a norma usada em
$\Lin(\R^m,\R^n)$ (para fazer $\big\Vert\dd f(z)\big\Vert$) é a
{\rm norma de operadores} correspondente.

\smallskip

A demonstração da desigualdade do valor médio segue facilmente do
teorema do valor médio se utilizarmos o seguinte:

\proclaim Lema. Seja $V$ um espaço vetorial normado não nulo. Dado
$v\in V$ então existe um funcional linear $\lambda:V\to\R$ com
$\Vert\lambda\Vert=1$ e $\lambda(v)=\Vert v\Vert$.

Dizemos que $\lambda$ é um funcional linear de norma $1$ que {\sl
reproduz a norma\/} do vetor $v\in V$.

\smallskip

\noindent{\tt Observação}. Para quem não sabe, um {\sl funcional
linear\/} num espaço vetorial real $V$ é simplesmente um operador
linear $\lambda:V\to\R$ definido em $V$ tomando valores em $\R$. O
espaço $\Lin(V,\R)$ de todos os funcionais lineares em $V$ é
chamado o {\sl espaço dual\/} de $V$ e é denotado por $V^*$. Mais
adiante no curso precisaremos fazer uma revisão sobre a teoria do
espaço dual --- isso será importante para entender a teoria sobre
integral de linha usando $1$-formas.

\smallskip

\noindent{\bf Prova da desigualdade do valor médio}.\enspace Pelo
lema, existe um funcional linear $\lambda\in{\R^n}^*$ com
$\Vert\lambda\Vert=1$ e que reproduz a norma do vetor
$f(y)-f(x)\in\R^n$, i.e.:
$$\lambda\big(f(y)-f(x)\big)=\big\Vert
f(y)-f(x)\big\Vert.$$ Aplicando o teorema do valor médio para a
função $\lambda\circ f:U\subset\R^m\longrightarrow\R$ obtemos que
existe $z\in(x,y)$ com:
$$\lambda\big(f(y)\big)-\lambda\big(f(x)\big)=\dd(\lambda\circ
f)(z)\cdot(y-x);$$ usando a regra da cadeia e a linearidade de
$\lambda$ obtemos:
$$\lambda\big(f(y)-f(x)\big)=\lambda\big[\dd
f(z)\cdot(y-x)\big].$$ Finalmente, como $\Vert\lambda\Vert=1$ e
$\lambda$ reproduz a norma de $f(y)-f(x)$ obtemos:
$$\eqalign{\big\Vert
f(y)-f(x)\big\Vert&=\lambda\big(f(y)-f(x)\big)=\lambda\big[\dd
f(z)\cdot(y-x)\big]\le\Big\vert\lambda\big[\dd
f(z)\cdot(y-x)\big]\Big\vert\cr &\le\big\Vert\dd
f(z)\cdot(y-x)\big\Vert\le\big\Vert\dd f(z)\big\Vert\,\Vert
y-x\Vert\le\sup_{z\in(x,y)}\big\Vert\dd f(z)\big\Vert\,\Vert
y-x\Vert.\fimprova}$$

\smallskip

O lema sobre a existência de um funcional $\lambda\in V^*$ que
reproduz a norma de um vetor dado é bem fácil de ser provado
quando a norma de $V$ provém de um produto interno $\pint$.
Fazemos assim: tomamos $v\in V$ e consideramos o funcional
$\lambda=\big\langle{v\over\Vert v\Vert},\cdot\big\rangle$
associado ao vetor unitário ${v\over\Vert v\Vert}$; é fácil ver
que $\Vert\lambda\Vert=1$ e que $\lambda(v)=\Vert v\Vert$ (veja o
Exercício 1). Essa demonstração não funciona para $v=0$, mas esse
caso é trivial (basta escolher {\sl qualquer\/} $\lambda\in V^*$
com $\Vert\lambda\Vert=1$).

A demonstração do lema para espaços normados quaisquer seguirá do
seguinte:

\proclaim Teorema de Hahn--Banach. Sejam $V$ um espaço vetorial
normado, $S\subset V$ um subespaço e $\lambda:S\to\R$ um funcional
linear. Então existe uma extensão linear $\mu:V\to\R$ de $\lambda$
(i.e., $\lambda=\mu\vert_S$) tal que
$\Vert\mu\Vert=\Vert\lambda\Vert$ (onde $\Vert\mu\Vert$ e
$\Vert\lambda\Vert$ denotam as normas de operadores de $\mu\in
V^*$ e $\lambda\in S^*$ respectivamente).

\smallskip

Antes de provar o Teorema de Hahn--Banach, vamos mostrar como o
lema sobre funcionais que reproduzem a norma de vetores segue
diretamente de tal teorema. Seja $V$ um espaço vetorial normado e
seja $v\in V$ não nulo (o caso $v=0$ é trivial). Seja $S=\R v
\subset V$ o subespaço unidimensional gerado por $v$. Podemos
definir $\lambda\in S^*$ fazendo $\lambda(v)=\Vert v\Vert$ (pois
$v$ é uma base para $S$) e é fácil ver que $\Vert\lambda\Vert=1$
(a esfera unitária de $S$ possui apenas os vetores
$\pm{v\over\Vert v\Vert}$). Pelo Teorema de Hahn--Banach podemos
estender $\lambda$ a um funcional linear em $V$ que ainda possui
norma $1$ (e que ainda reproduz a norma de $v$, obviamente).

\smallskip

\noindent{\bf Prova do Teorema de Hahn--Banach}.\enspace A parte
central da demonstração do Teorema de Hahn--Banach é a seguinte:
consideramos um vetor $v\in V$ fora de $S$ e mostramos que é
possível estender $\lambda$ para um funcional linear $\mu$ no
espaço $S\oplus\R v$ gerado por $S$ e $v$, de modo que
$\Vert\mu\Vert=\Vert\lambda\Vert$. Uma vez que esse resultado
tenha sido demonstrado, o teorema de Hahn--Banach segue da
seguinte maneira:
\item{(i)} se $V$ tem dimensão finita (que é o caso que nos
interessa) então basta repetir o processo descrito acima um número
finito de vezes e eventualmente obteremos uma extensão de
$\lambda$ para $V$;
\item{(ii)} se $V$ é arbitrário, a conclusão final segue facilmente
do {\sl Lema de Zorn}. O caso de espaços de dimensão infinita é
totalmente irrelevante para esse curso, de modo que quem não tem
familiaridade com o Lema de Zorn pode simplesmente ignorar esse
caso.

\smallskip

Vamos então demonstrar o passo central, i.e., que podemos estender
$\lambda\in S^*$ para $\mu\in(S\oplus\R v)^*$ de modo que
$\Vert\lambda\Vert=\Vert\mu\Vert$. A extensão $\mu$ de $\lambda$
fica totalmente definida quando escolhemos o valor de $\mu$ em $v$
(veja o Exercício 3); esse valor será um número real $c\in\R$.
Daí:
$$\mu(x+tv)=\lambda(x)+tc,$$
para todos $x\in S$ e $t\in\R$. Obviamente teremos
$\Vert\mu\Vert\ge\Vert\lambda\Vert$ para qualquer escolha de $c\in\R$; o
problema é mostrar que é
possível escolher $c\in\R$ de modo que:
$$\big\vert\lambda(x)+tc\big\vert\le\Vert\lambda\Vert\,\Vert
x+tv\Vert,$$ para todos $x\in S$, $t\in\R$. A condição acima é
trivialmente satisfeita para $t=0$; se $t\ne0$, podemos dividir os
dois lados por $\vert t\vert$, obtendo a condição equivalente:
$$\big\vert\lambda\big({\textstyle{x\over
t}}\big)+c\big\vert\le\Vert\lambda\Vert\,\big\Vert{\textstyle{x\over
t}}+v\big\Vert,$$ para todos $x\in S$, $t\in\R\setminus\{0\}$.
Observe que ${x\over t}$ é também um vetor em $S$ e portanto nosso
trabalho ficará completo se mostrarmos que $c\in\R$ pode ser
escolhido de modo que:
$$\big\vert\lambda(y)+c\big\vert\le\Vert\lambda\Vert\,\Vert
y+v\Vert,$$ para todo $y\in S$. A condição acima é equivalente a:
$$-\Vert\lambda\Vert\,\Vert
y+v\Vert\le\lambda(y)+c\le\Vert\lambda\Vert\,\Vert y+v\Vert;$$ a
última é por sua vez equivalente a:
$$-\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y)\le c\le\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y),$$ para todo $y\in S$. Para completar a
demonstração, escolheremos $c\in\R$ tal que:
$$\sup_{y\in S}\big[{-\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y)}\big]\le c\le\inf_{y\in
S}\big[\Vert\lambda\Vert\,\Vert y+v\Vert-\lambda(y)\big];$$ para
mostrar que isso é possível, devemos mostrar antes que:
$$\sup_{y\in S}\big[{-\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y)}\big]\le\inf_{y\in
S}\big[\Vert\lambda\Vert\,\Vert y+v\Vert-\lambda(y)\big].$$ A
condição acima é equivalente a:
$$-\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y)\le\Vert\lambda\Vert\,\Vert
z+v\Vert-\lambda(z),$$ para todos $y,z\in S$. Temos agora:
$$\eqalign{-\Vert\lambda\Vert\,\Vert
y+v\Vert-\lambda(y)&\le\Vert\lambda\Vert\,\Vert
z+v\Vert-\lambda(z)\cr &\Longleftrightarrow
\lambda(z)-\lambda(y)\le\Vert\lambda\Vert\big[\Vert y+v\Vert+\Vert
z+v\Vert\big]\cr
&\Longleftrightarrow\lambda(z-y)\le\Vert\lambda\Vert\big[\Vert
y+v\Vert+\Vert z+v\Vert\big];\cr}$$ mas:
$$\eqalign{\lambda(z-y)\le\big\vert\lambda(z-y)\big\vert&\le\Vert\lambda\Vert\,\Vert
z-y\Vert =\Vert\lambda\Vert\,\big\Vert(z+v)-(y+v)\big\Vert\cr
&\le\Vert\lambda\Vert\big[\Vert z+v\Vert+\Vert
y+v\Vert\big],\cr}$$ para todos $y,z\in S$. Isso completa a
demonstração do Teorema de Hahn--Banach.\fimprova

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Álgebra Linear}.

\medskip

\item{1.} Seja $V$ um espaço vetorial real munido de um produto
interno $\pint$. A forma bilinear $\pint:V\times V\to\R$
corresponde ao operador linear:
$$\phi:V\ni v\longmapsto\langle v,\cdot\rangle\in\Lin(V,\R)=V^*$$
onde $\langle v,\cdot\rangle$ denota o funcional linear $V\ni
w\mapsto\langle v,w\rangle\in\R$.
\smallskip
\itemitem{(a)} Mostre que $\phi$ é injetor ({\sl dica}: $\phi\
\hbox{injetor}\Leftrightarrow\Ker(\phi)=0$; quanto vale
$\phi(v)(v)$?). Conclua que se $V$ tem dimensão finita então
$\phi:V\to V^*$ é um isomorfismo.
\itemitem{(b)} Suponha que $V$ tem dimensão finita e considere
$\Lin(V,\R)=V^*$ munido da norma de operadores. Mostre que $\phi$
é uma isometria, i.e., que $\Vert\phi(v)\Vert=\Vert v\Vert$ para
todo $v\in V$ ({\sl dica}: para mostrar que
$\Vert\phi(v)\Vert\le\Vert v\Vert$ use a desigualdade de
Cauchy-Schwarz; para mostrar a igualdade avalie $\phi(v)$ em
${v\over\Vert v\Vert}$).

\smallskip

\item{2.} ({\sl somas diretas}) Sejam $V$ um espaço vetorial e
$S_1,S_2\subset V$ subespaços. Denotamos por $S_1+S_2$ a {\sl
soma\/} dos subespaços $S_1$ e $S_2$, ou seja:
$$S_1+S_2=\big\{v_1+v_2:v_1\in V_1,\ v_2\in V_2\big\};$$
$S_1+S_2$ coincide com o {\sl subespaço gerado\/} pela união
$S_1\cup S_2$, i.e., $S_1+S_2$ é o menor subespaço de $V$ que
contém $S_1\cup S_2$. Mostre que as seguintes condições são
equivalentes:
\smallskip
\itemitem{(a)} $S_1\cap S_2=\{0\}$ e $V=S_1+S_2$;
\itemitem{(b)} todo $v\in V$ se escreve de modo {\sl único\/} na
forma $v=v_1+v_2$ com $v_1\in S_1$ e $v_2\in S_2$.
\smallskip
Quando uma das condições acima é satisfeita dizemos que $V$ é a
{\sl soma direta\/} (ou a {\sl soma direta interna}) dos
subespaços $S_1$ e $S_2$ e escrevemos $V=S_1\oplus S_2$; dizemos
também que $S_2$ é um {\sl subespaço complementar\/} a $S_1$ ({\sl
não\/} confundir com o {\sl conjunto complementar\/} $V\setminus
S_1=\big\{v\in V:v\not\in S_1\big\}$ que em geral não é sequer um
subespaço). Defina aplicações $P_1:V\to S_1$ e $P_2:V\to S_2$ da
seguinte forma: dado $v\in V$ escreva $v=v_1+v_2$ com $v_1\in
V_1$, $v_2\in V_2$ (isso pode ser feito de modo único) e defina:
$$P_1(v)=v_1,\quad P_2(v)=v_2;$$
mostre que as aplicações $P_1$ e $P_2$ são lineares (elas são
chamadas as {\sl projeções\/} relativas à soma direta $V=S_1\oplus
S_2$).

\smallskip

\item{3.} Seja $V$ um espaço vetorial e suponha que $V$ se escreve
como soma direta $V=S_1\oplus S_2$ de dois subespaços $S_1$ e
$S_2$. Dados um outro espaço vetorial $W$ qualquer e operadores
lineares $T_1:V_1\to W$, $T_2:V_2\to W$, mostre que existe {\sl um
único\/} operador linear $T:V\to W$ tal que $T\vert_{V_1}=T_1$ e
$T\vert_{V_2}=T_2$ ({\sl dica}: defina $T=T_1\circ P_1+T_2\circ P_2$, onde
$P_1$ e $P_2$ denotam as projeções relativas à soma direta $V=S_1\oplus S_2$).

\goodbreak
\smallskip

\item{4.} Sejam $V$ um espaço vetorial e $S_1,S_2\subset V$
subespaços de $V$. Mostre que:
\smallskip
\itemitem{(a)} se $V=S_1\oplus S_2$ então a concatenação de uma base
qualquer de $S_1$ com uma base qualquer de $S_2$ produz uma base
de $V$;
\itemitem{(b)} se a concatenação de alguma base de $S_1$ com alguma base
de $S_2$ produz uma base de $V$ então $V=S_1\oplus S_2$.
\smallskip Conclua que todo subespaço de $V$ admite um subespaço
complementar ({\sl dica}: todo subconjunto linearmente
independente estende-se a uma base).

\smallskip

\item{5.} ({\sl somas diretas externas}) Se $S_1$ e $S_2$ são
espaços vetoriais (não necessariamente sub\-espaços de um espaço
vetorial maior dado) então definimos no produto cartesiano
$S_1\times S_2$ as operações de {\sl soma e produto por escalar
coordenada por coordenada}:
$$(v_1,v_2)+(w_1,w_2)=(v_1+w_1,v_2+w_2),\quad
c(v_1,v_2)=(cv_1,cv_2).$$ Mostre que $S_1\times S_2$ munido das
operações acima torna-se um espaço vetorial; dizemos que
$S_1\times S_2$ é a {\sl soma direta\/} (ou a {\sl soma direta
externa}) dos espaços vetoriais $S_1$ e $S_2$. Usualmente
escrevemos $S_1\oplus S_2$ em vez de $S_1\times S_2$.
\smallskip
\itemitem{(a)} Se $S_1\oplus S_2$ denota a soma direta externa de
$S_1$ e $S_2$, mostre que as aplicações:
$$S_1\ni v_1\longmapsto(v_1,0)\in S_1\oplus S_2,\quad S_2\ni
v_2\longmapsto(0,v_2)\in S_1\oplus S_2$$ são lineares injetoras e
que $S_1\oplus S_2$ é a soma direta interna de suas imagens. As
imagens das aplicações lineares injetoras acima são usualmente
identificadas com os espaços $S_1$ e $S_2$.
\itemitem{(b)} Se um espaço vetorial $V$ escreve-se como soma
direta interna de subespaços $S_1$ e $S_2$, mostre que a
aplicação:
$$\underbrace{S_1\oplus S_2}_{\scriptstyle\rm soma\ direta\atop
\scriptstyle\rm externa}\ni(v_1,v_2)\longmapsto
v_1+v_2\in\underbrace{S_1\oplus S_2}_{\scriptstyle\rm soma\
direta\atop\scriptstyle\rm interna}=V$$ é um isomorfismo.

\smallskip

\item{6.} ({\sl somas diretas arbitrárias}) Tente generalizar os
Exercícios 2--5 acima para o caso de somas diretas de uma
quantidade arbitrária (até infinita) de subespaços. Vamos dar
apenas algumas idéias centrais:
\smallskip
\itemitem{(i)} se $V$ é um espaço vetorial e $(S_i)_{i\in I}$ é
uma família de subespaços $S_i\subset V$ então denota-se por
$\sum_{i\in I}S_i$ a {\sl soma\/} dos subespaços $S_i$ que é o
subespaço gerado pela união $\bigcup_{i\in I}S_i$; na prática,
$\sum_{i\in I}S_i$ consiste de todos os elementos da forma
$\sum_{i\in I}v_i$, onde cada $v_i\in S_i$ e $v_i\ne0$ no {\sl
máximo para um número finito\/} de $i$'s ({\sl não se faz somas
infinitas de vetores});
\itemitem{(ii)} a condição $S_1\cap S_2=\{0\}$ no item (a) do Exercício 2 deve ser trocada
por:
$$S_i\cap\sum_{\scriptstyle j\in I\atop\scriptstyle j\ne i}S_j=\{0\},$$
para todo $i\in I$ ({\sl não
basta\/} $S_i\cap S_j=\{0\}$ para todos $i,j\in I$);
\itemitem{(iii)} o item (b) do Exercício 2 deve ser trocado por
``cada $v\in V$ se escreve de modo único na forma $v=\sum_{i\in
I}v_i$, $v_i\in V_i$'', onde a soma $\sum_{i\in I}v_i$ deve ser
entendida no sentido que $v_i\ne0$ no máximo para um número finito
de $i$'s (mais uma vez: {\sl não se faz\/} somas infinitas de
vetores);
\itemitem{(iv)} a notação padrão para uma soma direta de uma
família arbitrária (interna ou externa) é $\bigoplus_{i\in I}S_i$;
\itemitem{(v)} no Exercício 5, quando define-se somas diretas
externas, {\sl não se deve usar\/} o produto cartesiano
$\prod_{i\in I}S_i$, mas sim o subconjunto de $\prod_{i\in I}S_i$
formado pelas famílias $(v_i)_{i\in I}$ tais que o conjunto
$\big\{i\in I:v_i\ne0\big\}$ é finito (uma tal família é chamada
{\sl quase nula}).

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{7.} Mostre que a noção de aplicação diferenciável não
depende da escolha particular de uma norma, i.e., mostre que se
$f:U\subset\R^m\to\R^n$ é diferenciável no ponto $x\in U$ quando
usamos as normas Euclideanas então $f$ continua diferenciável em
$x\in U$ (com a mesma diferencial) se usarmos outras normas em
$\R^m$ e em $\R^n$ ({\sl observação}: a noção de limite é
topológica e portanto não se altera quando trocamos uma métrica
por outra equivalente; tome um cuidado porém: a norma do espaço
aparece explicitamente na definição de diferenciabilidade).

\vfil\eject

\centerline{\bf Aula número $10$ (05/04)}
\bigskip
\bigskip

\item{(1)} {\bf Um critério para diferenciabilidade em termos de derivadas
parciais}.

\smallskip

\proclaim Teorema. Seja $f:U\subset\R^m\to\R^n$ uma função
definida num aberto $U\subset\R^m$. Se $f$ admite derivadas
parciais ${\partial f\over\partial x^i}(x)$, $i=1,\ldots,m$, em
todo ponto $x\in U$ e se essas derivadas parciais são funções
contínuas ${\partial f\over\partial x^i}:U\to\R^n$ num ponto $x\in
U$ então $f$ é diferenciável no ponto $x$ e sua diferencial é dada
por:
$$\dd f(x)\cdot h=\sum_{i=1}^m{\partial f\over\partial
x^i}(x)h_i,$$ para todo $h\in\R^m$ (resumindo: ``se uma função
admite derivadas parciais contínuas num aberto então ela é
diferenciável'').

\Prova Uma vez que fique provado que $f$ é diferenciável em $x$, a
fórmula acima para $\dd f(x)$ seguirá trivialmente do fato que
$\dd f(x)$ é linear e do fato que ${\partial f\over\partial
x^i}(x)$ é obtida aplicando $\dd f(x)$ no $i$-ésimo vetor da base
canônica de $\R^m$. Na verdade, como o lado direito da fórmula
acima para $\dd f(x)$ define de fato uma aplicação linear em $h$,
podemos usar tal fórmula como ``candidato'' a $\dd f(x)$ quando
provamos que $f$ é diferenciável em $x$; escrevemos então:
$$f(x+h)=f(x)+\sum_{i=1}^m{\partial f\over\partial
x^i}(x)h_i+r(h),$$ e devemos mostrar que
$\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$. Temos:
$$f(x+h)-f(x)=\sum_{i=1}^m\Delta_i(x,h),$$
onde:
$$\Delta_i(x,h)=f(x_1+h_1,\ldots,x_i+h_i,x_{i+1},\ldots,x_m)-f(x_1+h_1,\ldots,x_{i-1}+h_{i-1},x_i,\ldots,x_m),$$
para $i=1,\ldots,m$. A idéia é estimar o valor de $\Delta_i(x,h)$
usando o teorema do valor médio. Seria possível agora terminar a
demonstração usando a {\sl desigualdade\/} do valor médio; por
outro lado, como ``$\hbox{$f$ é
diferenciável}\Leftrightarrow\hbox{cada $f^j$ é diferenciável}$'',
podemos supor sem perda de generalidade que $n=1$ e daí podemos
usar diretamente a {\sl igualdade\/} do valor médio. Como
$U\subset\R^m$ é aberto, podemos escolher $\varepsilon>0$ tal que:
$$\Vert h\Vert_\infty\le\varepsilon\Longleftrightarrow\vert h_1\vert\le\varepsilon,\ldots,\vert h_m\vert\le\varepsilon\Longrightarrow x+h\in U;$$
a hipótese que $f$ admite a $i$-ésima derivada parcial em $U$,
implica que a função:
$$[x_i-\varepsilon,x_i+\varepsilon]\ni t\longmapsto
f(x_1+h_1,\ldots,x_{i-1}+h_{i-1},t,x_{i+1},\ldots,x_m)\in\R$$ é
derivável e portanto o teorema do valor médio nos dá:
$$\Delta_i(x,h)=h_i{\partial f\over\partial
x^i}(x_1+h_1,\ldots,x_{i-1}+h_{i-1},c_i,x_{i+1},\ldots,x_m),$$
para algum $c_i=c_i(x,h)$ entre $x_i$ e $x_i+h_i$, desde que
$\Vert h\Vert_\infty\le\varepsilon$. Se $\Vert
h\Vert_\infty\le\varepsilon$ obtemos então:
$$r(h)=\sum_{i=1}^m\rho_i(x,h)h_i,$$
onde:
$$\rho_i(x,h)={\partial f\over\partial
x^i}(x_1+h_1,\ldots,x_{i-1}+h_{i-1},c_i,x_{i+1},\ldots,x_m)-{\partial
f\over\partial x^i}(x_1,\ldots,x_m).$$ A continuidade de
${\partial f\over\partial x^i}$ em $x$ implica que
$\lim_{h\to0}\rho_i(x,h)=0$; além do mais:
$${\big\vert r(h)\big\vert\over\;\;\Vert
h\Vert_\infty}\le\sum_{i=1}^m\big\vert\rho_i(x,h)\big\vert{\vert
h_i\vert\over\;\;\Vert
h\Vert_\infty}\le\sum_{i=1}^m\big\vert\rho_i(x,h)\big\vert,$$ o
que mostra que $\lim_{h\to0}{r(h)\over\;\;\Vert
h\Vert_\infty}=0$.\fimprova

\medskip

\item{(2)} {\bf Diferenciação de ordem superior}.

\smallskip

Seja $f:U\subset\R^m\to\R^n$ uma função definida num aberto
$U\subset\R^m$. Se $f$ é diferenciável em todos os pontos de $U$
então podemos considerar a função:
$$\dd f:U\subset\R^m\longrightarrow\Lin(\R^m,\R^n);$$
essa é novamente uma função definida num aberto de um espaço
vetorial real de dimensão finita e tomando valores num espaço
vetorial real de dimensão finita. Se a mesma for diferenciável em
todos os pontos de $U$ podemos considerar a função:
$$\dd(\dd
f)=\dd^2f:U\subset\R^m\longrightarrow\Lin\big(\R^m,\Lin(\R^m,\R^n)\big);$$
iterando o raciocínio acima, vemos que se $f$ pode ser
diferenciada $k-1$ vezes numa vizinhança de $x\in U$ e se essa
$(k-1)$-ésima diferencial pode ser diferenciada no ponto $x$ então
fica bem definida a {\sl diferencial de $k$-ésima ordem\/} de $f$
no ponto $x$, que é um operador linear da forma:
$$\dd^kf(x):\R^m\longrightarrow\underbrace{\Lin\big(\R^m,\Lin\big(\R^m,\ldots,\Lin(\R^m,\R^n)\ldots\big)\big)}_{k-1\ {\rm Lin's}}.$$
Quando $f$ pode ser diferenciada $k$ vezes em todos os pontos de
$U$ obtemos uma aplicação:
$$\dd^kf:U\subset\R^m\longrightarrow\underbrace{\Lin\big(\R^m,\Lin\big(\R^m,\ldots,\Lin(\R^m,\R^n)\ldots\big)\big)}_{k\ {\rm Lin's}}.$$
\smallskip

\proclaim Definição. Se $f$ pode ser diferenciada $k-1$ vezes numa
vizinhança de $x$ e se sua $(k-1)$-ésima diferencial pode ser
diferenciada no ponto $x$ então dizemos que $f$ é {\rm $k$ vezes
diferenciável} no ponto $x$; a aplicação linear $\dd^kf(x)$ é
chamada a {\rm diferencial de $k$-ésima ordem} de $f$ no ponto
$x$. Quando $f$ é $k$ vezes diferenciável em todos os pontos de
$U$, dizemos que $f$ é {\rm $k$ vezes diferenciável em $U$}. Se
$f$ é $k$ vezes diferenciável em $U$ e se a aplicação $\dd^kf$ é
contínua em $U$ então dizemos que $f$ é uma {\rm função de classe
$C^k$} em $U$. Quando $f$ é $k$ vezes diferenciável para todo
$k\ge1$ (e portanto de classe $C^k$ para todo $k\ge1$) dizemos que
$f$ é uma {\rm função de classe $C^\infty$\/} em $U$.

\smallskip

\noindent{\tt Observação}. Uma função contínua é às vezes chamada
uma {\sl função de classe $C^0$}.

\smallskip

A coisa por enquanto parece feia mas não é tanto assim. Como
podemos pensar num operador:
$$T\in\Lin\big(V_1,\Lin\big(V_2,\ldots,\Lin(V_k,W)\ldots\big)\big)$$
de maneira prática? Aplicamos $T$ num vetor $v_1\in V_1$ e obtemos
um operador:
$$T(v_1)\in\Lin\big(V_2,\Lin\big(V_3,\ldots,\Lin(V_k,W)\ldots\big)\big);$$
aplicamos agora $T(v_1)$ sucessivamente num vetor $v_2\in V_2$,
num vetor $v_3\in V_3$ e assim por diante. Obtemos um vetor:
$$T(v_1)(v_2)\ldots(v_k)\in W.$$
Em vez de aplicar $T$ sucessivamente nos vetores $v_1$, $v_2$,
\dots, $v_k$, poderíamos pensar diretamente na regra que associa a
$k$-upla $(v_1,\ldots,v_k)$ a um vetor de $W$. Definimos:
$$B:V_1\times V_2\times\cdots\times V_k\longrightarrow W$$
fazendo $B(v_1,\ldots,v_k)=T(v_1)\ldots(v_k)$. Não é difícil ver
que a linearidade de $T$ é equivalente à {\sl multi-linearidade\/}
de $B$. Na verdade, temos um isomorfismo:
$$\psi_k:\Lin\big(V_1,\Lin\big(V_2,\ldots\Lin(V_k,W)\ldots\big)\big)\ni
T\longrightarrow B\in\Multlin_k(V_1,V_2,\ldots,V_k;W)$$ definido
como acima; denotamos por $\Multlin_k(V_1,V_2,\ldots,V_k;W)$ o
espaço de operadores $k$-lineares $B:V_1\times\cdots\times V_k\to
W$. A demonstração formal de que a aplicação $\psi_k$ é de fato
bem definida e é um isomorfismo de espaços vetoriais pode ser
feita por indução em $k$ com todos os detalhes
--- acreditamos que pouco se ganha com isso. Quem já trabalhou com
o caso $k=2$ e adquiriu alguma intuição sobre o assunto deve se
convencer facilmente de que a mesma idéia funciona para $k$
arbitrário.

\smallskip

\noindent{\tt Notação}. Quando $V_1=V_2=\cdots=V_k=V$ o espaço
$\Multlin_k(V_1,\ldots,V_k;W)$ será denotado simplesmente por
$\Multlin_k(V;W)$.

\smallskip

\noindent{\tt Observação}. Usaremos o nome $\psi_k$ para o isomorfismo
descrito acima sejam lá quais forem os espaços $V_1$, \dots, $V_k$, $W$
envolvidos; em princípio, isso introduz alguma ambigüidade de notação, mas
essa ambigüidade dificilmente gerará confusão.

\smallskip

Utilizaremos agora o isomorfismo $\psi_k$ para identificar a
diferencial de $k$-ésima ordem $\dd^kf(x)$ com um objeto mais
amigável. Definimos:
$$\dd^{(k)}f(x)=\psi_k\big(\dd^kf(x)\big);$$
isso significa que $\dd^{(k)}f(x)\in\Multlin_k(\R^m;\R^n)$ e que:
$$\dd^{(k)}f(x)(v_1,\ldots,v_k)=\dd^kf(x)(v_1)\ldots(v_k),$$
para todos $v_1,\ldots,v_k\in\R^m$. Na prática, é comum denotar os
objetos $\dd^kf(x)$ e $\dd^{(k)}f(x)$ pelo mesmo símbolo
(tipicamente $\dd^kf(x)$) --- o uso do isomorfismo $\psi_k$ deve
ser automatizado! No início, porém, é mais saudável utilizar uma
notação mais explícita (i.e., não omitir $\psi_k$). Observamos que
as notações $\dd^kf(x)$ e $\dd^{(k)}f(x)$ {\sl não são usuais};
estamos utilizando-as apenas neste texto --- muitos autores
identificam os objetos $\dd^kf(x)$ e $\dd^{(k)}f(x)$ desde o
princípio, causando uma certa confusão para o leitor iniciante.
Nós usaremos o nome ``{\sl diferencial de $k$-ésima ordem}'' tanto
para $\dd^kf(x)$ como para $\dd^{(k)}f(x)$ --- esperamos que
apenas a distinção a nível de notação simbólica seja suficiente
para evitar a confusão.

\smallskip

\proclaim Definição. A aplicação bilinear
$\dd^{(2)}f(x):\R^m\times\R^m\to\R^n$ associada à diferencial segunda de $f$
no ponto $x$ é conhecida como o {\rm Hessiano} de $f$ no ponto $x$
e é denotada por $\Hess(f)_x$.

\smallskip

\noindent{\tt Observação}. Uma aplicação $f$ é de classe $C^k$ se e somente se
$f$ for $k$ vezes diferenciável e a aplicação $\dd^{(k)}f$ for contínua. De
fato, o isomorfismo $\psi_k$ é um homeomorfismo e portanto $\dd^kf$ é contínua
se e somente se $\dd^{(k)}f=\psi_k\circ\dd^kf$ é contínua.

\smallskip

\noindent{\tt Observação}. Segue diretamente da definição da diferencial de
$k$-ésima ordem que:
$$\dd^{k+1}f(x)=\dd\big(\dd^kf)(x).$$
Diferenciando a igualdade $\dd^{(k)}f=\psi_k\circ\dd^kf$ e utilizando a regra da
cadeia obtemos:
$$\dd\big(\dd^{(k)}f\big)(x)=\psi_k\circ\dd^{k+1}f(x):\R^m\longrightarrow\Multlin_k(\R^m;\R^n);$$
isso significa que:
$$\dd\big(\dd^{(k)}f\big)(x)(v_1)(v_2,\ldots,v_{k+1})=\dd^{(k+1)}f(x)(v_1,\ldots,v_{k+1}),\eqno{(*)}$$
para todos $v_1,\ldots,v_{k+1}\in\R^m$.

\smallskip

\noindent{\tt Exemplo}. Uma função constante é de classe
$C^\infty$ pois todas as suas diferenciais são nulas. Uma
aplicação linear $T:\R^m\to\R^n$ também é de classe $C^\infty$
pois sua diferencial
$$\dd T:\R^m\longrightarrow\Lin(\R^m,\R^n)$$
é constante;
de fato, temos $\dd T(x)=T$ para todo $x\in\R^m$. Uma aplicação
bilinear $B:\R^m\times\R^n\to\R^p$ também é de classe $C^\infty$.
De fato, a diferencial de $B$:
$$\dd B:\R^m\times\R^n\to\Lin(\R^m\times\R^n,\R^p)$$
é dada por:
$$\dd B(x,y)\cdot(h,k)=B(x,k)+B(h,y);$$
não é difícil ver que $\dd B$ é linear, o que conclui a
demonstração do fato que $B$ é de classe $C^\infty$. Segue também
da linearidade de $\dd B$ que a diferencial segunda de $B$ (i.e.,
o Hessiano de $B$) é dada por:
$$\dd^{(2)}B(x,y)\cdot\big((h_1,k_1),(h_2,k_2)\big)=\dd
B(h_1,k_1)\cdot(h_2,k_2)=B(h_1,k_2)+B(h_2,k_1).$$
É verdade também que aplicações multi-lineares quaisquer são de classe
$C^\infty$ (veja o Exercício 1).

\smallskip

\proclaim Teorema. Sejam $f,g:U\subset\R^m\to\R^n$ funções
definidas num aberto $U\subset\R^m$. Se $f$ e $g$ são $k$ vezes
diferenciáveis num ponto $x\in U$ então $f+g$ é $k$ vezes diferenciável no
ponto $x$ e valem as identidades:
$$\dd^k(f+g)(x)=\dd^kf(x)+\dd^kg(x),\quad\dd^{(k)}(f+g)(x)=\dd^{(k)}f(x)+\dd^{(k)}g(x);$$
em particular, se $f$ e $g$ são de classe $C^k$ então $f+g$ também é de
classe $C^k$.

\Prova Usamos indução em $k$. O caso $k=1$ já é conhecido. Suponha
o resultado válido para um certo $k\ge1$. Se $f$ e $g$ são $k+1$ vezes
diferenciáveis em $x$ então $f$ e $g$ também são $k$ vezes diferenciáveis em
$x$; pela hipótese de indução, $f+g$ é $k$ vezes diferenciável em $x$ e vale a
identidade $\dd^k(f+g)=\dd^kf+\dd^kg$. Como $\dd^kf$ e $\dd^kg$ são
diferenciáveis no ponto $x$, segue que $\dd^k(f+g)$ é diferenciável no ponto
$x$, donde $f+g$ é $k+1$ vezes diferenciável no ponto $x$. Calculamos agora:
$$\eqalign{\dd^{k+1}(f+g)(x)&=\dd\big[\dd^k(f+g)\big](x)=\dd\big[\dd^kf+\dd^kg\big](x)=\dd\big(\dd^kf\big)(x)+\dd\big(\dd^kg\big)(x)\cr
&=\dd^{k+1}f(x)+\dd^{k+1}g(x);\cr}$$
isso completa a demonstração do passo de indução. Observe agora que:
$$\eqalign{\dd^{(k)}(f+g)(x)&=\psi_k\big(\dd^k(f+g)(x)\big)=\psi_k\big(\dd^kf(x)+\dd^kg(x)\big)=\psi_k\big(\dd^kf(x)\big)+\psi_k\big(\dd^kg(x)\big)\cr
&=\dd^{(k)}f(x)+\dd^{(k)}g(x).\cr}$$
Quando $f$ e $g$ são de classe
$C^k$ então $\dd^kf$ e $\dd^kg$ são contínuas, donde a soma
$\dd^k(f+g)=\dd^kf+\dd^kg$ é contínua e $f+g$ também é de classe $C^k$.\fimprova

\smallskip

\proclaim Lema. Seja $f:U\subset\R^m\to\R^n$ uma função definida
num aberto $U\subset\R^m$ e seja $T:\R^n\to\R^p$ um operador
linear. Se $f$ é $k$ vezes diferenciável num ponto $x\in U$ então
$T\circ f$ é $k$ vezes diferenciável no ponto $x$ e vale a
identidade:
$$\dd^{(k)}(T\circ
f)(x)=T\circ\dd^{(k)}f(x):\underbrace{\R^m\times\cdots\times\R^m}_{k\
{\rm vezes}}\longrightarrow\R^p;$$ em particular, se $f$ é de
classe $C^k$ então $T\circ f$ é de classe $C^k$.

\Prova Usamos indução em $k$. O caso $k=1$ já é conhecido. Suponha
o resultado válido para um certo $k\ge1$ e seja $f$ uma função
$k+1$ vezes diferenciável no ponto $x\in U$. Em particular, $f$ é
$k$ vezes diferenciável no ponto $x$, donde pela hipótese de
indução a função $T\circ f$ é $k$ vezes diferenciável no ponto $x$
e vale a identidade $\dd^{(k)}(T\circ f)(x)=T\circ\dd^{(k)}f(x)$.
Considere o operador linear:
$$\tau:\Multlin_k(\R^m;\R^n)\ni B\longmapsto T\circ
B\in\Multlin_k(\R^m;\R^p);$$ sabemos que:
$$\dd^{(k)}(T\circ f)=\tau\circ\dd^{(k)}f.$$
Como $\dd^k(T\circ f)=\psi_k^{-1}\circ\dd^{(k)}(T\circ f)$ e como
$\dd^{(k)}f=\psi_k\circ\dd^kf$ é diferenciável em $x$, segue da identidade acima que
$\dd^k(T\circ f)$ é diferenciável no ponto $x$, donde $T\circ f$ é
$k+1$ vezes diferenciável no ponto $x$. Diferenciando a identidade
acima e utilizando a regra da cadeia obtemos:
$$\dd\big(\dd^{(k)}(T\circ
f)\big)(x)=\tau\circ\dd\big(\dd^{(k)}f\big)(x):\R^m\longrightarrow\Multlin_k(\R^m;\R^p);$$
aplicando os dois lados da igualdade acima em $v_1\in\R^m$ e
depois na $k$-upla $(v_2,\ldots,v_{k+1})$ obtemos:
$$\eqalign{\dd\big(\dd^{(k)}(T\circ
f)\big)(x)(v_1)(v_2,\ldots,v_{k+1})&=\tau\Big[\dd\big(\dd^{(k)}f\big)(x)(v_1)\Big](v_2,\ldots,v_{k+1})\cr
&=\Big[T\circ\dd\big(\dd^{(k)}f\big)(x)(v_1)\Big](v_2,\ldots,v_{k+1})\cr
&=T\Big[\dd\big(\dd^{(k)}f\big)(x)(v_1)(v_2,\ldots,v_{k+1})\Big].\cr}$$
Usando a relação observada anteriormente (veja fórmula $(*)$)
entre $\dd\big(\dd^{(k)}f\big)$ e $\dd^{(k+1)}f$ obtemos
finalmente:
$$\dd^{(k+1)}(T\circ
f)(x)(v_1,\ldots,v_{k+1})=T\big[\dd^{(k+1)}f(x)(v_1,\ldots,v_{k+1})\big],$$
o que completa a demonstração do passo de indução. Finalmente, se
$f$ é de classe $C^k$ então $\dd^{(k)}f$ é contínua e portanto
$\dd^{(k)}(T\circ f)=\tau\circ\dd^{(k)}f$ é contínua, donde
$T\circ f$ é de classe $C^k$.\fimprova

\smallskip

\proclaim Corolário. Uma função $f:U\subset\R^m\to\R^n$ é $k$
vezes diferenciável num ponto $x\in U$ se e somente se cada
coordenada $f^i:U\to\R$ é $k$ vezes diferenciável no ponto $x$;
além do mais, a $i$-ésima coordenada de
$\dd^{(k)}f(x)\in\Multlin_k(\R^m;\R^n)$ coincide com
$\dd^{(k)}f^i(x)\in\Multlin_k(\R^m;\R)$. Analogamente, $f$ é de
classe $C^k$ se e somente se cada $f^i$ é de classe $C^k$.

\Prova Seja $\pi_i:\R^n\to\R$ a $i$-ésima projeção. Temos
$f^i=\pi_i\circ f$; como $\pi_i$ é linear, o lema nos diz que se
$f$ é $k$ vezes diferenciável no ponto $x\in U$ então cada $f^i$ é
$k$ vezes diferenciável em $x$ e que a $i$-ésima coordenada de
$\dd^{(k)}f(x)$ coincide com $\dd^{(k)}f^i(x)$. Reciprocamente,
suponha que cada $f^i$ é $k$ vezes diferenciável no ponto $x$; se
$e_i\in\R^n$ denota o $i$-ésimo vetor da base canônica então a
aplicação:
$$\R\ni c\longmapsto c\,e_i\in\R^n$$
é linear donde pelo lema a aplicação $U\ni y\mapsto f^i(y)e_i$ é
$k$ vezes diferenciável no ponto $x$. Como $f=\sum_{i=1}^nf^ie_i$,
concluímos que $f$ é $k$ vezes diferenciável no ponto $x$.
A demonstração de que $f$ é de classe $C^k$ se e somente se cada $f^i$ é de
classe $C^k$ é feita de maneira análoga.\fimprova

\goodbreak
\smallskip

\proclaim Teorema. (``regra da cadeia'' $C^k$) Sejam $f:U\subset\R^m\to\R^n$,
$g:V\subset\R^n\to\R^p$ funções definidas em abertos
$U\subset\R^m$, $V\subset\R^n$, de modo que $f(U)\subset V$. Se
$f$ é $k$ vezes diferenciável num ponto $x\in U$ e $g$ é $k$ vezes
diferenciável no ponto $f(x)\in V$ então a aplicação composta
$g\circ f$ é $k$ vezes diferenciável no ponto $x$. Além do mais,
se $f$ e $g$ são de classe $C^k$ então também $g\circ f$ é de
classe $C^k$.

\Prova Seja ${\cal
C}:\Lin(\R^n,\R^p)\times\Lin(\R^m,\R^n)\to\Lin(\R^m,\R^p)$ a {\sl
aplicação composição\/} definida por ${\cal C}(S,T)=S\circ T$.
Sabemos que ${\cal C}$ é bilinear; da regra da cadeia segue que:
$$\dd(g\circ f)={\cal C}\circ(\dd g\circ f,\dd f).$$
Mostraremos agora o resultado desejado por indução em $k$. O caso
$k=1$ é conhecido. Supondo o resultado válido para um certo
$k\ge1$, suponha que $f$ é $k+1$ vezes diferenciável em $x$ e que
$g$ é $k+1$ vezes diferenciável em $f(x)$. Daí $g\circ f$ é
diferenciável numa vizinhança de $x$ e vale a fórmula acima para
$\dd(g\circ f)$. Sabemos que $\dd f$ é $k$ vezes diferenciável no
ponto $x$, que $f$ é $k$ vezes diferenciável no ponto $x$, que
$\dd g$ é $k$ vezes diferenciável no ponto $f(x)$ e que ${\cal C}$
é $k$ vezes diferenciável em qualquer ponto (na verdade ${\cal C}$
é de classe $C^\infty$) --- segue da hipótese de indução que $\dd
g\circ f$ é $k$ vezes diferenciável no ponto $x$ (e portanto que
$(\dd g\circ f,\dd f)$ é $k$ vezes diferenciável no ponto $x$) e
que $\dd(g\circ f)={\cal C}\circ(\dd g\circ f,\dd f)$ é $k$ vezes
diferenciável no ponto $x$. Daí $g\circ f$ é $k+1$ vezes
diferenciável no ponto $x$. Supondo agora que $f$ e $g$ são de
classe $C^{k+1}$, concluímos de modo análogo usando a identidade
$\dd(g\circ f)={\cal C}\circ(\dd g\circ f,\dd f)$ e a hipótese de
indução que $\dd(g\circ f)$ é de classe $C^k$; daí $g\circ f$ é de
classe $C^{k+1}$, o que completa a demonstração.\fimprova

\smallskip

\proclaim Corolário. Se $f,g:U\subset\R^m\to\R$ são $k$ vezes diferenciáveis
no ponto $x\in U$ então o produto $fg:U\to\R$ é $k$ vezes diferenciável em
$x$. Além do mais, se $f$ e $g$ são de classe $C^k$ então também $fg$ é de
classe $C^k$.

\Prova Observe que $fg=B\circ(f,g)$, onde $B:\R\times\R\to\R$ denota a
multiplicação de números reais.\fimprova

\smallskip

\noindent{\tt Observação}. Juntando todos os resultados mostrados até agora,
já podemos concluir que se as coordenadas $f^i$ de uma função
$f:U\subset\R^m\to\R^n$
são dadas por {\sl fórmulas\/} (i.e., funções elementares) que não envolvem
raízes de expressões que podem se anular então $f$ é de classe $C^\infty$.

\smallskip

\noindent{\tt Exemplo}. Seja $f:U\subset\R^m\to\R^n$ uma função
diferenciável em $U$. Podemos identificar o espaço vetorial
$\Lin(\R^m,\R^n)$ com o espaço das matrizes reais $n\times m$, o
qual pode ser identificado com $\R^{mn}$ da maneira óbvia. Temos
que a função $\dd f:U\to\Lin(\R^m,\R^n)$ é contínua se e somente
se é contínua coordenada por coordenada quando identificamos
$\Lin(\R^m,\R^n)$ com $\R^{mn}$. Recordando que a matriz que
representa o operador linear $\dd f(x)$ na base canônica (i.e., a
matriz Jacobiana de $f$ no ponto $x$) possui como entradas as
derivadas parciais ${\partial f^i\over\partial x^j}(x)$, vemos que
$f$ é de classe $C^1$ se e somente se $f$ é diferenciável e possui
derivadas parciais contínuas --- mas já sabemos que uma função que
possui derivadas parciais contínuas é diferenciável. Mostramos
então que:
$$\hbox{$f$ é de classe $C^1$}\Longleftrightarrow\hbox{$f$ possui
todas as derivadas parciais ${\partial f^i\over\partial x^j}$
contínuas}.$$ Obviamente uma função $f$ é de classe $C^k$ se e
somente se $f$ for diferenciável e sua diferencial $\dd f$ for de
classe $C^{k-1}$; mas $\dd f$ é de classe $C^{k-1}$ se e somente
se suas coordenadas ${\partial f^i\over\partial x^j}$ são de
classe $C^{k-1}$. Mostramos então (por indução em $k$) que $f$ é de classe
$C^k$ se e
somente se $f$ admite {\sl derivadas parciais de ordem $k$}:
$${\partial\over\partial x^{j_1}}{\partial\over\partial
x^{j_2}}\cdots{\partial f^i\over\partial
x^{j_k}}(x)={\partial^kf^i\over\partial x^{j_1}\partial
x^{j_2}\ldots\partial x^{j_k}}(x),$$ para todos
$j_1,\ldots,j_k=1,\ldots,m$, $i=1,\ldots,n$, todas contínuas em
$U$.

\smallskip

\proclaim Teorema. Suponha que $f:U\subset\R^m\to\R^n$ é $k$ vezes
diferenciável num ponto $x\in U$. Então dados vetores
$e_{j_1},\ldots,e_{j_k}\in\R^m$ da base canônica de $\R^m$ temos:
$$\dd^{(k)}f(x)(e_{j_1},\ldots,e_{j_k})={\partial^kf\over\partial x^{j_1}\partial
x^{j_2}\ldots\partial x^{j_k}}(x).$$

\Prova Fazemos indução em $k$. O caso $k=1$ é a definição de
derivada parcial. Suponha o resultado válido para um certo $k\ge1$
e suponha que $f$ é $k+1$ vezes diferenciável no ponto $x$.
Fixados $j_1,\ldots,j_{k+1}\in\{1,\ldots,m\}$, denote por
$\lambda$ o operador linear de {\sl avaliação na $k$-upla
$(e_{j_2},\ldots,e_{j_{k+1}})$}, i.e.:
$$\lambda:\Multlin_k(\R^m;\R^n)\ni B\longmapsto
B(e_{j_2},\ldots,e_{j_{k+1}})\in\R^n.$$ Pela hipótese de indução
temos:
$$\lambda\big(\dd^{(k)}f(x)\big)={\partial^kf\over\partial x^{j_2}\partial
x^{j_3}\ldots\partial x^{j_{k+1}}}(x);$$ diferenciando a igualdade
$\lambda\circ\dd^{(k)}f={\partial^kf\over\partial
x^{j_2}\ldots\partial x^{j_{k+1}}}$ no ponto $x$ e aplicando o
resultado em $e_{j_1}$ obtemos:
$$\dd\big(\dd^{(k)}f\big)(x)(e_{j_1})(e_{j_2},\ldots,e_{j_{k+1}})={\partial^{k+1}f\over\partial
x^{j_1}\partial x^{j_2}\ldots\partial x^{j_{k+1}}}(x).$$ Usando a
fórmula $(*)$ que relaciona $\dd\big(\dd^{(k)}f\big)(x)$ e
$\dd^{(k+1)}f(x)$ obtemos o resultado desejado.\fimprova

\smallskip

\noindent{\tt Exemplo}. ({\sl o caso $m=1$}) Se $f:U\subset\R\to\R^n$ é uma
curva $k$ vezes diferenciável no instante $t\in U$ então pelo teorema acima
temos:
$$\dd^{(k)}f(t)(\underbrace{1,1,\ldots,1}_{k\ {\rm vezes}})={\dd^kf\over\dd
t^k}(t);$$
além do mais, da multi-linearidade de $\dd^{(k)}f(t)$ obtemos:
$$\dd^{(k)}f(t)(h_1,h_2,\ldots,h_k)={\dd^kf\over\dd
t^k}(t)h_1h_2\ldots h_k,$$
para todos $h_1,h_2,\ldots,h_k\in\R$.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{1.} Seja
$B:\R^{m_1}\times\R^{m_2}\times\cdots\times\R^{m_p}\to\R^n$ uma
aplicação $p$-linear.
\smallskip
\itemitem{(a)} Mostre que $B$ é diferenciável e que sua diferencial é dada
por:
$$\dd
B(x_1,\ldots,x_p)\cdot(h_1,\ldots,h_p)=\sum_{i=1}^pB(x_1,\ldots,x_{i-1},h_i,x_{i+1},\ldots,x_p),$$
para todos $x_i\in\R^{m_i}$, $h_i\in\R^{m_i}$, $i=1,\ldots,p$.
\itemitem{(b)} para $i=1,\ldots,p$ considere a aplicação:
$$B_i:\R^{m_1}\times\cdots\times\R^{m_{i-1}}\times\R^{m_{i+1}}\times\cdots\times\R^{m_p}\longrightarrow\Lin(\R^{m_1}\times\cdots\times\R^{m_p},\R^n)$$
definida por
$B_i(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_p)(h)=B(x_1,\ldots,x_{i-1},h_i,x_{i+1},\ldots,x_p)$,
onde $h=(h_1,\ldots,h_p)$. Mostre que $B_i$ é uma aplicação
$(p-1)$-linear.
\itemitem{(c)} Seja ${\frak
p}_i:\R^{m_1}\times\cdots\times\R^{m_p}\to\R^{m_1}\times\cdots\times\R^{m_{i-1}}\times\R^{m_{i+1}}\times\cdots\times\R^{m_p}$
o operador linear que {\sl esquece a $i$-ésima coordenada}, i.e.:
$${\frak
p}_i(x_1,\ldots,x_p)=(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_p).$$
Mostre que:
$$\dd B=\sum_{i=1}^pB_i\circ{\frak p}_i.$$
\itemitem{(d)} Utilize indução em $p$ e o resultado do item (c)
para concluir que toda aplicação $p$-linear é de classe
$C^\infty$.

\vfil\eject

\centerline{\bf Adendo à Aula número $9$}
\bigskip
\bigskip

Aí vai uma aplicação importante da desigualdade do valor médio que ficou
faltando na aula número $9$.

\smallskip

\proclaim Teorema. Seja $f:U\subset\R^m\to\R^n$ uma função diferenciável com
$\dd f(x)=0$ para todo $x\in U$, onde $U$ é um aberto conexo de $\R^m$. Então
$f$ é constante.

\Prova Se $U$ é convexo então para todos $x,y\in U$ podemos aplicar a
desigualdade do valor médio para $f$ no segmento $[x,y]$ obtendo:
$$\big\Vert f(y)-f(x)\big\Vert\le\sup_{z\in[x,y]}\big\Vert\dd
f(z)\big\Vert\Vert y-x\Vert=0,$$
donde $f(x)=f(y)$ e $f$ é constante em $U$. Vamos agora mostrar o caso geral,
i.e., o caso que $U$ é um aberto conexo qualquer de $\R^m$. Seja $c\in\R^n$ um
elemento qualquer da imagem de $f$ (o caso $U=\emptyset$ é meio ridículo) ---
considere o conjunto:
$$A=\big\{x\in U:f(x)=c\big\}=f^{-1}(c).$$
Temos $A\ne\emptyset$ pois $c\in f(U)$; além do mais, $A$ é fechado {\sl em
$U$\/} pois $f$ é contínua e $\{c\}$ é fechado em $\R^n$. Se mostrarmos que
$A$ é aberto (em $U$ ou em $\R^m$, dá no mesmo), poderemos concluir da
conexidade de $U$ que $A=U$, o que completará a demonstração. Observe então
que se $x\in A$ então temos uma bola $\Bola(x;r)\subset U$ contida em $U$ e
como a bola é
convexa, segue da primeira parte da demonstração que $f$ é constante em
$\Bola(x;r)$ e portanto $\Bola(x;r)\subset A$. Isso completa a
demonstração.\fimprova

\vfil\eject

\centerline{\bf Aula número $11$ (17/04)}
\bigskip
\bigskip

A aula número 11 cobriu o ``adendo à aula número 9'' e boa parte
do material originalmente destinado à aula número 10 sobre
``diferenciação de ordem superior'', faltando apenas a relação
entre as diferenciais de ordem superior e as derivadas parciais de
ordem superior.

\bigskip\bigskip\bigskip\bigskip

\centerline{\bf Aula número $12$ (19/04)}
\bigskip
\bigskip

A aula começa cobrindo o material sobre a relação entre as
diferenciais de ordem superior e as derivadas parciais de ordem
superior, originalmente destinado à aula número 10.

\medskip

\item{(1)} {\bf O teorema de Schwarz}.

\smallskip

\proclaim Teorema. (de Schwarz) Seja $f:U\subset\R^m\to\R^n$ uma função duas
vezes diferenciável num ponto $x\in U$, onde $U\subset\R^m$ é um
aberto. Então:
$${\partial^2f\over\partial x^i\partial x^j}(x)={\partial^2f\over\partial
x^j\partial x^i}(x),$$
para todos $i,j=1,\ldots,m$.

\Prova O teorema pode obviamente ser demonstrado coordenada por
coordenada e portanto podemos supor sem perda de generalidade que
$n=1$. Na verdade, podemos supor também que $m=2$; o caso geral
segue do caso $m=2$ considerando a função:
$$(x'_i,x'_j)\longmapsto f(x_1,\ldots,x'_i,\ldots,x'_j,\ldots,x_m)$$
definida numa vizinhança aberta do ponto $(x_i,x_j)$ em $\R^2$.

Seja então $f:U\to\R$ uma função definida num aberto $U$ em
$\R^2$, duas vezes diferenciável num ponto $(x,y)\in\R^2$ ---
vamos denotar a diferenciação parcial para funções definidas em
abertos de $\R^2$ usando os símbolos ${\partial\over\partial x}$ e
${\partial\over\partial y}$. Devemos mostrar então que:
$${\partial^2f\over\partial x\partial y}(x,y)={\partial^2f\over\partial
y\partial x}(x,y).$$
Considere a função:
$$\varphi(t)=f(x+t,y+t)-f(x+t,y)-f(x,y+t)+f(x,y),$$
definida para $t$ numa vizinhança aberta da origem em $\R$. A estratégia da
demonstração é mostrar que tanto ${\partial^2f\over\partial x\partial y}(x,y)$
como ${\partial^2f\over\partial y\partial x}(x,y)$ são iguais ao limite
$\lim_{t\to0}{\varphi(t)\over t^2}$.

Como $U$ é aberto, podemos escolher $\varepsilon>0$ tal que $\vert
x'-x\vert\le\varepsilon$ e $\vert y'-y\vert\le\varepsilon$ implicam
$(x',y')\in U$. Fixado
$t$ com $\vert t\vert\le\varepsilon$, podemos considerar a função:
$$[y-\varepsilon,y+\varepsilon]\ni y'\longmapsto\xi(y')=f(x+t,y')-f(x,y')\in\R$$ e
observar que $\varphi(t)=\xi(y+t)-\xi(y)$. Aplicando o teorema do valor médio
para $\xi$ concluímos que existe $\theta\in(0,1)$ tal que:
$$\varphi(t)=\xi(y+t)-\xi(y)=t\xi'(y+\theta t)=t\left[{\partial f\over\partial
y}(x+t,y+\theta t)-{\partial f\over\partial y}(x,y+\theta t)\right];$$
como $\dd f$ é diferenciável em $x$, temos que ${\partial f\over\partial y}$ é
diferenciável em $x$ (veja o Exercício 1) e portanto podemos escrever:
$${\partial f\over\partial y}(x+h,y+k)={\partial f\over\partial
y}(x,y)+{\partial^2f\over\partial x\partial y}(x,y)h+{\partial^2f\over\partial
y^2}(x,y)k+\rho(h,k)\big\Vert(h,k)\big\Vert,$$
com $\lim_{(h,k)\to0}\rho(h,k)=0$. Usando a fórmula acima
com $(h,k)=(t,\theta t)$ e com $(h,k)=(0,\theta t)$ obtemos:
$$\eqalign{\varphi(t)&=t\left[{\partial f\over\partial
y}(x,y)+{\partial^2f\over\partial
x\partial y}(x,y)t+{\partial^2f\over\partial y^2}(x,y)\theta t+\rho(t,\theta
t)\big\Vert(t,\theta t)\big\Vert\right]\cr
&-t\left[{\partial f\over\partial y}(x,y)+{\partial^2f\over\partial
y^2}(x,y)\theta t+\rho(0,\theta t)\big\Vert(0,\theta
t)\big\Vert\right]\cr
&=t^2{\partial^2f\over\partial x\partial
y}(x,y)+t\rho(t,\theta t)\big\Vert(t,\theta t)\big\Vert-t\rho(0,\theta
t)\big\Vert(0,\theta t)\big\Vert,\cr}$$
e portanto
$$\lim_{t\to0}{\varphi(t)\over t^2}={\partial^2f\over\partial x\partial
y}(x,y).$$
(observe que $\theta$ não é uma constante quando fazemos $t\to0$, mas sim uma
{\sl função\/} de $t$; no entanto, o valor de $\theta$ fica no intervalo
$(0,1)$ para todo $t$). Observe agora que, fixado
$t\in[-\varepsilon,\varepsilon]$, temos $\varphi(t)=\eta(x+t)-\eta(x)$, onde
$\eta$ é a função definida por:
$$[x-\varepsilon,x+\varepsilon]\ni x'\longmapsto\eta(x')=f(x',y+t)-f(x',y).$$
Repetimos agora o que foi feito acima usando $\eta$ no lugar de
$\xi$ e usando a diferenciabilidade de ${\partial f\over\partial
x}$; concluiremos que:
$$\lim_{t\to0}{\varphi(t)\over t^2}={\partial^2f\over\partial y\partial
x}(x,y),$$
o que completa a demonstração.\fimprova

\smallskip

Uma aplicação bilinear $B:V\times V\to W$ é dita {\sl simétrica\/}
quando $B(v_1,v_2)=B(v_2,v_1)$ para todos $v_1,v_2\in V$. Mais
geralmente, uma aplicação multi-linear $B\in\Multlin_k(V;W)$ é
dita {\sl simétrica\/} quando para todos $v_1,\ldots,v_k\in V$, o
valor de $B(v_1,\ldots,v_k)$ é invariante (i.e., não se altera)
por permutações dos vetores $v_i$, i.e., quando:
$$B(v_1,\ldots,v_k)=B(v_{\sigma(1)},\ldots,v_{\sigma(k)}),$$
para toda bijeção $\sigma:\{1,\ldots,k\}\to\{1,\ldots,k\}$.

\smallskip

\noindent{\tt Observação}. Na verdade, para que
$B\in\Multlin_k(V;W)$ seja simétrica é suficiente que para todos
$v_1,\ldots,v_k\in V$, o valor de $B(v_1,\ldots,v_k)$ seja
invariante por trocas de vetores consecutivos $v_i$, $v_{i+1}$,
i.e., que:
$$B(v_1,\ldots,v_i,v_{i+1},\ldots,v_k)=B(v_1,\ldots,v_{i+1},v_i,\ldots,v_k),$$
para todo $i=1,\ldots,k-1$. Isso segue do fato extremamente
intuitivo de que toda permutação de $k$ elementos pode ser
realizada através de trocas de elementos consecutivos; a
formalização correta desse fato occore quando se estuda em álgebra
a estrutura do {\sl grupo de permutações\/} $S_k$ (para quem já
estudou um pouco de teoria dos grupos, veja os Exercícios~6, 7 e
8).

\smallskip

\proclaim Corolário. Se $f:U\subset\R^m\to\R^n$ é duas vezes diferenciável num
ponto $x$ do aberto $U$ então o Hessiano de $f$ no ponto $x$ é uma aplicação
bilinear simétrica.

\Prova Segue do Teorema de Schwarz e do Exercício 2.

\smallskip

\proclaim Corolário. Se $f:U\subset\R^m\to\R^n$ é $k$ vezes diferenciável num
ponto $x$ do aberto $U$ então a diferencial de $k$-ésima ordem $\dd^{(k)}f(x)$
é uma aplicação $k$-linear simétrica.

\Prova Se $f$ é $k$ vezes diferenciável em $x$ então as derivadas
parciais de $f$ de ordem $l$ com $l\le k-2$ são duas vezes
diferenciáveis em $x$ (veja o Exercício 1) e portanto aplicando o
Teorema de Schwarz indutivamente concluímos que o valor de uma
derivada parcial de ordem $k$ de $f$ no ponto $x$ não depende da
ordem em que as $k$ derivadas parciais são feitas. A conclusão
segue do Exercício 3.\fimprova

\medskip

\item{(2)} {\bf Aplicações multi-lineares em coordenadas}.

\smallskip

O objetivo desta seção é o de fazer uma preparação para o estudo
da fórmula de Taylor para funções de várias variáveis.

\smallskip

Seja $B\in\Multlin_k(\R^m;\R^n)$ uma aplicação $k$-linear. Vamos
recordar um pouco o que foi deixado como exercício na aula número
3 (13/03) --- dissemos lá que uma aplicação multi-linear fica
univocamente determinada por seus valores em vetores de uma base.
Vamos explorar isso mais a fundo: seja $(e_i)_{i=1}^m$ a base
canônica de $\R^m$. A aplicação $B$ deve ser calculada numa
$k$-upla de vetores de $\R^m$; vamos então calcular $B$ em $k$
vetores da base canônica, digamos $e_{i_1}$, \dots, $e_{i_k}$,
onde os índices $i_1,\ldots,i_k$ variam em $\{1,\ldots,m\}$.
Escrevemos então:
$$B(e_{i_1},e_{i_2},\ldots,e_{i_k})=b_{i_1i_2\ldots
i_k}\in\R^n.$$ Esse processo produz $m^k$ vetores de $\R^n$. O
fato importante para se ter em mente é que os vetores
$b_{i_1i_2\ldots i_k}$, $i_1,\ldots,i_k=1,\ldots,m$, {\sl
descrevem a aplicação $k$-linear $B$ completamente}. Vamos fazer
algumas contas. Digamos que os vetores $b_{i_1\ldots i_k}\in\R^n$
sejam dados e digamos que queremos calcular o valor de $B$ numa
$k$-upla arbitrária $(v_1,\ldots,v_k)$ de vetores de $\R^m$, sendo
que sabemos que $B(e_{i_1},\ldots,e_{i_k})=b_{i_1\ldots i_k}$.
Procedemos da seguinte forma: escrevemos cada $v_\alpha$,
$\alpha=1,\ldots,k$, como combinação linear dos vetores da base
canônica, i.e.:
$$v_\alpha=\sum_{i_\alpha=1}^mv_\alpha^{i_\alpha}e_{i_\alpha},\quad\alpha=1,\ldots,k;$$
denotamos por $i_\alpha$ o índice usado na $\alpha$-ésima
somatória --- em princípio, na fórmula acima, não é importante
usar nomes diferentes para os índices das $k$ somatórias, porém
essa distinção será necessária a seguir. Calculamos:
$$B(v_1,\ldots,v_k)=B\Big(\sum\nolimits_{i_1=1}^mv_1^{i_1}e_{i_1},
\sum\nolimits_{i_2=1}^mv_2^{i_2}e_{i_2},\ldots,\sum\nolimits_{i_k=1}^mv_k^{i_k}e_{i_k}\Big),$$
e pela multi-linearidade de $B$ obtemos:
$$\eqalign{B(v_1,\ldots,v_k)&=\sum_{i_1=1}^m\sum_{i_2=1}^m\cdots\sum_{i_k=1}^mv_1^{i_1}v_2^{i_2}\cdots
v_k^{i_k}B(e_{i_1},e_{i_2},\ldots,e_{i_k})\cr
&=\sum_{i_1=1}^m\sum_{i_2=1}^m\cdots\sum_{i_k=1}^mv_1^{i_1}v_2^{i_2}\cdots
v_k^{i_k}b_{i_1i_2\ldots i_k}.}$$

\smallskip

\noindent{\tt Observação}. Para quem acha a quantidade de
somatórias e índices acima uma coisa incompreensível, tente
escrever sozinho(a) os casos $k=1$ (que é bem fácil) e $k=2$, ao
menos --- note que, quando $k$ é pequeno, não surge a necessidade
de usar índices $i_\alpha$, $\alpha=1,\ldots,k$. Por exemplo, se
$k=2$ você pode usar $i$ e $j$ em vez de $i_1$ e $i_2$.

\smallskip

O que foi mostrado acima pode ser resumido na seguinte afirmação:
{\sl a aplicação}
$$\Multlin_k(\R^m;\R^n)\ni
B\longmapsto\big(B(e_{i_1},\ldots,e_{i_k})\big)_{i_1,\ldots,i_k=1}^m\in(\R^n)^{(m^k)}$$
{\sl é um isomorfismo}.

\smallskip

\noindent{\tt Observação}. A {\sl expansão em coordenadas\/} de
$B$ feita acima pode ser levada ainda um nível adiante: podemos
descrever os vetores $b_{i_1\ldots i_k}\in\R^n$ em termos de suas
coordenadas na base canônica de $\R^n$, obtendo números reais
$b_{i_1\ldots i_k}^j\in\R$, $i_1,\ldots,i_k=1,\ldots,m$,
$j=1,\ldots,n$. Podemos então pensar da seguinte maneira: {\sl uma
aplicação $k$-linear $B\in\Multlin_k(\R^m;\R^n)$ pode ser descrita
por uma {\rm hiper-matriz} real
$n\times\underbrace{m\times\cdots\times m}_{k\ {\rm vezes}}$ com
$k+1$ índices, i.e., por uma coleção de $nm^k$ números reais}. Em
particular, quando $k=1$ obtemos novamente o fato familiar que
aplicações lineares $T:\R^m\to\R^n$ podem ser descritas por
matrizes reais $n\times m$; quando $k=2$ e $n=1$ obtemos o fato
(familiar?) que formas bilineares $B:\R^m\times\R^m\to\R$ podem
ser descritas por matrizes reais $m\times m$.

\smallskip

\noindent{\tt Exemplo}. Se $f:U\subset\R^m\to\R^n$ é $k$ vezes
diferenciável no ponto $x\in U$ então a aplicação $k$-linear
$\dd^{(k)}f(x)\in\Multlin_k(\R^m;\R^n)$ é descrita pelos vetores:
$$\dd^{(k)}f(x)(e_{i_1},\ldots,e_{i_k})={\partial^kf\over\partial
x^{i_1}\ldots\partial x^{i_k}}(x)\in\R^n.$$

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Diferenciação}.

\medskip

\item{1.} Seja $f:U\subset\R^m\to\R^n$ uma função definida num
aberto $U\subset\R^m$. Mostre que se $f$ é $k$ vezes diferenciável
num ponto $x\in U$ ($k\ge1$) então as derivadas parciais
${\partial f\over\partial x^i}$ de $f$ são $k-1$ vezes
diferenciáveis em $x$. Mostre também que se $f$ é de classe $C^k$
($k\ge1$) então as derivadas parciais ${\partial f\over\partial
x^i}$ são de classe $C^{k-1}$ ({\sl dica}: ${\partial
f\over\partial x_i}=\lambda\circ\dd f$, onde $\lambda:T\mapsto
T(e_i)$ é o operador de avaliação no $i$-ésimo vetor da base
canônica de $\R^m$).

\medskip

\noindent{\tt Álgebra linear}.

\smallskip

\item{2.} Seja $B:V\times V\to W$ uma aplicação bilinear e seja
$(e_i)_{i=1}^m$ uma base de $V$. Mostre que se
$B(e_i,e_j)=B(e_j,e_i)$ para todos $i,j=1,\ldots,m$ então $B$ é
simétrica ({\sl dica}: defina uma aplicação bilinear $\widetilde
B:V\times V\to W$ fazendo $\widetilde B(x,y)=B(y,x)$ para todos
$x,y\in V$. Recorde dos Exercícios da Aula número 3 (13/03) que
duas aplicações bilineares que coincidem sobre vetores de uma base
são iguais).

\smallskip

\item{3.} Generalize o Exercício 2 para o caso de aplicações
multi-lineares, i.e., dada uma aplicação $k$-linear
$B\in\Multlin_k(V;W)$ e uma base $(e_i)_{i=1}^m$ de $V$, se a
quantidade $B(e_{i_1},e_{i_2},\ldots,e_{i_k})$ é invariante por
permutações dos índices $i_1$, \dots, $i_k$, para todos
$i_1,\ldots,i_k=1,\ldots,m$ então $B$ é simétrica ({\sl dica}: se
$\sigma:\{1,\ldots,k\}\to\{1,\ldots,k\}$ é uma bijeção, defina uma
aplicação $k$-linear $B_\sigma\in\Multlin_k(V;W)$ fazendo:
$$B_\sigma(v_1,\ldots,v_k)=B(v_{\sigma(1)},\ldots,v_{\sigma(k)}),\quad
\forall\ v_1,\ldots,v_k\in V;$$ observe que $B_\sigma$ e $B$
coincidem sobre vetores da base e conclua que $B_\sigma=B$).

\medskip

\noindent{\tt Mais espaços métricos}.

\smallskip

\item{4.} Sejam $M$, $N$ espaços métricos. Uma aplicação $f:M\to
N$ é dita {\sl fechada\/} quando $f$ leva fechados em fechados,
i.e., quando dado um fechado $G$ em $M$ então $f(G)$ é fechado em
$N$. Mostre que:
\smallskip
\itemitem{(a)} se $M$ é compacto então toda aplicação contínua
$f:M\to N$ é fechada;
\itemitem{(b)} se $f:M\to N$ é uma bijeção contínua então $f$ é
fechada se e somente se $f$ é um homeomorfismo;
\itemitem{(c)} se $f:M\to N$ é uma bijeção contínua e $M$ é
compacto então $f$ é um homeomorfismo.

\smallskip

\item{5.} ({\sl o teorema de alfândega}) Sejam $M$ um espaço
métrico, $C\subset M$ um subconjunto conexo e $S\subset M$ um
subconjunto qualquer. Mostre que se $C$ possui pontos de $S$ e
pontos do complementar de $S$ então $C$ possui pontos da fronteira
de $S$, i.e.:
$$C\cap S\ne\emptyset,\ C\not\subset S\Longrightarrow
C\cap\partial S\ne\emptyset.$$ ({\sl dica}: se $C\cap\partial
S=\emptyset$ então
$C=\big(C\cap\Int(S)\big)\cup\big(C\cap\Ext(S)\big)$ é uma cisão
de $C$).

\goodbreak
\medskip

\noindent{\tt Álgebra (só para quem conhece um pouco de teoria dos
grupos!)}.

\smallskip

\item{6.} Seja $S_k$ o grupo formado por todas as bijeções
$\sigma:\{1,\ldots,k\}\to\{1,\ldots,k\}$ munido da operação de
composição; o grupo $S_k$ é conhecido como o {\sl grupo das
permutações de $k$ elementos}. Dados $i,j\in\{1,\ldots,k\}$ com
$i\ne j$ então a permutação $\sigma=(i\;j)\in S_k$ tal que
$\sigma(i)=j$, $\sigma(j)=i$ e $\sigma(x)=x$ para
$x\not\in\{i,j\}$, é chamada a {\sl transposição\/} dos elementos
$i$ e $j$. Mostre que as transposições $(i\;{i+1})$ com
$i=1,\ldots,k-1$ geram o grupo $S_k$ ({\sl dica}: use indução em
$k$).

\smallskip

\item{7.} Seja $G$ um grupo e suponha que seja dada uma ação de
$G$ num conjunto $X$. Dada uma função $f:X\to Y$, um ponto $x\in
X$ e um conjunto de geradores $S\subset G$, suponha que $f(g\cdot
x)=f(x)$ para todo $g\in S$ e todo $x\in X$. Mostre que $f(g\cdot
x)=f(x)$ para todo $g\in G$ e todo $x\in X$ ({\sl dica}: o
conjunto $H=\big\{g\in G:f(g\cdot x)=f(x),\ \forall x\in X\big\}$
é um subgrupo de $G$).

\smallskip

\item{8.} Seja $B\in\Multlin_k(V;W)$ uma aplicação multi-linear.
Seja $X$ o conjunto das funções $\iota:\{1,\ldots,k\}\to V$, i.e.,
o conjunto $V^k$ das $k$-uplas $(v_1,\ldots,v_k)$ de vetores de
$V$. Para $\sigma\in S_k$, defina
$\sigma\cdot\iota=\iota\circ\sigma^{-1}$, i.e.,
$\sigma\cdot(v_1,\ldots,v_k)=(v_{\sigma^{-1}(1)},\ldots,v_{\sigma^{-1}(k)})$.
Mostre que $(\sigma,\iota)\mapsto\sigma\cdot\iota$ define uma ação
do grupo $S_k$ no conjunto $V^k$. Conclua dos Exercícios~6 e 7 que
$B$ é simétrica se e somente se para todos $v_1,\ldots,v_k\in V$ o
valor de $B(v_1,\ldots,v_k)$ é invariante por trocas de vetores
consecutivos $v_i$, $v_{i+1}$, $i=1,\ldots,k-1$ ({\sl observação}:
multi-linearidade não tem nada a ver com este exercício).

\vfil\eject

\centerline{\bf Aula número $13$ (24/04)}
\bigskip
\bigskip

\item{(1)} {\bf Polinômios de várias variáveis}.

\smallskip

Um {\sl monômio\/} em $\R^m$ é uma função da forma:
$$\R^m\ni x=(x_1,\ldots,x_m)\longmapsto
x_1^{\lambda_1}x_2^{\lambda_2}\cdots x_m^{\lambda_m}\in\R,$$ onde
$\lambda_1,\lambda_2,\ldots,\lambda_m$ são inteiros não negativos
--- por simplicidade de terminologia, é conveniente convencionar
que $0^0=1$. O inteiro $\lambda_1+\cdots+\lambda_m$ é chamado o
{\sl grau\/} do monômio acima. Uma {\sl função polinomial\/} em
$\R^m$ tomando valores em $\R^n$ é uma ``combinação linear'' de
monômios em $\R^m$ com coeficientes em $\R^n$; de outro modo, uma
função polinomial em $\R^m$ tomando valores em $\R^n$ é uma função
da forma:
$$\R^m\ni x\longmapsto
p(x)=\sum_{\lambda_1,\ldots,\lambda_m=0}^{+\infty}a_{\lambda_1\ldots\lambda_m}x_1^{\lambda_1}\cdots
x_m^{\lambda_m}\in\R^n,$$ onde os coeficientes
$a_{\lambda_1\ldots\lambda_m}$ pertencem a $\R^n$ e {\sl no máximo
um número finito deles são não nulos}, i.e., a soma acima é {\sl
finita}. O {\sl grau\/} de uma função polinomial $p$ é o maior
grau de um monômio aparecendo em $p$ com coeficiente não nulo,
i.e., o maior dos números $\lambda_1+\cdots+\lambda_m$ tal que
$a_{\lambda_1\ldots\lambda_m}\ne0$.

\smallskip

\noindent{\tt Observação}. Uma função polinomial $p:\R^m\to\R^n$
de grau $1$ é o mesmo que uma função afim não constante, i.e., $p$
é da forma $p(x)=T(x)+c$ onde $T:\R^m\to\R^n$ é um operador linear
não nulo e $c\in\R^n$ é um vetor. Uma função polinomial de grau
zero é o mesmo que uma função constante não nula. Quanto à função
nula, pode-se convencionar que a mesma {\sl não possui grau
definido}, ou {\sl possui grau $-1$\/} ou que {\sl possui grau
$-\infty$}, tanto faz.

\smallskip

Para trabalhar com polinômios de várias variáveis de maneira
satisfatória é conveniente utilizar a {\sl notação de
multi-índice}. Isso significa o seguinte: denotamos uma $m$-upla
$(\lambda_1,\ldots,\lambda_m)$ de inteiros não negativos pelo
símbolo $\lambda$, o coeficiente $a_{\lambda_1\ldots\lambda_m}$ é
denotado por $a_\lambda$, o monômio $x_1^{\lambda_1}\cdots
x_m^{\lambda_m}$ é denotado por $x^\lambda$ e o grau
$\lambda_1+\cdots+\lambda_m$ desse monômio é denotado por
$\vert\lambda\vert$. Uma função polinomial $p:\R^m\to\R^n$ de grau
menor ou igual a $k$ será denotada então por:
$$p(x)=\sum_{\vert\lambda\vert\le k}a_\lambda x^\lambda,\quad x\in\R^m,$$
onde cada $a_\lambda\in\R^n$. Uma $m$-upla
$\lambda=(\lambda_1,\ldots,\lambda_m)$ de inteiros não negativos é
normalmente chamada um {\sl multi-índice de dimensão $m$\/} e o
número $\vert\lambda\vert$ é chamado o {\sl grau\/} do
multi-índice $\lambda$.

\smallskip

\proclaim Definição. Uma {\rm função polinomial homogênea de grau
$k$} é uma função polinomial que pode ser escrita usando apenas
monômios de grau $k$, i.e., uma função polinomial $p$ da forma:
$$\R^m\ni x\longmapsto p(x)=\sum_{\vert\lambda\vert=k}a_\lambda
x^\lambda\in\R^n,$$ com cada $a_\lambda\in\R^n$.

\smallskip

\noindent{\tt Observação}. O leitor com senso crítico pode se
sentir incomodado pelas definições acima, pelo seguinte motivo: em
princípio é possível que uma função polinomial admita {\sl mais de
uma representação como combinação linear de monômios}, i.e., seria
possível que:
$$\sum_{\vert\lambda\vert\le k}a_\lambda x^\lambda=\sum_{\vert\lambda\vert\le k}a'_\lambda
x^\lambda,$$ para todo $x\in\R^m$, porém com $a_\lambda\ne
a'_\lambda$ para algum $\lambda$. Isso tornaria muitas das
definições acima ambíguas, pois o grau de uma função polinomial
poderia em princípio depender da representação utilizada --- além
do mais, uma função polinomial homogênea poderia admitir também
representações não homogêneas. Na verdade, {\sl não há problema nenhum}! A
representação de uma
função polinomial como combinação de monômios {\sl é de fato
única}. Para os que tem uma boa base de álgebra, esse fato deve
ser conhecido na seguinte forma:
\smallskip
\noindent``{\sl Se $D$ é um domínio de integridade infinito (por
exemplo, $D=\R$) então o anel de polinômios $D[X_1,\ldots,X_m]$ é
canonicamente isomorfo ao anel de funções polinomiais de $m$ variáveis em $D$.}''
\smallskip
Para os que não tem familiaridade com o fato algébrico acima,
podemos usar um outro argumento: se $p:\R^m\to\R^n$ é dada por:
$$p(x)=\sum_{\vert\lambda\vert\le k}a_\lambda x^\lambda\in\R^n,\quad
x\in\R^m,$$ então:
$$a_\lambda={1\over\lambda!}{\partial^{\vert\lambda\vert}p\over\partial
x^\lambda}(0),$$ onde
$\lambda!=\lambda_1!\lambda_2!\cdots\lambda_m!$ e
${\partial^{\vert\lambda\vert}p\over\partial
x^\lambda}={\partial^{\vert\lambda\vert}p\over\partial
x_1^{\lambda_1}\partial x_2^{\lambda_2}\cdots\partial
x_m^{\lambda_m}}$. Isso mostra que os coeficientes $a_\lambda$ são
univocamente determinados pela função polinomial $p$.

\smallskip

\noindent{\tt Observação}. Uma maneira agradável de lembrar os
fatos acima usando a linguagem da álgebra linear é o seguinte:
considere o espaço vetorial ${\cal F}(\R^m,\R^n)$ de {\sl todas\/}
as funções $f:\R^m\to\R^n$ --- as operações de espaço vetorial em
${\cal F}(\R^m,\R^n)$ são definidas da maneira óbvia. As funções
polinomiais homogêneas de grau $k$ formam um subespaço de ${\cal
F}(\R^m;\R^n)$ que será denotado por $\Pol_k(\R^m,\R^n)$. A soma
dos subespaços $\Pol_k(\R^m,\R^n)$ de ${\cal F}(\R^m,\R^n)$ é {\sl
direta\/}, i.e., se para $i=0,\ldots,k$, $p_i:\R^m\to\R^n$ é uma
função polinomial homogênea de grau $i$ então
$p_0+p_1+\cdots+p_k=0$ implica $p_i=0$ para todo $i=0,\ldots,k$;
denotando por $\Pol(\R^m,\R^n)$ o subespaço de ${\cal
F}(\R^m,\R^n)$ formado por todas as funções polinomiais obtemos
então:
$$\Pol(\R^m,\R^n)=\bigoplus_{k=0}^{+\infty}\Pol_k(\R^m,\R^n).$$
A soma direta:
$$\Pol_{\le k}(\R^m,\R^n)=\bigoplus_{i=0}^k\Pol_i(\R^m,\R^n)$$
é igual ao subespaço $\Pol_{\le k}(\R^m,\R^n)$ de
$\Pol(\R^m,\R^n)$ formado pelas funções polinomiais de grau menor
ou igual a $k$. Enquanto $\Pol(\R^m,\R^n)$ possui dimensão
infinita (para $m,n\ne0$), os espaços $\Pol_k(\R^m,\R^n)$ e
$\Pol_{\le k}(\R^m,\R^n)$ possuem dimensão finita (qual seria uma
base para tais espaços? veja o Exercício~1!).

\medskip

\item{(2)} {\bf Relação entre aplicações multi-lineares e polinômios}.

\smallskip

Se $B\in\Multlin_k(\R^m;\R^n)$ é uma aplicação multi-linear então
escrevemos: $$B(x)^{(k)}=B(\underbrace{x,x,\ldots,x}_{k\ {\rm
vezes}}),\quad x\in\R^m.$$ Considere a aplicação
$\phi:\R^m\to\R^n$ definida por:
$$\phi(x)=B(x)^{(k)},\quad x\in\R^m.$$
Escrevendo $x=\sum_{i=1}^mx_ie_i$ obtemos:
$$\phi(x)=\sum_{i_1,\ldots,i_k=1}^mx_{i_1}x_{i_2}\cdots x_{i_k}b_{i_1i_2\ldots
i_k},$$ onde $b_{i_1\ldots
i_k}=B(e_{i_1},\ldots,e_{i_k})\in\R^n$. Vocês agora podem se
convencer que a expressão acima define uma {\sl função polinomial
homogênea de grau $k$}. De fato, dentre os índices
$i_1,\ldots,i_k\in\{1,\ldots,m\}$, temos uma certa quantidade
deles, digamos $\lambda_1\ge0$, que são iguais a $1$; alguns
outros, digamos $\lambda_2\ge0$, são iguais a $2$. Em geral,
podemos denotar por $\lambda_j$ o número de índices
$i_1,\ldots,i_k$ que é igual a $j$ (com $j=1,\ldots,m$). Obtemos
então um multi-índice $\lambda=(\lambda_1,\ldots,\lambda_m)$ com
$\vert\lambda\vert=k$; vamos denotá-lo por:
$$\lambda=\#i_1\ldots i_k.$$
Observe que:
$$x_{i_1}x_{i_2}\cdots x_{i_k}=x_1^{\lambda_1}x_2^{\lambda_2}\cdots
x_m^{\lambda_m}\vtop{\baselineskip=0pt\halign{\hfil#\hfil\cr$=$\cr{\pequeno\
em notação de\ }\cr{\pequeno\ multi-índice\ }\cr}}x^\lambda.$$
Concluímos então que:
$$\phi(x)=\sum_{\vert\lambda\vert=k}a_\lambda x^\lambda,$$
onde $a_\lambda$ é dado por:
$$a_\lambda=\sum_{\scriptstyle
i_1,\ldots,i_k=1\atop\scriptstyle\#i_1\ldots
i_k=\lambda}^mb_{i_1\ldots i_k}\in\R^n.$$
Um exercício simples de combinatória mostra que a somatória acima tem
exatamente ${k!\over\lambda!}$ termos (verifique!).

Vamos mostrar agora que toda função polinomial homogênea de grau $k$ é da forma
$x\mapsto B(x)^{(k)}$ para alguma aplicação $k$-linear
$B\in\Multlin_k(\R^m;\R^n)$; na verdade, tal aplicação $B$ pode
ser escolhida de muitas formas. Por exemplo, dada uma função
polinomial homogênea $p(x)=\sum_{\vert\lambda\vert=k}a_\lambda
x^\lambda$ de grau $k$ então podemos definir uma aplicação
multi-linear $B\in\Multlin_k(\R^m;\R^n)$ fazendo:
$$B(e_{i_1},\ldots,e_{i_k})=b_{i_1\ldots
i_k}={a_\lambda\over k!}\lambda!,$$ para todos
$i_1,\ldots,i_k=1,\ldots,m$ com $\#i_1\ldots i_k=\lambda$. Obtemos
então uma aplicação $k$-linear {\sl simétrica\/}
$B\in\Multlin_k(\R^m;\R^n)$ com $p(x)=B(x)^{(k)}$ para todo
$x\in\R^m$.

\smallskip

\noindent{\tt Exemplo}. Suponha que $B:\R^m\times\R^m\to\R^n$ é
uma aplicação bilinear e escreva $B(e_i,e_j)=b_{ij}$, para
$i,j=1,\ldots,m$. Temos:
$$B(x,x)=\sum_{i,j=1}^mb_{ij}x_ix_j,$$
para todo $x\in\R^m$. Podemos escrever também:
$$B(x,x)=\sum_{i=1}^mb_{ii}x_i^2+\sum_{{\scriptstyle
i,j=1\atop\scriptstyle i<j}}^m(b_{ij}+b_{ji})x_ix_j.$$ Observe que
se $\widetilde B:\R^m\times\R^m\to\R^n$ é uma outra aplicação
bilinear com: $$\widetilde B(e_i,e_j)+\widetilde
B(e_j,e_i)=B(e_i,e_j)+B(e_j,e_i),$$ para todos $i,j=1,\ldots,m$
então $B$ e $\widetilde B$ {\sl definem a mesma função
polinomial}, i.e., $B(x,x)=\widetilde B(x,x)$ para todo
$x\in\R^m$. Em geral então é possível que {\sl uma infinidade de
aplicações multi-lineares definam a mesma função polinomial
homogênea}! Veremos adiante, no entanto, que têm-se a unicidade da
aplicação multi-linear associada a uma função polinomial homogênea
quando restringe-se às {\sl aplicações multi-lineares simétricas}!

\smallskip

\noindent{\tt Observação}. Uma aplicação polinomial homogênea de grau $2$ é
também chamada uma {\sl forma quadrática}. Recorde que se $\phi$ é uma forma
quadrática em $\R^2$ então o conjunto solução de $\phi(x)=1$ é
chamado uma {\sl cônica\/} e que se $\phi$ é uma forma quadrática
em $\R^3$ então o conjunto solução de $\phi(x)=1$ é chamado uma
{\sl quádrica}.

\smallskip

\proclaim Teorema. Seja $B\in\Multlin_k(\R^m;\R^n)$ uma aplicação
$k$-linear {\rm simétrica}. A aplicação $\phi(x)=B(x)^{(k)}$ é de
classe $C^\infty$ e sua $r$-ésima diferencial é dada por:
$$\dd^{(r)}\phi(x)\cdot
(h_1,\ldots,h_r)=k(k-1)\cdots(k-r+1)B(\underbrace{x,\ldots,x}_{k-r\
{\rm vezes}},h_1,\ldots,h_r),$$ para $r\le k$ e
$\dd^{(r)}\phi(x)=0$ para $r>k$. Em particular:
$$\dd^{(k)}\phi(x)=k!B,$$
para todo $x\in\R^m$.

\Prova Usamos indução em $r$. O caso $r=1$ segue observando que
$\phi=B\circ\Delta$, onde $\Delta:\R^m\to(\R^m)^k$ é a {\sl
aplicação diagonal\/} dada por
$\Delta(x)=(\underbrace{x,\ldots,x}_{k\ {\rm vezes}})$. Como
$\Delta$ é linear e $B$ é multi-linear, a regra da cadeia nos dá:
$$\dd\phi(x)\cdot
h=B(h,x,\ldots,x)+B(x,h,x,\ldots,x)+\cdots+B(x,x,\ldots,x,h)=kB(x,\ldots,x,h),$$
pois $B$ é simétrica. Suponha agora o resultado verdadeiro para um certo
$r\ge1$. Considere a aplicação $(k-r)$-linear simétrica: $$\widetilde
B:\underbrace{\R^m\times\cdots\times\R^m}_{k-r\ {\rm
vezes}}\longrightarrow\Multlin_r(\R^m;\R^n)$$ definida por:
$$\widetilde
B(x_1,\ldots,x_{k-r})\cdot(h_1,\ldots,h_r)=B(x_1,\ldots,x_{k-r},h_1,\ldots,h_r).$$
A hipótese de indução nos diz que:
$$\dd^{(r)}\phi(x)=k(k-1)\cdots(k-r+1)\widetilde B(x)^{(k-r)};$$
diferenciando a igualdade acima dos dois lados obtemos:
$$\eqalign{\dd^{(r+1)}\phi(x)\cdot(h_1,\ldots,h_{r+1})&=\dd\big(\dd^{(r)}\phi\big)(x)\cdot(h_1)(h_2,\ldots,h_{r+1})
\cr &=k(k-1)\cdots(k-r+1)(k-r)\widetilde
B(x,\ldots,x,h_1)\cdot(h_2,\ldots,h_{r+1})\cr
&=k(k-1)\cdots(k-r+1)(k-r)B(x,\ldots,x,h_1,\ldots,h_{r+1}),\cr}$$
o que completa a demonstração.\fimprova

\smallskip

\proclaim Corolário. Se $\Multlinsym_k(\R^m;\R^n)$ denota o espaço
das aplicações $k$-lineares simétricas
$B:\R^m\times\cdots\times\R^m\to\R^n$ então temos um isomorfismo:
$$\Multlinsym_k(\R^m;\R^n)\ni
B\longmapsto\phi_B\in\Pol_k(\R^m;\R^n)$$ onde
$\phi_B(x)=B(x)^{(k)}$ para todo $x\in\R^m$.

\Prova A sobrejetividade já foi estabelecidada quando observamos que toda
função polinomial homogênea de grau $k$ é da forma $x\mapsto B(x)^{(k)}$ para
alguma aplicação $k$-linear simétrica $B\in\Multlin_k(\R^m;\R^n)$. A linearidade de
$B\mapsto\phi_B$ é
muito simples. Resta mostrar a injetividade. Suponha então que
$B\in\Multlinsym_k(\R^m;\R^n)$ é tal que $\phi_B=0$. Obtemos
então:
$$\dd^{(k)}\phi_B(x)=k!B=0,$$
e portanto $B=0$.\fimprova

\medskip

\item{(3)} {\bf O polinômio de Taylor}.

\smallskip

Segue do que foi visto na seção anterior que toda aplicação
polinomial $p:\R^m\to\R^n$ de grau menor ou igual a $k$ pode ser
escrita na forma:
$$p(x)=\sum_{i=0}^kB_i(x)^{(i)},\quad x\in\R^m,$$
onde cada $B_i\in\Multlin_i(\R^m;\R^n)$ é uma aplicação
multi-linear simétrica (com exceção de $B_0$, que denota simplesmente um
vetor fixado em $\R^n$). Além do mais, as aplicações $B_i$ são
{\sl univocamente determinadas\/} por $p$.

\smallskip

\proclaim Teorema. Seja $f:U\subset\R^m\to\R^n$ uma função
definida num aberto $U\subset\R^m$, $k$ vezes diferenciável num
ponto $x\in U$. Então:
$$p(h)=\sum_{i=0}^k{1\over i!}\,\dd^{(i)}f(x)\cdot(h)^{(i)},\quad
h\in\R^m,$$ é a única função polinomial de grau menor ou igual a
$k$ tal que $p(0)=f(x)$ e $\dd^{(i)}p(0)=\dd^{(i)}f(x)$ para
$i=1,\ldots,k$.

\Prova Se $p(h)=\sum_{i=0}^kB_i(h)^{(i)}$ é uma aplicação
polinomial de grau menor ou igual a $k$, onde cada $B_i$ é uma
aplicação $i$-linear simétrica $B_i\in\Multlin_i(\R^m;\R^n)$ então
é fácil ver que $p(0)=B_0$ e que $\dd^{(i)}p(0)=i!B_i$ para
$i=1,\ldots,k$. A conclusão segue.\fimprova

\smallskip

Seja $f:U\subset\R^m\to\R^n$ uma função $k$ vezes diferenciável
num ponto $x\in U$, onde $U\subset\R^m$ é um aberto. A função
polinomial $p(h)=\sum_{i=0}^k{1\over
i!}\,\dd^{(i)}f(x)\cdot(h)^{(i)}$ é conhecida como o {\sl
$k$-ésimo polinômio de Taylor\/} de $f$ em torno do ponto $x$.
Podemos escrever:
$$f(x+h)=f(x)+\dd f(x)\cdot
h+{1\over2!}\,\dd^{(2)}f(x)\cdot(h)^{(2)}+\cdots+{1\over
k!}\,\dd^{(k)}f(x)\cdot(h)^{(k)}+r(h),$$ onde $r$ é uma função $k$
vezes diferenciável no ponto $0$ tal que $r(0)=0$ e
$\dd^{(i)}r(0)=0$ para $i=1,\ldots,k$. Dizemos que $r$ é o {\sl
resto do $k$-ésimo polinômio de Taylor de $f$ em torno de $x$}.

\smallskip

\noindent{\tt Observação}. Se $r$ é o resto do $k$-ésimo polinômio
de Taylor de $f$ em torno do ponto $x$ e se $f$ é $l$ vezes
diferenciável em $x$ para um certo $l\ge k$ então $r$ também é $l$
vezes diferenciável no ponto $x$ e $\dd^{(i)}r(x)=\dd^{(i)}f(x)$
para todo $i=k+1,\ldots,l$.

\smallskip

O fato que $r$ se anula no ponto $0$ juntamente com suas $k$
primeiras diferenciais implica em certo sentido que o resto $r$ é
``bem pequeno'' em torno de $0$. Essa idéia intuitiva de ``bem
pequeno'' pode ser expressa precisamente em termos de três
resultados que apresentamos agora.

\smallskip

\proclaim Teorema. (Taylor infinitesimal) Se $f$ é $k$ vezes
diferenciável no ponto $x$ ($k\ge1$) e se $r$ é o resto do seu
$k$-ésimo polinômio de Taylor em torno de $x$ então:
$$\lim_{h\to0}{r(h)\over\,\Vert h\Vert^k}=0.$$

\Prova Vamos mostrar por indução em $k$ que se $r$ é uma função
que se anula juntamente com suas $k$ primeiras diferenciais no
ponto $0$ então $\lim_{h\to0}{r(h)\over\,\Vert h\Vert^k}=0$. O
caso $k=1$ segue trivialmente da definição de função diferenciável
(quando se escreve a definição de função diferenciável para a
função $r$ no ponto $0$ vê-se que $r$ é o próprio resto). Supondo
o resultado válido para um certo $k$, suponha que $r$ se anula
juntamente com suas $k+1$ primeiras diferenciais no ponto $0$.
Pela desigualdade do valor médio temos:
$$\Vert r(h)\Vert\le\sup_{\theta\in(0,1)}\big\Vert\dd r(\theta
h)\big\Vert\,\Vert h\Vert,$$ para $h$ suficientemente pequeno.
Como $\dd r$ se anula juntamente com suas $k$ primeiras
diferenciais em $0$, segue da hipótese de indução que:
$$\lim_{h\to0}{\dd r(h)\over\,\Vert h\Vert^k}=0;$$
assim, dado $\varepsilon>0$, podemos encontrar $\delta>0$ tal que
$\Vert h\Vert<\delta$, implica $\big\Vert\dd
r(h)\big\Vert\le\varepsilon\Vert h\Vert^k$. Daí $\Vert
h\Vert<\delta$ implica:
$$\big\Vert r(h)\big\Vert\le\sup_{\theta\in(0,1)}\big\Vert\dd
r(\theta h)\big\Vert\,\Vert
h\Vert\le\sup_{\theta\in(0,1)}\varepsilon\Vert\theta
h\Vert^k\,\Vert h\Vert\le\varepsilon\Vert h\Vert^{k+1}.$$
Concluímos então que:
$$\lim_{h\to0}{r(h)\over\,\,\Vert h\Vert^{k+1}}=0,$$
o que completa a demonstração.\fimprova

\smallskip

Antes de provar nosso próximo teorema, precisamos do seguinte:
\proclaim Lema. (desigualdade do valor médio generalizada) Sejam
$\phi:[a,b]\to\R^m$, $\psi:[a,b]\to\R$ funções contínuas,
diferenciáveis no intervalo aberto $(a,b)$, tais que
$\big\Vert\phi'(t)\big\Vert\le\psi'(t)$ para todo $t\in(a,b)$,
onde usamos uma {\rm norma arbitrária} $\Vert\cdot\Vert$ em
$\R^m$. Então:
$$\big\Vert\phi(b)-\phi(a)\big\Vert\le\psi(b)-\psi(a).$$

\Prova Seja $\lambda:\R^m\to\R$ um funcional linear unitário
(i.e., $\Vert\lambda\Vert=1$) que reproduz a norma do vetor
$\phi(b)-\phi(a)$, i.e., tal que
$\lambda\big(\phi(b)-\phi(a)\big)=\big\Vert\phi(b)-\phi(a)\big\Vert$
(recorde Aula número 9 --- 03/04). Aplicando o teorema do valor
médio para a função $\psi-\lambda\circ\phi:[a,b]\to\R$ obtemos que
existe $c\in(a,b)$ com:
$$(b-a)\big[\psi'(c)-\lambda\big(\phi'(c)\big)\big]=\psi(b)-\psi(a)-\lambda\big(\phi(b)-\phi(a)\big)=\psi(b)-\psi(a)-\big\Vert\phi(b)-\phi(a)\big\Vert;$$
observe agora que:
$$\lambda\big(\phi'(c)\big)\le\big\vert\lambda\big(\phi'(c)\big)\big\vert\le\big\Vert\phi'(c)\big\Vert\le\psi'(c),$$
donde $\psi'(c)-\lambda\big(\phi'(c)\big)\ge0$. A conclusão
segue.\fimprova

\smallskip

\proclaim Teorema. (Taylor com resto de Lagrange) Suponha que o
segmento de reta $[x,x+h]$ esteja contido no domínio $U$ de $f$,
que $f$ seja de classe $C^k$ em $U$ ($k\ge0$) e $k+1$ vezes
diferenciável nos pontos do segmento aberto $(x,x+h)$. Se $r$
denota o resto do $k$-ésimo polinômio de Taylor de $f$ em torno de
$x$ então:
$$\Vert
r(h)\Vert\le{1\over(k+1)!}\sup_{z\in(x,x+h)}\big\Vert\dd^{(k+1)}f(z)\big\Vert\,\Vert
h\Vert^{k+1},$$ onde $\big\Vert\dd^{(k+1)}f(z)\big\Vert$ denota a
norma do operador multi-linear $\dd^{(k+1)}f(z)$ (veja o Exercício
29 da aula número 7 --- 27/03).

\Prova Considere a curva $\phi:[0,1]\to U\subset\R^m$ definida
por:
$$\eqalign{\phi(t)=f(x+th)&+(1-t)\,\dd f(x+th)\cdot
h+{(1-t)^2\over2!}\,\dd^{(2)}f(x+th)\cdot(h)^{(2)}+\cdots\cr
&\cdots+{(1-t)^k\over k!}\,\dd^{(k)}f(x+th)\cdot(h)^{(k)},\cr}$$
para todo $t\in[0,1]$. Temos que $\phi$ é contínua em $[0,1]$,
derivável em $(0,1)$ e usando indução em $k$ é fácil mostrar que
(veja o Exercício~4):
$$\phi'(t)={(1-t)^k\over k!}\,\dd^{(k+1)}f(x+th)\cdot(h)^{(k+1)},$$
para todo $t\in(0,1)$. Considere também a função $\psi:[0,1]\to\R$
definida por:
$$\psi(t)=-{\,\,(1-t)^{k+1}\over(k+1)!}\sup_{z\in(x,x+h)}\big\Vert\dd^{(k+1)}f(z)\big\Vert\,\Vert
h\Vert^{k+1};$$ temos:
$$\big\Vert\phi'(t)\big\Vert\le{(1-t)^k\over k!}\sup_{z\in(x,x+h)}\big\Vert\dd^{(k+1)}f(z)\big\Vert\,\Vert
h\Vert^{k+1}=\psi'(t),\quad t\in(0,1).$$ Como
$\phi(1)-\phi(0)=r(h)$ e
$\psi(1)-\psi(0)={1\over(k+1)!}\sup_{z\in(x,x+h)}\big\Vert\dd^{(k+1)}f(z)\big\Vert\,\Vert
h\Vert^{k+1}$, a conclusão segue do lema anterior.\fimprova

\smallskip

\proclaim Teorema. (Taylor com resto integral) Se $f$ é de classe
$C^{k+1}$ em $U$ ($k\ge0$) e se o segmento $[x,x+h]$ está contido
em $U$ então o resto $r$ do $k$-ésimo polinômio de Taylor de $f$
em torno de $x$ é dado por:
$$r(h)={1\over
k!}\int_0^1(1-t)^k\dd^{(k+1)}f(x+th)\cdot(h)^{(k+1)}\,\dd t.$$

\Prova Defina $\phi$ como na demonstração da fórmula de Taylor com
resto de Lagrange. Observe que $\phi$ é de classe $C^1$ em $[0,1]$
e aplique o teorema fundamental do cálculo para $\phi$, i.e.,
observe que:
$$r(h)=\phi(1)-\phi(0)=\int_0^1\phi'(t)\,\dd t.$$
A conclusão segue.\fimprova

\smallskip

\noindent{\tt Observação}. Uma boa maneira de lembrar os
enunciados precisos das fórmulas de Taylor é o seguinte: a fórmula
de Taylor com resto infinitesimal é uma generalização da definição
de função diferenciável (recaímos na definição de função
diferenciável quando $k=1$). A fórmula de Taylor com resto de
Lagrange é uma generalização da desigualdade do valor médio
(reobtemos a desigualdade do valor médio com $k=0$). Finalmente, a
fórmula de Taylor com resto integral é uma generalização do
Teorema Fundamental do Cálculo (recaímos no Teorema Fundamental do
Cálculo para $k=0$).

\smallskip

\noindent{\tt Observação}. O lema que foi chamado acima de
``desigualdade do valor médio generalizada'', leva esse nome pois
tal lema nos dá novamente a desigualdade do valor médio para a
função $\phi:[a,b]\to\R^m$ se tomarmos
$\psi(t)=t\sup_{s\in(a,b)}\big\Vert\phi'(s)\big\Vert$,
$t\in[a,b]$.

\smallskip

\noindent{\tt Exemplo}. Se $f:U\subset\R^m\to\R^n$ é $k$ vezes
diferenciável num ponto $x\in U$ então não é difícil mostrar que
(em notação de multi-índice):
$$\dd^{(k)}f(x)\cdot(h)^{(k)}=\sum_{\vert\lambda\vert=k}{k!\over\lambda!}{\partial^kf\over
\partial x^\lambda}(x)h^\lambda,$$
para todo $h\in\R^m$. Segue que o polinômio de Taylor de ordem $k$
de $f$ em torno de $x$ pode ser escrito de maneira explícita em
termos de derivadas parciais na forma:
$$p(h)=\sum_{i=0}^k\sum_{\vert\lambda\vert=i}{1\over\lambda!}{\partial^if\over\partial
x^\lambda}(x)h^\lambda.$$

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Funções polinomiais}.

\medskip

\item{1.} Denote por $(e_i)_{i=1}^n$ a base canônica de $\R^n$.
Para todo multi-índice $\lambda=(\lambda_1,\ldots,\lambda_m)$ de
dimensão $m$, denote por $p^i_\lambda:\R^m\to\R^n$ a função
polinomial dada por $p^i_\lambda(x)=e_ix^\lambda$, para todo
$x\in\R^m$. Mostre que:

\smallskip

\itemitem{(a)} as funções $p^i_\lambda$ com $i=1,\ldots,n$,
$\vert\lambda\vert=k$ formam uma base do espaço
$\Pol_k(\R^m,\R^n)$ de todas as funções polinomiais homogêneas
$p:\R^m\to\R^n$ de grau $k$; conclua que $\Pol_k(\R^m,\R^n)$ tem
dimensão $n{m+k-1\choose k}$;

\itemitem{(b)} as funções $p^i_\lambda$ com $i=1,\ldots,n$,
$\vert\lambda\vert\le k$ formam uma base do espaço $\Pol_{\le
k}(\R^m,\R^n)$ de todas as funções polinomiais $p:\R^m\to\R^n$ de
grau menor ou igual a $k$; conclua que $\Pol_{\le k}(\R^m,\R^n)$
tem dimensão $n{m+k\choose k}$.

\smallskip

\item{2.} Sejam $V$, $W$ espaços vetoriais e seja
$B\in\Multlin_k(V;W)$ uma aplicação multi-linear. O {\sl
simetrizador\/} de $B$ é definido por:
$$\Sym(B)(v_1,\ldots,v_k)={1\over k!}\sum_{\sigma\in
S_k}B(v_{\sigma(1)},\ldots,v_{\sigma(k)}),$$ para todos
$v_1,\ldots,v_k\in V$, onde $S_k$ denota o conjunto das bijeções
$\sigma:\{1,\ldots,k\}\to\{1,\ldots,k\}$. Mostre que:

\smallskip

\itemitem{(a)} $\Sym(B)\in\Multlin_k(V;W)$, i.e., que o simetrizador
de $B$ é $k$-linear;

\itemitem{(b)} $\Sym(B)$ é simétrica;

\itemitem{(c)} $B$ é simétrica se e somente se $\Sym(B)=B$;

\itemitem{(d)} $B(x)^{(k)}=\Sym(B)(x)^{(k)}$ para todo $x\in V$,
i.e., $B$ e $\Sym(B)$ definem a mesma função polinomial homogênea
de grau $k$;

\itemitem{(e)} $\Multlin_k(V;W)\in
B\mapsto\Sym(B)\in\Multlin_k(V;W)$ é um {\sl operador de
projeção\/} (i.e., um operador linear cujo quadrado é igual a si
próprio) e sua imagem é $\Multlinsym_k(V;W)$.

\smallskip

\item{3.} Seja $B\in\Multlinsym_k(V;W)$ um operador linear
simétrico tal que $B(x)^{(k)}=0$ para todo $x\in V$. O objetivo
deste exercício é mostrar que $B=0$ {\sl sem usar derivadas}!

\smallskip

\itemitem{(a)} dados $x,y\in V$, mostre a {\sl fórmula do
binômio}:
$$B(x+y)^{(k)}=\sum_{i=0}^k{k\choose i}B(x)^{(i)}(y)^{(k-i)},$$
onde $B(x)^{(i)}(y)^{(k-i)}=B(\underbrace{x,\ldots,x}_{i\ {\rm
vezes}},\underbrace{y,\ldots,y}_{k-i\ {\rm vezes}})$;

\itemitem{(b)} dados $x,y\in V$, $t\in\R$, use o fato que
$B(x+ty)^{(k)}=0$ juntamente com a fórmula do binômio para
concluir que $B(x)^{(i)}(y)^{(k-i)}=0$ para todos $x,y\in V$ ({\sl
dica}: vocês lembram do determinante de Vandermond?)

\itemitem{(c)} defina uma aplicação $(k-1)$-linear simétrica
$\widetilde B\in\Multlin_{k-1}\big(V;\Lin(V,W)\big)$ fazendo
$\widetilde B(x_1,\ldots,x_{k-1})(y)=B(x_1,\ldots,x_{k-1},y)$. Use
o item (b) para concluir que $\widetilde B(x)^{(k-1)}=0$ para todo
$x\in V$ e obtenha a conclusão final usando indução em $k$.

\medskip

\noindent{\tt Diferenciação}.

\medskip

\item{4.} Sejam $f:U\subset\R^m\to\R^n$ uma função (com $U\subset\R^m$ aberto), $[x,x+h]$ um segmento contido em $U$ e $t\in(0,1)$.
Se $f$ é $k+1$ vezes diferenciável no ponto $x+th$, mostre que:
$${\dd\over\dd t}\dd^{(k)}f(x+th)\cdot(h)^{(k)}=\dd^{(k+1)}f(x+th)\cdot(h)^{(k+1)}.$$
{\sl dica}: considere a composta
$\lambda\circ\dd^{(k)}f\circ\iota$, onde $\iota:(0,1)\to\R^m$ é
definida por $\iota(t)=x+th$ e
$\lambda:\Multlin_k(\R^m;\R^n)\to\R^n$ é definida por
$\lambda(B)=B(h)^{(k)}$. Use também a equação ($*$) da Aula número 10 (05/04).

\vfil\eject

\centerline{\bf Aula número $14$ (26/04)}
\bigskip
\bigskip

A aula começa cobrindo o material sobre as estimativas para o
resto do polinômio de Taylor, originalmente destinado à aula
número 13.

\medskip

\item{(1)} {\bf A recíproca da fórmula de Taylor}.

\smallskip

O objetivo desta seção é provar o seguinte: \proclaim Teorema.
Seja $f:U\subset\R^m\to\R^n$ uma função $k$ vezes diferenciável
num ponto $x$ do aberto $U\subset\R^m$ ($k\ge1$) e suponha que
$p\in\Pol_{\le k}(\R^m,\R^n)$ é um polinômio de grau menor ou
igual a $k$ tal que, definindo $r$ pela fórmula:
$$f(x+h)=p(h)+r(h),$$
então $\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$. Então $p$ é o
polinômio de Taylor de ordem $k$ de $f$ em torno de $x$.

O teorema acima nos diz que {\sl o polinômio de Taylor é o único
polinômio de grau menor ou igual a $k$ que faz a ``estimativa
infinitesimal'' do resto funcionar}.

\smallskip

\noindent{\tt Observação}. A ``estimativa infinitesimal'' para o
resto $r$ do polinômio de Taylor de ordem $k$ é a mais fraca das
estimativas que consideramos, no sentido que tanto a ``estimativa
de Lagrange'' para $r$ como a fórmula integral para $r$ implicam a
``estimativa infinitesimal'', i.e., que
$\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$ (supondo $f$ de classe
$C^{k+1}$, ao menos). Segue então do teorema acima que, se $p$ é
um polinômio de grau menor ou igual a $k$ para o qual o resto $r$
satisfaz a ``estimativa de Lagrange'' ou a fórmula integral então
$p$ é necessariamente o polinômio de Taylor de $f$ de ordem $k$.

\smallskip

Antes de mostrar o teorema, mostraremos o seguinte: \proclaim
Lema. Seja $p\in\Pol_{\le k}(\R^m,\R^n)$ um polinômio de grau
menor ou igual a $k$ e suponha que $\lim_{h\to0}{p(h)\over\Vert
h\Vert^k}=0$. Então $p=0$.

\Prova Fixado $v\in\R^m$, $v\ne0$, fazemos $h=tv$ e $t\to0$
obtendo:
$$\lim_{t\to0}{p(tv)\over\vert t\vert^k\Vert
v\Vert^k}=\lim_{t\to0}{p(tv)\over t^k}=0.$$ Escreva
$p=\sum_{i=0}^kp_i$, onde cada $p_i\in\Pol_i(\R^m,\R^n)$ é um
polinômio homogêneo de grau $i$ (e $p_0\in\R^n$ é um vetor
fixado). Obtemos então:
$$\lim_{t\to0}{p_0+p_1(v)t+p_2(v)t^2+\cdots+p_k(v)t^k\over
t^k}=0.$$ Um exercício simples de Cálculo I mostra que a igualdade
acima só é possível se $p_i(v)=0$ para $i=0,\ldots,k$. Como
$v\in\R^m\setminus\{0\}$ é arbitrário (e obviamente $p_i(0)=0$),
temos $p_i=0$ para todo $i$ e portanto $p=0$.\fimprova

\smallskip

Vamos agora mostrar o resultado principal da seção. Suponha que
$f:U\subset\R^m\to\R^n$ ($U\subset\R^m$ aberto) é $k$ vezes
diferenciável no ponto $x\in U$ ($k\ge1$), que $p\in\Pol_{\le
k}(\R^m,\R^n)$ é um polinômio de grau menor ou igual a $k$ e que o
resto $r(h)=f(x+h)-p(h)$ satisfaz $\lim_{h\to0}{r(h)\over\Vert
h\Vert^k}=0$. Devemos mostrar que $p$ é o polinômio de Taylor de
ordem $k$ de $f$ em torno de $x$. Como $f$ é $k$ vezes
diferenciável em $x$ e $p$ é de classe $C^\infty$, temos que $r$ é
$k$ vezes diferenciável em $0$ e aplicando a fórmula de Taylor
infinitesimal para $r$ em torno de $0$ obtemos:
$$r(h)=q(h)+\widetilde r(h),$$
onde $q(h)=r(0)+\dd r(0)\cdot h+\cdots+{1\over
k!}\dd^{(k)}r(0)\cdot(h)^{(k)}$ é o polinômio de Taylor de ordem
$k$ de $r$ em torno de $0$ e $\widetilde r$ satisfaz:
$$\lim_{h\to0}{\widetilde r(h)\over\Vert h\Vert^k}=0;$$
como também $\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$, obtemos
$\lim_{h\to0}{q(h)\over\Vert h\Vert^k}=0$ e segue do lema acima
que $q=0$. Concluímos então que $r=\widetilde r$ e portanto
$r$ se anula em $0$ juntamente com suas $k$ primeiras diferenciais
(recorde que o resto de um polinômio de Taylor de ordem $k$ {\sl
sempre\/} se anula juntamente com suas $k$ primeiras diferenciais
no ponto $0$ --- e $r=\widetilde r$). Daí $f(x)=p(0)$ e
$\dd^{(i)}f(x)=\dd^{(i)}p(0)$ para $i=1,\ldots,k$, donde segue do
primeiro Teorema da Seção~3 da Aula número $13$ (24/04) que $p$ é o
polinômio de Taylor de ordem $k$ de $f$ em torno de $x$. Isso
completa o argumento.

\vfil\eject

\centerline{\bf Aula número $15$ (03/05)}
\bigskip
\bigskip

\item{(1)} {\bf Aplicação da fórmula de Taylor: máximos e mínimos
locais}.

\smallskip

Vocês provavelmente já sabem que se $f:U\to\R$ ($U\subset\R^m$
aberto) é diferenciável num ponto $x\in U$ e possui um máximo ou
mínimo local em $x$ então $\dd f(x)=0$ (uma maneira de ver isso é
a seguinte: para todo $v\in\R^m$, a função de uma variável
$t\mapsto f(x+tv)$ possui um máximo local em $t=0$ e portanto
sabemos do Cálculo I que sua derivada $\left.{\dd\over\dd
t}f(x+tv)\right\vert_{t=0}$ em $t=0$ se anula; isso significa que
$\dd f(x)\cdot v=0$ e como $v\in\R^m$ é arbitrário temos $\dd
f(x)=0$). Esse fato simples será redemonstrado nesta seção como
conseqüência da fórmula de Taylor, e muito mais: obteremos uma
série de critérios que relacionam os mínimos e máximos locais de
$f$ com suas diferenciais de ordem superior.

\smallskip

\proclaim Definição. Sejam $U\subset\R^m$ um aberto e $f:U\to\R$
uma função. Dizemos que $x\in U$ é um {\rm ponto de máximo}
(global) para $f$ se $f(y)\le f(x)$ para todo $y\in U$; dizemos
que $x\in U$ é um {\rm ponto de máximo estrito} (global) para $f$
se $f(y)<f(x)$ para todo $y\in U$ com $y\ne x$. Dizemos que $x\in
U$ é um {\rm ponto de máximo local} quando $x$ possui uma
vizinhança $V$ em $U$ (ou uma vizinhança $V$ em $\R^m$ contida em
$U$, tanto faz) tal que $x$ é um ponto de máximo de $f\vert_V$;
dizemos que $x$ é um {\rm ponto de máximo local estrito} se $x$
possui uma vizinhança $V$ em $U$ tal que $x$ é um ponto de máximo
estrito de $f\vert_V$.

Analogamente, define-se as noções de {\sl ponto de mínimo\/}
(global), {\sl ponto de mínimo estrito\/} (global),
{\sl ponto de mínimo local\/} e de {\sl ponto de mínimo local estrito}; é
só trocar $f(y)\le f(x)$ por $f(y)\ge f(x)$ e $f(y)<f(x)$ por
$f(y)>f(x)$ na definição acima.

\smallskip

\noindent{\tt Observação}. Obviamente noções envolvendo máximos e
mínimos só podem ser definidas para {\sl funções a valores em
$\R$}.

\smallskip

Seja $f:U\subset\R^m\to\R$ uma função $k$ vezes diferenciável num
ponto $x\in U$, onde $U\subset\R^m$ é um aberto. Suponha que as
$k-1$ primeiras diferenciais de $f$ em $x$ sejam nulas, mas que a
$k$-ésima não seja, i.e., $\dd^{(i)}f(x)=0$ para $i=1,\ldots,k-1$
mas $\dd^{(k)}f(x)\ne0$. A fórmula de Taylor infinitesimal nos dá
então:
$$f(x+h)=f(x)+{1\over k!}\dd^{(k)}f(x)\cdot(h)^{(k)}+r(h),$$
com $\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$.

\smallskip

\noindent{\tt Observação}. Tipicamente, o que ocorre é o seguinte:
temos uma função $f:U\subset\R^m\to\R$ de classe $C^\infty$ num
aberto $U\subset\R^m$ e escolhemos um ponto $x\in U$. Daí sempre
podemos escolher o menor inteiro $k\ge1$ tal que
$\dd^{(k)}f(x)\ne0$ e portanto $\dd^{(i)}f(x)=0$ para
$i=1,\ldots,k-1$; o único caso em que nossa técnica falha é se
{\sl todas\/} as diferenciais de $f$ no ponto $x$ se anulam.
Infelizmente, é de fato possível que todas as diferenciais de $f$
se anulem em $x\in U$, sem que $f$ seja constante numa vizinhança
de $x$ (veremos exemplos neste curso). De qualquer modo, a técnica
explicada nesta seção ainda se aplica numa enorme classe de
funções.

\smallskip

Voltando à situação acima, escrevemos:
$$f(x+h)=f(x)+\Vert
h\Vert^k\left({1\over k!}
\dd^{(k)}f(x)\cdot\left({\textstyle{h\over\Vert
h\Vert}}\right)^{(k)}+{r(h)\over\Vert
h\Vert^k}\right),\eqno{(*)}$$ para todo $h\in\R^m$ suficientemente
pequeno com $h\ne0$. Nossa estratégia agora é a seguinte: como
${r(h)\over\Vert h\Vert^k}$ é muito pequeno para $h$ próximo da
origem, vamos usar a equação ($*$) para concluir que o sinal de
$f(x+h)-f(x)$ é o mesmo de $\dd^{(k)}f(x)\cdot\left({h\over\Vert
h\Vert}\right)^{(k)}$ para $h$ suficientemente próximo de $0$,
desde que $\dd^{(k)}f(x)\cdot\left({h\over\Vert
h\Vert}\right)^{(k)}\ne0$. Note que, ao contrário de
${r(h)\over\Vert h\Vert^k}$, o valor de
$\dd^{(k)}f(x)\cdot\left({h\over\Vert h\Vert}\right)^{(k)}$ {\sl
não diminui quando $h\to0$}!

\smallskip

Passamos às considerações formais. Suponha que existam $h_1,h_2\in\R^m$ com:
$$\dd^{(k)}f(x)\cdot(h_1)^{(k)}>0\quad\hbox{e}\quad\dd^{(k)}f(x)\cdot(h_2)^{(k)}<0;$$
por exemplo, isso {\sl sempre\/} ocorre se $k$ é ímpar, pois nesse
caso:
$$\dd^{(k)}f(x)\cdot(-h)^{(k)}=-\dd^{(k)}f(x)\cdot(h)^{(k)},$$
para todo $h\in\R^m$.

Fazendo $h=th_1$ com $t>0$ na equação ($*$) obtemos:
$$f(x+th_1)=f(x)+t^k\Vert h_1\Vert^k\left({1\over k!}\dd^{(k)}f(x)\cdot\left({\textstyle{h_1\over\Vert
h_1\Vert}}\right)^{(k)}+{r(th_1)\over\Vert th_1\Vert^k}\right);$$
como $\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$ temos
$\lim_{t\to0^+}{r(th_1)\over\Vert th_1\Vert^k}=0$ e portanto:
$$\left\vert{r(th_1)\over\Vert th_1\Vert^k}\right\vert<\underbrace{{1\over k!}\dd^{(k)}f(x)\cdot\left({\textstyle{h_1\over\Vert
h_1\Vert}}\right)^{(k)}}_{>0},$$ para todo $t>0$ suficientemente
pequeno. Concluímos então que:
$$f(x+th_1)>f(x),$$
para todo $t>0$ suficientemente pequeno e portanto $x$ {\sl não é
um ponto de máximo local}. Repetindo o raciocínio acima com $h_2$
no lugar de $h_1$ concluiremos que:
$$f(x+th_2)<f(x),$$
para todo $t>0$ suficientemente pequeno e portanto $x$ também {\sl
não é um ponto de mínimo local}.

\smallskip

Mostramos acima que se $f$ é $k$ vezes diferenciável em $x\in U$ e
$\dd^{(i)}f(x)=0$ para $i=1,\ldots,k-1$ então {\sl o valor de $f$
cresce (estritamente) quando andamos a partir de $x$ numa direção
$h\in\R^m$ com $\dd^{(k)}f(x)\cdot(h)^{(k)}>0$ e o valor de $f$
decresce (estritamente) quando andamos a partir de $x$ numa
direção $h\in\R^m$ com $\dd^{(k)}f(x)\cdot(h)^{(k)}<0$}.

\smallskip

\proclaim Definição. Seja $B\in\Multlinsym_k(V;\R)$ uma aplicação
$k$-linear simétrica definida num espaço vetorial real $V$.
Dizemos que:
\smallskip
\item{\rm($\bullet$)} $B$ é {\rm definida positiva} quando
$B(v)^{(k)}>0$ para todo $v\in V$, $v\ne0$;
\item{\rm($\bullet$)} $B$ é {\rm semi-definida positiva} quando
$B(v)^{(k)}\ge0$ para todo $v\in V$;
\item{\rm($\bullet$)} $B$ é {\rm definida negativa} quando
$B(v)^{(k)}<0$ para todo $v\in V$, $v\ne0$;
\item{\rm($\bullet$)} $B$ é {\rm semi-definida negativa} quando
$B(v)^{(k)}\le0$ para todo $v\in V$;
\item{\rm($\bullet$)} $B$ é {\rm indefinida} quando existem $v,w\in
V$ com $B(v)^{(k)}>0$ e $B(w)^{(k)}<0$.

Observe que se $k$ é ímpar então uma aplicação $k$-linear
simétrica $B$ é {\sl sempre\/} indefinida, a menos que $B=0$ (isso
segue da igualdade $B(-v)^{(k)}=-B(v)^{(k)}$, como já havíamos
comentado).

\smallskip

Suponha agora que $f:U\to\R$ é $k$ vezes diferenciável num ponto
$x\in U$ ($U\subset\R^m$ aberto), $\dd^{(i)}f(x)=0$ para
$i=1,\ldots,k-1$ e que $\dd^{(k)}f(x)$ seja definida positiva.
Pelo que mostramos acima, o valor de $f$ cresce quando caminhamos
a partir de $x$ {\sl em qualquer direção}. Dá para desconfiar
então que $x$ seja um mínimo local de $f$, mas ainda falta um
argumento para que possamos concluir isso. De fato, sabemos que
para todo $h\in\R^m$, $\Vert h\Vert=1$, existe $\delta>0$ tal que
$f(x+th)>f(x)$ para $0<t<\delta$ --- o problema é que {\sl
$\delta$ pode em princípio depender de $h$}. Nosso objetivo agora
é mostrar que {\sl de fato é possível escolher $\delta>0$
independente de $h$}.

Como aplicações multi-lineares são contínuas e a esfera unitária
de $\R^m$ é compacta, temos:
$$\inf_{\Vert h\Vert=1}{1\over
k!}\dd^{(k)}f(x)\cdot(h)^{(k)}=\varepsilon>0,$$ pois uma função
contínua num compacto assume seu valor máximo e seu valor mínimo.
Como $\lim_{h\to0}{r(h)\over\Vert h\Vert^k}=0$, podemos encontrar
$\delta>0$ tal que:
$$0<\Vert h\Vert<\delta\Longrightarrow\left\vert{r(h)\over\Vert
h\Vert^k}\right\vert<\varepsilon\le{1\over
k!}\dd^{(k)}f(x)\cdot\left({\textstyle{h\over\Vert h\Vert}
}\right)^{(k)}.$$ A partir da equação ($*$) obtemos então:
$$0<\Vert h\Vert<\delta\Longrightarrow f(x+h)>f(x),$$
e portanto {\sl $x$ é de fato um mínimo local estrito de $f$}.
Obviamente, um argumento análogo mostra que se $\dd^{(k)}f(x)$ é
definida negativa então $x$ é um ponto de máximo local estrito de
$f$.

\smallskip

Podemos resumir todas as informações obtidas nesta seção no
seguinte: \proclaim Teorema. Sejam $U\subset\R^m$ um aberto, $x\in
U$ e $f:U\to\R$ uma função $k$ vezes diferenciável no ponto $x$
($k\ge1$). Suponha que $\dd^{(i)}f(x)=0$ para $i=1,\ldots,k-1$ e
que $\dd^{(k)}f(x)\ne0$. Temos que:
\item{\rm(a)} se $k$ é ímpar então $x$ não é nem um ponto de máximo
local nem um ponto de mínimo local para $f$;
\item{\rm(b)} se $k$ é par e $\dd^{(k)}f(x)$ é indefinida então $x$
não é nem um ponto de máximo local nem um ponto de mínimo local
para $f$;
\item{\rm(c)} se $k$ é par e $\dd^{(k)}f(x)$ é definida positiva
então $x$ é um ponto de mínimo local estrito para $f$;
\item{\rm(d)} se $k$ é par e $\dd^{(k)}f(x)$ é definida negativa
então $x$ é um ponto de máximo local estrito para $f$;
\item{\rm(e)} se $x$ é um ponto de máximo local para $f$ então $k$ é
par e $\dd^{(k)}f(x)$ é semi-definida negativa;
\item{\rm(f)} se $x$ é um ponto de mínimo local para $f$ então $k$ é
par e $\dd^{(k)}f(x)$ é semi-definida positiva.\fimprova

\smallskip

\proclaim Corolário. Seja $f:U\to\R$ uma função diferenciável num
ponto $x$ do aberto $U\subset\R^m$. Se $x$ é um ponto de máximo ou
mínimo local para $f$ então $\dd f(x)=0$.

\Prova Aplique o teorema acima com $k=1$.\fimprova

\medskip

\item{(2)} {\bf Seqüências de funções}.

\smallskip

\proclaim Definição. Sejam $X$ um conjunto, $(M,d)$ um espaço
métrico e $(f_n)_{n\ge1}$ uma seqüência de funções $f_n:X\to M$.
Dizemos que $(f_n)_{n\ge1}$ {\rm converge pontualmente} (ou {\rm
simplesmente}) para uma função $f:X\to M$ se para cada $x\in X$ a
seqüência $\big(f_n(x)\big)_{n\ge1}$ converge para $f(x)$ no
espaço métrico $M$. Dito de outro modo, $(f_n)_{n\ge1}$ converge
pontualmente para $f:X\to M$ quando dados $x\in X$ e
$\varepsilon>0$, existe $n_0\in\N$ tal que
$d\big(f_n(x),f(x)\big)<\varepsilon$ para todo $n\ge n_0$. Dizemos
que $(f_n)_{n\ge1}$ {\rm converge uniformemente} para $f:X\to M$
quando dado $\varepsilon>0$, existe $n_0\in\N$ tal que
$d\big(f_n(x),f(x)\big)<\varepsilon$ para todo $n\ge n_0$ e {\rm
todo $x\in X$}.

Usamos a seguinte notação:
$$\eqalign{%
f_n\limpp f\quad&\hbox{significa ``$(f_n)_{n\ge1}$ converge
pontualmente para $f$''},\cr f_n\limu f\quad&\hbox{significa
``$(f_n)_{n\ge1}$ converge uniformemente para $f$''}.\cr}$$

A diferença entre convergência uniforme e pontual é apenas uma
diferença no posicionamento de quantificadores; de fato, observe
que:
$$\eqalign{%
f_n\limpp f\quad&\Longleftrightarrow\quad\forall\,\varepsilon>0,\;
\forall\,x\in X,\;\exists\,n_0\in\N,\;\forall\,n\in\N,\;n\ge
n_0\Longrightarrow d\big(f_n(x),f(x)\big)<\varepsilon,\cr f_n\limu
f\quad&\Longleftrightarrow\quad\forall\,\varepsilon>0,\;
\exists\,n_0\in\N,\;\forall\,x\in X,\;\forall\,n\in\N,\;n\ge
n_0\Longrightarrow d\big(f_n(x),f(x)\big)<\varepsilon.\cr}$$ No
primeiro caso, $n_0$ pode depender de $x$, e no segundo deve ser
possível encontrar {\sl um único\/} $n_0$ para {\sl todo\/} $x\in
X$. Essa ``pequena'' diferença faz uma diferença enorme, como
veremos adiante. Observe que obviamente convergência uniforme
implica convergência pontual, i.e., $f_n\limu f$ implica
$f_n\limpp f$.

\smallskip

\noindent{\tt Exemplo}. Para cada $n\in\N$, seja $f_n:\R\to\R$ a
função dada por $f_n(x)={x\over n}$, para todo $x\in\R$.
Obviamente $f_n(x)\to0$ para todo $x\in\R$, donde $f_n\limpp\,0$.
Por outro lado, fixado $n\in\N$ então $\lim_{x\to+\infty}\big\vert
f_n(x)\big\vert=+\infty$ --- isso implica que $(f_n)_{n\ge1}$ {\sl
não converge uniformemente\/} para a função nula. Por outro lado,
se $X\subset\R$ é um conjunto limitado então $f_n\vert_X\limu
f\vert_X$; de fato, se $M>0$ é tal que $X\subset[-M,M]$ então
$\big\vert f_n(x)\big\vert<\varepsilon$ para todo $x\in X$ e todo
$n\ge n_0$, desde que $n_0\in\N$ seja escolhido de modo que
${M\over n_0}<\varepsilon$.

\smallskip

O exemplo acima poderia dar ao leitor uma falsa impressão de que
convergência pontual implica convergência uniforme num espaço
compacto (já que compactos normalmente estão associados à
``uniformidades'' --- por exemplo, funções contínuas em compactos
{\sl são\/} uniformemente contínuas). Veremos no exemplo a seguir
que uma seqüência pontualmente convergente num compacto {\sl pode
não ser uniformemente convergente, mesmo que todas as funções
envolvidas sejam contínuas}.

\smallskip

\noindent{\tt Exemplo}. Para cada $n\in\N$, seja $f_n:[0,1]\to\R$
a única função contínua que se anula fora de $\big[{1\over
n+1},{1\over n}\big]$, é afim em cada metade de $\big[{1\over
n+1},{1\over n}\big]$ e vale $1$ no ponto médio de $\big[{1\over
n+1},{1\over n}\big]$; explicitamente:
$$f_n(x)=\cases{
0,&$x\in[0,1],\ x\not\in\big[{1\over n+1},{1\over n}\big]$,\cr
\noalign{\medskip}2n(n+1)\big(x-{1\over n+1}\big),&${1\over
n+1}\le x\le{2n+1\over2n(n+1)}$,\cr\noalign{\medskip}
2n(n+1)\big({1\over n}-x\big),&${2n+1\over2n(n+1)}\le x\le{1\over
n}$.\cr}$$ Fixado $x\in[0,1]$, temos que $f_n(x)=0$ para todo $n$
suficientemente grande e portanto $f_n\limpp\,0$. Por outro lado,
toda função $f_n$ assume o valor $1$ e portanto não pode ser
$f_n\limu\,0$.

\smallskip

O próximo teorema é uma das principais motivações para a
introdução do conceito de convergência uniforme.

\proclaim Teorema. Sejam $M$, $N$ espaços métricos e
$(f_n)_{n\ge1}$ uma seqüência de funções $f_n:M\to N$. Suponha que
$(f_n)_{n\ge1}$ converge uniformemente para uma função $f:M\to N$
e que todas as funções $f_n$ são contínuas num certo ponto $x\in
M$. Então $f$ também é contínua em $x$. Em particular, se cada
$f_n$ é contínua em $M$ então $f$ é contínua em $M$ (``o limite
uniforme de funções contínuas é contínuo'').

\Prova Seja dado $\varepsilon>0$. Como $(f_n)_{n\ge1}$ converge
uniformemente para $f$, existe $n\in\N$ tal que
$d\big(f_n(y),f(y)\big)<{\varepsilon\over3}$ para todo $y\in M$.
Como $f_n$ é contínua no ponto $x$, existe $\delta>0$ tal que
$d\big(f_n(y),f_n(x)\big)<{\varepsilon\over3}$, para todo $y\in M$
com $d(y,x)<\delta$. Daí, para todo $y\in M$:
$$\eqalign{%
d(y,x)<\delta\Longrightarrow d\big(f(y),f(x)\big)&\le
d\big(f(y),f_n(y)\big)+d\big(f_n(y),f_n(x)\big)+d\big(f_n(x),f(x)\big)\cr
&<{\varepsilon\over3}+{\varepsilon\over3}+{\varepsilon\over3}=\varepsilon.\cr}$$
Logo $f$ é contínua no ponto $x$.\fimprova

\smallskip

\noindent{\tt Exemplo}. Para cada $n\in\N$, seja $f_n:[0,1]\to\R$
a função definida por $f_n(x)=x^n$, para todo $x\in[0,1]$. Temos
que $f_n\limpp f$, onde $f(x)=0$ para $x\in\left[0,1\right)$ e
$f(1)=1$. Observe que cada $f_n$ é contínua, mas $f$ não é (em
particular, pelo teorema acima, não pode ser $f_n\limu f$).

\smallskip

O seguinte conceito é útil para reconhecer seqüências
uniformemente convergentes sem achar o limite explicitamente:
\proclaim Definição. Sejam $X$ um conjunto, $(M,d)$ um espaço
métrico e $(f_n)_{n\ge1}$ uma seqüência de funções $f_n:X\to M$.
Dizemos que $(f_n)_{n\ge1}$ é {\rm uniformemente de Cauchy} quando
dado $\varepsilon>0$, existe $n_0\in\N$ tal que
$d\big(f_n(x),f_m(x)\big)<\varepsilon$ para todo $x\in X$ e todos $n,m\ge
n_0$ ($n_0$ pode depender de $\varepsilon$, mas {\rm não} de $x$).

\smallskip

\proclaim Teorema. Sejam $X$ um conjunto, $M$ um espaço métrico
completo e $(f_n)_{n\ge1}$ uma se\-qüên\-cia uniformemente de
Cauchy de funções $f_n:X\to M$. Então existe uma (única) função
$f:X\to M$ tal que $f_n\limu f$.

\Prova Obviamente $\big(f_n(x)\big)_{n\ge1}$ é uma seqüência de
Cauchy em $M$ para todo $x\in M$ e portanto converge para um ponto
de $M$; podemos definir então $f:X\to M$ fazendo
$f(x)=\lim_{n\to+\infty}f_n(x)$. Obviamente $f_n\limpp f$; devemos
mostrar que $f_n\limu f$. Seja dado $\varepsilon>0$; como
$(f_n)_{n\ge1}$ é uniformemente de Cauchy, existe $n_0\in\N$ tal
que $d\big(f_n(x),f_m(x)\big)<\varepsilon$ para todo $x\in X$ e
todos $n,m\ge n_0$. Fixado $n\ge n_0$ e $x\in X$ temos:
$$d\big(f_n(x),f(x)\big)=\lim_{m\to+\infty}d\big(f_n(x),f_m(x)\big)\le\varepsilon,$$
onde a primeira igualdade acima é justificada pela continuidade da
função $d:M\times M\to\R$ (recorde Exercício~3 da Aula número 7
--- 27/03) e pelo fato que $\lim_{m\to+\infty}f_m(x)=f(x)$. Concluímos então que
$d\big(f_n(x),f(x)\big)\le\varepsilon$ para todo $n\ge n_0$ e todo
$x\in X$ (observe que $n_0$ foi escolhido antes de $x$!). Logo
$f_n\limu f$.\fimprova

\smallskip

Nosso interesse sobre convergência de funções está em suas
relações com a diferenciação. Seria razoável esperar resultados do
tipo ``$f_n\to f$ implica $\dd f_n\to\dd f$'' {\sl sob as
hipóteses certas}. No exemplo a seguir, veremos que a
possibilidade mais óbvia não funciona.

\smallskip

\noindent{\tt Exemplo}. Para cada $n\in\N$, seja $f_n:\R\to\R$ a
função definida por $f_n(x)={1\over n}\sen(nx)$, para todo
$x\in\R$. Como $\big\vert f_n(x)\big\vert\le{1\over n}$ para todo
$x\in\R$, é claro que $f_n\limu\,0$. Por outro lado,
$f'_n(x)=\cos(nx)$ e obviamente $(f'_n)_{n\ge1}$ não converge para
zero (nem pontualmente).

\smallskip

O que funciona é o seguinte: \proclaim Teorema. Sejam
$U\subset\R^m$ um aberto convexo limitado e $(f_k)_{k\ge1}$ uma
seqüência de funções diferenciáveis $f_k:U\to\R^n$, de modo que
$(\dd f_k)_{k\ge1}$ converge uniformemente para uma função
$g:U\to\Lin(\R^m,\R^n)$. Suponha que existe algum ponto $x_0\in U$
para o qual a seqüencia $\big(f_k(x_0)\big)_{k\ge1}$ converge em
$\R^n$. Então $(f_k)_{k\ge1}$ converge uniformemente para alguma
função $f:U\to\R^n$, $f$ é diferenciável e $\dd f=g$.

\Prova Sejam $x\in U$, $k,l\in\N$; como $U$ é convexo, podemos
aplicar a desigualdade do valor médio para a função $f_k-f_l$ no
segmento $[x_0,x]$ obtendo:
$$\big\Vert\big(f_k(x)-f_l(x)\big)-\big(f_k(x_0)-f_l(x_0)\big)\big\Vert\le\sup_{z\in(x_0,x)}\big\Vert
\dd f_k(z)-\dd f_l(z)\big\Vert\,\Vert x-x_0\Vert.$$ Como $U$ é
limitado, existe $M>0$ com $\Vert x-x_0\Vert\le M$ para todo $x\in
U$. Como $(\dd f_k)_{k\ge1}$ é uniformemente convergente, temos
que $(\dd f_k)_{k\ge1}$ é uniformemente de Cauchy (veja Exercício 5) e
portanto, dado $\varepsilon>0$, podemos encontrar $k_0\in\N$ tal que
$\big\Vert\dd f_k(z)-\dd f_l(z)\big\Vert<{\varepsilon\over2M}$
para todo $z\in U$ e todos $k,l\ge k_0$. Além do mais, a seqüência
$\big(f_k(x_0)\big)_{k\ge1}$ é convergente em $\R^n$ e portanto de
Cauchy; podemos então aumentar $k_0\in\N$ de modo que $\big\Vert
f_k(x_0)-f_l(x_0)\big\Vert<{\varepsilon\over2}$ para todos $k,l\ge
k_0$. Obtemos então:
$$\big\Vert f_k(x)-f_l(x)\big\Vert\le\big\Vert\big(f_k(x)-f_l(x)\big)-\big(f_k(x_0)-f_l(x_0)\big)\big\Vert
+\big\Vert
f_k(x_0)-f_l(x_0)\big\Vert<{\varepsilon\over2M}M+{\varepsilon\over2}=\varepsilon,$$
para todo $x\in U$ e todos $k,l\ge k_0$ (note que $k_0$ não
depende de $x$!). Mostramos então que $(f_k)_{k\ge1}$ é
uniformemente de Cauchy e portanto existe uma função $f:U\to\R^n$
tal que $(f_k)_{k\ge1}$ é uniformemente convergente para $f$.
Falta mostrar que $f$ é diferenciável e que $\dd f=g$. Fixe então
$x\in U$ e vamos mostrar que $f$ é diferenciável em $x$ com $\dd
f(x)=g(x)$; para isso escrevemos: $$f(x+h)=f(x)+g(x)\cdot h+r(h),$$ e
tentamos mostrar que $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$. Como
$f_k$ é diferenciável em $x$, podemos escrever:
$$f_k(x+h)=f_k(x)+\dd f_k(x)\cdot h+r_k(h),$$
com $\lim_{h\to0}{r_k(h)\over\Vert h\Vert}=0$ para todo $k\in\N$.
Como $f_k\limu f$ e $\dd f_k\limu g$ temos:
$$\lim_{k\to+\infty}r_k(h)=r(h),$$
para todo $h\in\R^m$ com $x+h\in U$ (note que nessa passagem só
usamos que $f_k\limpp f$ e $\dd f_k\limpp g$). Fixados $k,l\in\N$,
considere a função $\phi:U\to\R^n$ definida por:
$$\phi(z)=f_k(z)-f_l(z)-\dd f_k(x)\cdot z+\dd f_l(x)\cdot z,\quad
z\in U\subset\R^m;$$ para todo $h\in\R^m$ com $x+h\in U$ temos
$\phi(x+h)-\phi(x)=r_k(h)-r_l(h)$ e aplicando a desigualdade do
valor médio para $\phi$ no segmento $[x,x+h]\subset U$ obtemos:
$$\eqalign{\big\Vert
r_k(h)-r_l(h)\big\Vert&\le\sup_{z\in(x,x+h)}\big\Vert\dd\phi(z)\big\Vert\,\Vert
h\Vert\cr
&=\sup_{z\in(x,x+h)}\big\Vert\big(\dd
f_k(z)-\dd f_l(z)\big)-\big(\dd f_k(x)-\dd
f_l(x)\big)\big\Vert\,\Vert h\Vert,\cr}$$ para todo $h\in\R^m$ com
$x+h\in U$. Dado $\varepsilon>0$, como $(\dd f_k)_{k\ge1}$ é
uniformemente de Cauchy, podemos encontrar $k_0\in\N$ tal que
$\big\Vert\dd f_k(z)-\dd f_l(z)\big\Vert<{\varepsilon\over4}$ para
todo $z\in U$ e todos $k,l\ge k_0$. Daí:
$$\big\Vert\big(\dd
f_k(z)-\dd f_l(z)\big)-\big(\dd f_k(x)-\dd
f_l(x)\big)\big\Vert\le\big\Vert\dd f_k(z)-\dd
f_l(z)\big\Vert+\big\Vert\dd f_k(x)-\dd
f_l(x)\big\Vert<{\varepsilon\over2},$$ para todo $z\in U$ e
portanto:
$$\big\Vert r_k(h)-r_l(h)\big\Vert\le{\varepsilon\over2}\Vert
h\Vert,$$ para todo $h\in\R^m$ com $x+h\in U$ e todos $k,l\ge
k_0$. Fixando $h\in\R^m$, $k\ge k_0$ e fazendo $l\to+\infty$
obtemos (já que $r_l(h)\to r(h)$):
$$\big\Vert r_k(h)-r(h)\big\Vert\le{\varepsilon\over2}\Vert
h\Vert,$$ para todo $k\ge k_0$ e todo $h\in\R^m$ com $x+h\in U$.
Fixe agora $k=k_0$; como $\lim_{h\to0}{r_k(h)\over\Vert
h\Vert}=0$, vemos que existe $\delta>0$ tal que $\Vert
h\Vert<\delta$ implica $\Vert
r_k(h)\Vert\le{\varepsilon\over2}\Vert h\Vert$. Daí:
$$\Vert h\Vert<\delta\Longrightarrow\big\Vert
r(h)\big\Vert\le\big\Vert r(h)-r_k(h)\big\Vert+\big\Vert
r_k(h)\big\Vert\le{\varepsilon\over2}\Vert
h\Vert+{\varepsilon\over2}\Vert h\Vert=\varepsilon\Vert h\Vert.$$
Isso mostra que $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$ e completa
a demonstração.\fimprova

\smallskip

A hipótese que $U$ seja convexo e limitado no teorema acima é
bastante desagradável; na verdade, ela só é necessária se
quisermos concluir que $(f_k)_{k\ge1}$ {\sl converge
uniformemente\/} para $f$. Se admitirmos um tipo de convergência
mais fraco como conclusão, será possível supor apenas que $U$ é
aberto! Esse é nosso objetivo agora:

\proclaim Definição. Sejam $M$, $N$ espaços métricos e
$(f_n)_{n\ge1}$ uma seqüência de funções $f_n:M\to N$. Dada uma
função $f:M\to N$ então dizemos que:
\item{\rm($\bullet$)} $(f_n)_{n\ge1}$ converge {\rm
local-uniformemente} para $f$ se todo ponto de $M$ possui uma
vizinhança onde $f_n$ converge uniformemente para $f$, i.e., se
dado $x\in M$ então existe $V\subset M$ aberto com $x\in V$ e
$f_n\vert_V\limu f\vert_V$;
\item{\rm($\bullet$)} $(f_n)_{n\ge1}$ converge {\rm uniformemente
sobre compactos} para $f$ se para todo subespaço compacto
$K\subset M$ temos $f_n\vert_K\limu f\vert_K$.

É óbvio que tanto convergência local uniforme como convergência
uniforme sobre compactos implicam convergência pontual. É óbvio
também que convergência uniforme implica tanto convergência
local-uniforme como convergência uniforme sobre compactos. Para
seqüências de funções definidas em abertos de $\R^m$ é fácil ver
que {\sl convergência local-uniforme é equivalente à convergência
uniforme sobre compactos}! (veja os Exercícios~9, 10(a--d) e 11).

\smallskip

\proclaim Teorema. Sejam $U\subset\R^m$ um aberto, $(f_k)_{k\ge1}$
uma seqüência de funções diferenciáveis $f_k:U\to\R^n$ que
converge pontualmente para uma função $f:U\to\R^n$. Suponha que
$(\dd f_k)_{k\ge1}$ converge local-uniformemente (ou uniformemente
sobre compactos, tanto faz) para uma função
$g:U\to\Lin(\R^m,\R^n)$. Então $(f_k)_{k\ge1}$ converge
local-uniformemente (ou uniformemente sobre compactos) para $f$,
$f$ é diferenciável e $\dd f=g$.

\Prova Dado $x_0\in U$, seja $V=\Bola(x_0;r)$ uma bola aberta
contida em $U$ e tal que $(\dd f_k)_{k\ge1}$ converge uniformemente para $g$
em $V$. Aplicando o teorema anterior para as funções
$f_k\vert_V$ e para $g\vert_V$, concluímos que $f_k\vert_V\limu
f\vert_V$, $f\vert_V$ é diferenciável e que
$\dd\big(f\vert_V\big)=g\vert_V$. Como $x_0\in U$ é arbitrário, a
conclusão segue.\fimprova

\eject

\item{(3)} {\bf Funções real-analíticas}.

\smallskip

Este assunto não será explorado a fundo no curso. Faremos apenas
uma rápida exposição para cultura geral. Vamos começar recordando
alguns fatos simples sobre séries.

\smallskip

\proclaim Definição. Seja $(x_k)_{k\ge1}$ uma seqüência em $\R^n$.
Dizemos que a {\rm série} $\sum_{k=1}^{+\infty}x_k$ {\rm converge}
para um vetor $x\in\R^n$ se a seqüência $(s_r)_{r\ge1}$ em $\R^n$
definida por $s_r=\sum_{k=1}^rx_k$ converge para $x$ (dizemos que
$(s_r)_{r\ge1}$ é a {\rm seqüência das somas parciais} da série
$\sum_{k=1}^{+\infty}x_k$). Dizemos que a série
$\sum_{k=1}^{+\infty}x_k$ é {\rm normalmente convergente} quando a
série $\sum_{k=1}^{+\infty}\Vert x_k\Vert$ de números reais é
convergente (no caso $n=1$ é mais usual o termo {\rm absolutamente
convergente} em vez de normalmente convergente). Dizemos que a
série $\sum_{k=1}^{+\infty}x_k$ é {\rm comutativamente
convergente} em $\R^n$ se essa série converge para o mesmo vetor
$x\in\R^n$ ``independentemente da ordem em que os termos são
somados'', i.e., se para toda bijeção $\phi:\N\to\N$ temos que
$\sum_{k=1}^{+\infty}x_{\phi(k)}=x$.

\smallskip

\proclaim Definição. Sejam $X$ um conjunto e $(f_k)_{k\ge1}$ uma
seqüência de funções $f_k:X\to\R^n$. Dizemos que a {\rm série de
funções} $\sum_{k=1}^{+\infty}f_k$ {\rm converge pontualmente}
para uma função $f:X\to\R^n$ se a seqüência $(s_r)_{r\ge1}$
formada pelas funções $s_r:X\to\R^n$ definidas por
$s_r=\sum_{k=1}^rf_k$ converge pontualmente para a função $f$.
Dizemos que a série $\sum_{k=1}^{+\infty}f_k$ {\rm converge
uniformemente} para $f$ se a seqüência $(s_r)_{r\ge1}$ converge
uniformemente para $f$ (a seqüência $(s_r)_{r\ge1}$ é chamada a
{\rm seqüência das somas parciais} da série
$\sum_{k=1}^{+\infty}f_k$).

Para séries de funções $\sum_{k=1}^{+\infty}f_k$ (cada
$f_k:X\to\R^n$) é comum também usar os termos ``normalmente
convergente'' e ``comutativamente convergente'' significando
respectivamente que para todo $x\in X$ a série de vetores
$\sum_{k=1}^{+\infty}f_k(x)$ é normalmente convergente ou
comutativamente convergente.

\smallskip

Seja $f:U\subset\R^m\to\R^n$ uma função de classe $C^\infty$, onde
$U\subset\R^m$ é um aberto. A {\sl série de Taylor\/} de $f$ em torno de um ponto
$x\in U$ é a série (na variável $h\in\R^m$):
$$\sum_{\vert\lambda\vert=0}^{+\infty}{1\over\lambda!}{\partial^{\vert\lambda\vert}f\over\partial
x^\lambda}(x)h^\lambda;$$
o índice $\lambda$ usado na somatória acima percorre todos os multi-índices
$m$-dimensionais, i.e., todas as $m$-uplas $(\lambda_1,\ldots,\lambda_m)$ de
inteiros não negativos. A soma acima não é uma {\sl série\/} no sentido
estrito do termo, pois seus termos {\sl não estão indexados nos números
naturais}; observe porém, que o conjunto de todos os multi-índices $\lambda$ é
{\sl enumerável\/} e escolhendo uma enumeração específica para os $\lambda$'s
(i.e., uma bijeção entre $\N$ e o conjunto dos multi-índices $m$-dimensionais)
obtemos uma série no sentido estrito. Quando dissermos que a série de Taylor
converge, estaremos significando que ela ``converge comutativamente'', i.e.,
que o resultado da soma não depende de como os $\lambda$'s foram
enumerados. Essa não é uma hipótese dura, pois sabe-se que séries normalmente
convergentes são de fato comutativamente convergentes (veja o Exercício 18 ---
veja também o Exercício 26).

Observe que o polinômio de Taylor de ordem $k$ de $f$ em torno de $x$ é
exatamente a porção da série de Taylor correspondente a $\vert\lambda\vert\le
k$. Embora o resto do $k$-ésimo polinômio de Taylor seja ``pequeno'' em vários
sentidos, {\sl não é verdade em geral que a série de Taylor de uma função
$C^\infty$ seja convergente}! É possível também que a série de Taylor de
$f$ seja convergente, mas {\sl não para $f$}! Exemplos desse fenômeno serão
vistos mais adiante no curso.

\smallskip

\proclaim Definição. Uma função $f:U\subset\R^m\to\R^n$
($U\subset\R^m$ aberto) é chamada {\rm real-analítica} quando for
de classe $C^\infty$ e para todo $x\in U$ tivermos:
$$f(x+h)=\sum_{\vert\lambda\vert=0}^{+\infty}{1\over\lambda!}{\partial^{\vert\lambda\vert}f\over\partial
x^\lambda}(x)h^\lambda,$$ para todo $h$ em alguma vizinhança da
origem em $\R^m$ (a série acima deve {\rm convergir comutativamente}). De
outro modo, $f$ é real-analítica quando for igual à soma de sua série de
Taylor em torno de cada ponto (numa vizinhança suficientemente pequena desse ponto).

\smallskip

\noindent{\tt Observação}. É fácil ver que funções polinomiais são
real-analíticas, já que suas respectivas séries de Taylor são finitas (i.e.,
possuem apenas um número finito de termos não nulos). Para algumas funções
elementares (como seno, cosseno, exponencial) é possível mostrar
``manualmente'' a condição de real-analiticidade, fazendo estimativas sobre o
resto do polinômio de Taylor. Na verdade, é possível mostrar que {\sl
muitas\/} funções são real-analíticas, no seguinte sentido: a soma, produto e
a composição de funções real-analíticas ainda é real-analítica. Em particular,
toda ``fórmula'' que não envolva o cálculo de raízes $n$-ésimas em torno de
zero é real-analítica (na verdade, vale muito mais: as soluções de muitos tipos
importantes de equações diferenciais são real-analíticas, se os coeficientes
da equação original o forem). A demonstração ``manual'' desses resultados
(usando séries) é bastante dolorosa: a maneira inteligente de trabalhar com
funções real-analíticas é observar a relação que existe entre as mesmas e as
{\sl funções analíticas complexas}. Não entraremos em detalhes neste curso.

\smallskip

O seguinte teorema da uma idéia de que na verdade existem ``poucas'' funções
real-analíticas:
\proclaim Teorema. Seja $U\subset\R^m$ um aberto conexo e sejam
$f,g:U\to\R^n$ funções real-analíticas. Se $f$ e $g$ coincidem em
algum subconjunto aberto não vazio de $U$ então $f=g$.

\Prova Seja $A\subset U$ o conjunto dos pontos onde $f$ e $g$
coincidem juntamente com todas as suas diferenciais, i.e.:
$$A=\big\{x\in U:f(x)=g(x),\ \dd^{(k)}f(x)=\dd^{(k)}g(x),\
k=1,2,\ldots\big\}.$$ Como o conjunto dos pontos onde duas funções
contínuas são iguais é fechado e como a interseção de conjuntos
fechados é fechada, segue que $A$ é fechado em $U$. Além do mais,
se $x\in A$ então as séries de Taylor de $f$ e $g$ em torno de $x$
são iguais termo a termo e portanto a condição de
real-analiticidade implica que $f(x+h)=g(x+h)$ para $\Vert
h\Vert<r$ e algum $r>0$ suficientemente pequeno. Daí a bola
$\Bola(x;r)$ está contida em $A$, o que mostra que $A$ é aberto em
$\R^m$ (e em $U$). Como $f$ e $g$ são iguais em algum aberto não
vazio, segue que $A$ é não vazio e, como $U$ é conexo,
$A=U$.\fimprova

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Seqüências de funções}.

\medskip

\item{1.} Sejam $X$ um conjunto, $M$ um espaço métrico, $S\subset
X$ um subconjunto e $(f_n)_{n\ge1}$ uma seqüência de funções
$f_n:X\to M$. Dada uma função $f:X\to M$, mostre que:
\smallskip
\itemitem{(a)} se $f_n\limpp f$ então $f_n\vert_S\limpp f\vert_S$;
\itemitem{(b)} se $f_n\limu f$ então $f_n\vert_S\limu f\vert_S$.

\smallskip

\item{2.} Sejam $X$ um conjunto, $M$ um espaço métrico e
$(f_n)_{n\ge1}$ uma seqüência de funções. Dada uma função $f:X\to
M$, mostre que:
\smallskip
\itemitem{(a)} dada uma família $(X_i)_{i\in I}$ de subconjuntos
de $X$ com $X=\bigcup_{i\in I}X_i$, se para todo $i\in I$,
$f_n\vert_{X_i}\limpp f\vert_{X_i}$ então $f_n\limpp f$;
\itemitem{(b)} dada uma família {\sl finita\/} $(X_i)_{i=1}^n$ de
subconjuntos de $X$ com $X=\bigcup_{i=1}^nX_i$, se
$f_n\vert_{X_i}\limu f\vert_{X_i}$ para todo $i=1,\ldots,n$ então
$f_n\limu f$;
\itemitem{(c)} dê um contra-exemplo para o item (b) no caso que a
família de subconjuntos $X_i$ de $X$ não é finita.

\smallskip

\item{3.} Sejam $X$ um conjunto, $(M,d)$ um espaço métrico e $(f_n)_{n\ge1}$ uma
seqüência de funções $f_n:X\to M$. Dada uma função $f:X\to M$, mostre que as
seguintes afirmações são equivalentes:
\smallskip
\itemitem{$\bullet$} $f_n\limu f$;
\itemitem{$\bullet$} $s_n=\sup_{x\in X}d\big(f_n(x),f(x)\big)$ é finito para
$n$ suficientemente grande e a seqüência $(s_n)_{n\ge1}$ converge para zero em
$\R$.

\smallskip

\item{4.} Sejam $X$ um conjunto, $(M,d)$ um espaço métrico e $(f_n)_{n\ge1}$ uma
seqüência de funções $f_n:X\to M$ que converge uniformemente para uma função
$f:X\to M$. Mostre que se $f$ é limitada então $f_n$ é limitada para todo $n$
suficientemente grande. Mostre que essa limitação é {\sl uniforme para $n$ grande},
i.e., existem $n_0\in\N$ e $K>0$ de modo que $d\big(f_n(x),f_n(y)\big)\le K$
para todos $x,y\in X$ e todo $n\ge n_0$.

\smallskip

\item{5.} Sejam $X$ um conjunto, $M$ um espaço métrico e
$(f_n)_{n\ge1}$ uma seqüência de funções $f_n:X\to M$ que converge
uniformemente para uma função $f:X\to M$. Mostre que
$(f_n)_{n\ge1}$ é uniformemente de Cauchy.

\smallskip

\item{6.} Sejam $X$ um conjunto, $(M,d)$ um espaço métrico e $(f_n)_{n\ge1}$
uma seqüência de funções $f_n:X\to M$. Dada uma função $f:X\to M$, mostre que:
\smallskip
\itemitem{(a)} se $f_n\limpp f$ e se $(f_n)_{n\ge1}$ é {\sl uniformemente
limitada}, i.e., se existe $K>0$ tal que
$d\big(f_n(x),f_n(y)\big)\le K$
para todos $x,y\in X$ e
todo $n\in\N$, então $f$ é limitada; na verdade, vale que
$d\big(f(x),f(y)\big)\le K$
para todos $x,y\in X$ ({\sl dica}: a função $d:M\times M\to\R$ é contínua ---
veja o Exercício~3 da Aula número~7 -- 27/03);
\itemitem{(b)} se $(f_n)_{n\ge1}$ é uniformemente de Cauchy e cada $f_n$ é
limitada então $(f_n)_{n\ge1}$ é uniformemente limitada.
\itemitem{(c)} se $(f_n)_{n\ge1}$ converge uniformemente para uma função
$f:X\to M$ e se cada $f_n$ é limitada então $f$ é limitada.

\goodbreak
\smallskip

\item{7.} Sejam $X$ um conjunto, $M$, $N$ espaços métricos,
$\phi:M\to N$ uma função uniformemente contínua e $(f_n)_{n\ge1}$
uma seqüência de funções $f_n:X\to M$. Mostre que:
\smallskip
\itemitem{(a)} se $(f_n)_{n\ge1}$ é uniformemente de Cauchy então
$(\phi\circ f_n)_{n\ge1}$ é uniformemente de Cauchy;
\itemitem{(b)} se $(f_n)_{n\ge1}$ converge uniformemente para
alguma função $f:X\to M$ então a seqüência $(\phi\circ
f_n)_{n\ge1}$ converge uniformemente para $\phi\circ f$;
\itemitem{(c)} conclua do item (a) que se $d$ e $d'$ são métricas
uniformemente equivalentes em $M$ então uma seqüência de funções
$f_n:X\to M$ é uniformemente de Cauchy com respeito a $d$ se e
somente se o for com respeito a $d'$;
\itemitem{(d)} conclua do item (b) que se $d$ e $d'$ são métricas
uniformemente equivalentes em $M$ então uma seqüência de funções
$f_n:X\to M$ converge uniformemente para $f:X\to M$ com respeito a
$d$ se e somente $(f_n)_{n\ge1}$ converge uniformemente para $f$
com respeito a $d'$.

\smallskip

\item{8.} Sejam $X$ um conjunto e $(M_i,d_i)$, $i=1,\ldots,n$
espaços métricos. Considere o conjunto $M=\prod_{i=1}^nM_i$ munido
da métrica $d(x,y)=\sum_{i=1}^nd_i(x_i,y_i)$ (ou de qualquer outra
métrica usual para o produto que seja uniformemente equivalente a
$d$ --- recorde os Exercícios~18 e 33 da aula número 5 (20/03) e o
Exercício~34 da aula número 7 (27/03)). Dada uma seqüência de
funções $(f_k)_{k\ge1}$, $f_k:X\to M$, escreva
$f_k=(f_k^1,\ldots,f_k^n)$ com $f_k^i:X\to M_i$. Mostre que:
\smallskip
\itemitem{(a)} $(f_k)_{k\ge1}$ é uniformemente de Cauchy se e
somente se $(f_k^i)_{k\ge1}$ é uniformemente de Cauchy para todo
$i=1,\ldots,n$;
\itemitem{(b)} $(f_k)_{k\ge1}$ converge uniformemente para uma
função $f=(f^1,\ldots,f^n):X\to M$ se e somente se
$(f_k^i)_{k\ge1}$ converge uniformemente para $f^i$ para todo
$i=1,\ldots,n$.

\smallskip

\item{9.} Sejam $M$, $N$ espaços métricos e $(f_n)_{n\ge1}$ uma
seqüência de funções $f_n:M\to N$. Mostre que se $(f_n)_{n\ge1}$
converge local uniformemente para uma função $f:M\to N$ então
$(f_n)_{n\ge1}$ converge para $f$ uniformemente sobre compactos
({\sl dica}: use o item (b) do Exercício~2 acima).

\smallskip

\item{10.} Um espaço métrico $M$ é dito {\sl localmente compacto\/}
quando todo ponto de $M$ possui uma vizinhança compacta. Mostre
que:
\smallskip
\itemitem{(a)} $M$ é localmente compacto se e somente se todo
ponto de $M$ pertence a um aberto cujo fecho é compacto;
\itemitem{(b)} se $M$ é localmente compacto então todo aberto
$U\subset M$ é localmente compacto;
\itemitem{(c)} se $M$ é localmente compacto então todo fechado
$F\subset M$ é localmente compacto;
\itemitem{(d)} $\R^n$ é localmente compacto;
\itemitem{(e)} se $M$ é localmente compacto, $U\subset M$ é aberto
e $F\subset M$ é fechado então $F\cap U$ é localmente compacto
({\sl dica}: use os itens (b) e (c));
\itemitem{(f)} se $S\subset M$ é um subespaço denso e localmente
compacto então $S$ é aberto em $M$ ({\sl dica}: dado $x\in S$, use
o item (a) para concluir que existe uma vizinhança aberta $U$ de
$x$ em $M$ tal que $\overline{U\cap S}\cap S$ é compacto ---
mostre que $U\subset S$);
\itemitem{(g)} se $S\subset M$ é um
subespaço localmente compacto então $S$ é a interseção de um
aberto $U$ de $M$ com um fechado $F$ de $M$ ({\sl dica}:
$F=\overline S$; use o item (f)).

\goodbreak
\smallskip

\item{11.} Se $M$ é um espaço métrico localmente compacto, $N$ é
um espaço métrico qualquer e $(f_n)_{n\ge1}$ é uma seqüência de
funções $f_n:M\to N$, mostre que $(f_n)_{n\ge1}$ converge local
uniformemente para $f:M\to N$ se e somente se $(f_n)_{n\ge1}$
converge uniformemente sobre compactos para $f$.

\medskip

\noindent{\tt Séries}.

\smallskip

Muitos dos exercícios de séries que aparecem aqui não são fáceis! Na verdade,
eles são apenas um incentivo para aqueles que querem recordar a teoria das
séries (um livro legal para o assunto é o Curso de Análise vol.\ I do Elon).

\smallskip

\item{12.} Mostre que se $\sum_{k=1}^{+\infty}x_k$ é uma série
convergente em $\R^n$ então a seqüência $(x_k)_{k\ge1}$ converge
para zero ({\sl dica}: se $s_r=\sum_{k=1}^rx_k$ denota a $r$-ésima
soma parcial da série então $x_k=s_k-s_{k-1}$).

\smallskip

\item{13.} Mostre que se $\sum_{k=1}^{+\infty}x_k$ é uma série
convergente em $\R^n$ então para todo $r\ge1$ a série
$\sum_{k=r}^{+\infty}x_k$ é convergente e, denotando sua soma por
$S_r$, temos que a seqüência $(S_r)_{r\ge1}$ converge para zero
({\sl dica}: $S_r$ é a diferença entre a $(r-1)$-ésima soma
parcial de $\sum_{k=1}^{+\infty}x_k$ e a soma total da série
$\sum_{k=1}^{+\infty}x_k$).

\smallskip

\item{14.} Mostre que toda série normalmente convergente $\sum_{k=1}^{+\infty}x_k$ em $\R^n$
é convergente ({\sl dica}: se $s_r=\sum_{k=1}^rx_k$ denota a
$r$-ésima soma parcial da série, mostre que $\Vert
s_r-s_u\Vert\le\sum_{k=u+1}^r\Vert x_k\Vert$ para $u<r$ e conclua
usando o Exercício~13 que $(s_r)_{r\ge1}$ é uma seqüência de
Cauchy).

\smallskip

\item{15.} Mostre que a afirmação ``a série
$\sum_{k=1}^{+\infty}x_k$ é normalmente convergente em $\R^n$''
não depende da norma usada em $\R^n$.

\smallskip

\item{16.} ({\sl desigualdade triangular para séries}) Mostre que se
$\sum_{k=1}^{+\infty}x_k$ é uma série normalmente convergente em $\R^n$ então
vale a desigualdade:
$$\left\Vert\sum_{k=1}^{+\infty}x_k\right\Vert\le\sum_{k=1}^{+\infty}\Vert x_k\Vert.$$

\smallskip

\item{17.} Considere uma série $\sum_{k=1}^{+\infty}x_k$ em
$\R^n$ e denote por $x_k^i$ a $i$-ésima coordenada do vetor
$x_k\in\R^n$. Mostre que:
\smallskip
\itemitem{(a)} $\sum_{k=1}^{+\infty}x_k$ converge em $\R^n$ se e
somente se $\sum_{k=1}^{+\infty}x_k^i$ converge em $\R$ para todo
$i=1,\ldots,n$;
\itemitem{(b)} $\sum_{k=1}^{+\infty}x_k$ é normalmente convergente
em $\R^n$ se e somente se $\sum_{k=1}^{\infty}x_k^i$ é
absolutamente convergente em $\R$ para todo $i=1,\ldots,n$.

\smallskip

\item{18.} Mostre que se uma série $\sum_{k=1}^{+\infty}x_k$ é
normalmente convergente em $\R^n$ então essa série também é
comutativamente convergente ({\sl dica}: se $\phi:\N\to\N$ é uma
bijeção então, dado $u\in\N$, temos que para $r\in\N$
suficientemente grande a soma parcial $\sum_{k=1}^rx_{\phi(k)}$
inclui ao menos os $u$ primeiros termos de
$\sum_{k=1}^{+\infty}x_k$ --- agora use o Exercício~13 para
concluir que $\sum_{k=u+1}^{+\infty}\Vert x_k\Vert$ tende a zero
quando $u\to+\infty$).

\smallskip

\noindent{\tt Observação}. Na verdade, vale a recíproca do
resultado enunciado no Exercício~18, i.e., toda série
comutativamente convergente em $\R^n$ também é normalmente
convergente. Esse fato é mostrado da seguinte maneira: primeiro
reduz-se ao caso $n=1$ com um argumento do tipo ``coordenada por
coordenada''. Para $n=1$ é possível provar o seguinte fato mais
geral: se uma série é convergente em $\R$ mas não é absolutamente
convergente (diz-se então que a série é {\sl condicionalmente
convergente}) então é possível reordenar os termos dessa série de
modo a obter {\sl qualquer número real como resultado da soma da
série}. A idéia básica da prova desse fato é usar a parte positiva
e a parte negativa da série de maneira balanceada, de modo a fazer
com que as somas parciais oscilem em torno do resultado desejado.
Uma prova completa desse fato interessante pode ser encontrada por
exemplo no {\sl Curso de Análise, vol.\ I\/} de {\sl Elon Lages
Lima}, pg.\ 120.

\smallskip

\item{19.} Mostre que uma série $\sum_{k=1}^{+\infty}x_k$ de
números reais não negativos é convergente se e somente se a
seqüência $s_r=\sum_{k=1}^rx_k$ de suas somas parciais é limitada
--- em caso afirmativo, mostre que
$\sum_{k=1}^{+\infty}=\sup_{r\ge1}s_r$.

\smallskip

\item{20.} Sejam $\sum_{k=1}^{+\infty}x_k$,
$\sum_{k=1}^{+\infty}y_k$ duas séries convergentes de números
reais não negativos. Mostre que a ``série''
$\sum_{k,l=1}^{+\infty}x_ky_l$ converge para o produto
$\left(\sum_{k=1}^{+\infty}x_k\right)\left(\sum_{l=1}^{+\infty}y_l\right)$,
onde para somar os produtos $x_ky_l$ podemos escolher {\sl
qualquer\/} enumeração dos pares $(k,l)\in\N\times\N$ ({\sl dica}:
é só mostrar que toda soma parcial de
$\sum_{k,l=1}^{+\infty}x_ky_l$ é menor ou igual a algum produto da
forma $\left(\sum_{k=1}^rx_k\right)\left(\sum_{l=1}^ry_l\right)$ e
que, reciprocamente, qualquer produto dessa forma é menor ou igual
a alguma soma parcial de $\sum_{k,l=1}^{+\infty}x_ky_l$).

\smallskip

\item{21.} Generalize o Exercício~20 para o caso de séries
absolutamente convergentes, i.e., obtenha a conclusão do
Exercício~20 supondo apenas que $\sum_{k=1}^{+\infty}x_k$ e
$\sum_{k=1}^{+\infty}y_k$ sejam séries absolutamente convergentes
de números reais ({\sl dica}: usando o Exercício~20, você já sabe
que $\sum_{k,l=1}^{+\infty}x_ky_l$ é absolutamente convergente e
portanto comutativamente convergente; para obter a conclusão faça
$r\to+\infty$ na identidade:
$$\sum_{k,l=1}^rx_ky_l=\left(\sum_{k=1}^rx_k\right)\left(\sum_{l=1}^ry_l\right).$$
Note que é possível ordenar os termos de
$\sum_{k,l=1}^{+\infty}x_ky_l$ de modo que o lado esquerdo da
igualdade acima seja uma soma parcial de
$\sum_{k,l=1}^{+\infty}x_ky_l$).

\smallskip

\item{22.} ({\sl a série geométrica}) Mostre que se $x\in\R$, $\vert x\vert<1$ então
$\sum_{k=0}^{+\infty}x^k={1\over 1-x}$ ({\sl dica}: quanto vale
$\sum_{k=1}^rx^k$\enspace?). Mostre que a série $\sum_{k=1}^{+\infty}x^k$
é absolutamente convergente.

\smallskip

\item{23.} ({\sl a série geométrica em várias variáveis}) Dado
$x=(x_1,\ldots,x_n)\in\R^n$ com $\vert x_i\vert<1$
para $i=1,\ldots,n$, mostre que:
$${1\over(1-x_1)(1-x_2)\cdots(1-x_n)}=\sum_{\vert\lambda\vert=0}^{+\infty}x^\lambda,$$
onde na série do lado direito $\lambda$ percorre os multi-índices de
dimensão $n$ --- a série
$\sum_{\vert\lambda\vert=0}^{+\infty}x^\lambda$ é absolutamente convergente
({\sl dica: use os Exercícios~21 e 22}).

\goodbreak
\smallskip

\item{24.} ({\sl o teste $M$ de Weierstrass}) Seja $X$ um conjunto
e seja $\sum_{k=1}^{+\infty}f_k$ uma série de funções $f_k:X\to\R^n$.
Suponha que existem números reais não negativos $M_k$ tais que
$\Vert f_k(x)\Vert\le M_k$ para todo $x\in X$ e todo $k\in\N$;
suponha também que $\sum_{k=1}^{+\infty}M_k$ converge em $\R$.
Mostre que $\sum_{k=1}^{+\infty}f_k$ é uniformemente convergente
para uma função $f:X\to\R^n$ ({\sl dica}: se $s_r=\sum_{k=1}^rf_k$
denota a $r$-ésima soma parcial de $\sum_{k=1}^{+\infty}f_k$,
mostre que $\big\Vert s_r(x)-s_u(x)\big\Vert\le\sum_{k=u+1}^rM_k$
para $u<r$ e todo $x\in X$; conclua usando o Exercício~13 que
$(s_r)_{r\ge1}$ é uma seqüência uniformemente de Cauchy).

\smallskip

\item{25.} Sejam $X$ um conjunto e $\sum_{k=1}^{+\infty}f_k$ uma
série uniformemente convergente de funções $f_k:X\to\R^n$. Mostre
que $f_k\limu0$ ({\sl dica}: se $s_r=\sum_{k=1}^rf_k$ denota a
$r$-ésima soma parcial da série então por definição existe
$s:X\to\R^n$ com $s_r\limu s$; observe que $f_k=s_k-s_{k-1}$).

\medskip

\noindent{\tt Funções real-analíticas}.

\smallskip

Como já foi mencionado, não nos aprofundaremos na teoria de funções
real-analíticas neste curso. Os exercícios a seguir são apenas uma curiosidade.

\smallskip

\item{26.} Seja dado $m\in\N$. Para cada multi-índice
$m$-dimensional $\lambda$, seja $a_\lambda\in\R^n$ um vetor fixado.
Para cada $k\ge0$, denote por $p_k:\R^m\to\R^n$ a função
polinomial homogênea $p_k(x)=\sum_{\vert\lambda\vert=k}a_\lambda
x^\lambda$ e por $B_k\in\Multlin(\R^m;\R^n)$ o único operador
$k$-linear simétrico com $B_k(x)^{(k)}=p_k(x)$ para todo
$x\in\R^m$. Suponha que existe uma vizinhança $U\subset\R^m$ da
origem tal que a série de potências
$\sum_{\vert\lambda\vert=0}^{+\infty}a_\lambda x^\lambda$ converge
para todo $x\in U$ {\sl em alguma ordem}. Mostre que:
\smallskip
\itemitem{(a)} existem $M>0$, $R>0$ de modo que $\Vert
a_\lambda\Vert\le{M\over R^{\vert\lambda\vert}}$ para todo
multi-índice $\lambda$ ({\sl dica}: tome $R>0$ tal que o vetor
$x=(\underbrace{R,\ldots,R}_{m\ {\rm vezes}})$ pertence a $U$;
observe que a família $a_\lambda x^\lambda=a_\lambda
R^{\vert\lambda\vert}$ em $\R^n$ é limitada).
\itemitem{(b)} se $R$ é definido como no item (a) e $0<R'<R$,
mostre que a série $\sum_{\vert\lambda\vert=0}^{+\infty}a_\lambda
x^\lambda$ converge normalmente e uniformemente no bloco
retangular $$[-R',R']^m=\{x\in\R^m:\Vert x\Vert_\infty\le
R'\big\}$$ ({\sl dica}: use a majoração para $\Vert
a_\lambda\Vert$ obtida no item (a), o teste $M$ de Weierstrass e o
Exercício~23).

\smallskip

\item{27.} ({\sl unicidade da série de Taylor}) Seja $f:U\subset\R^m\to\R^n$
uma função definida num
aberto $U\subset\R^m$ e seja $x\in U$ fixado. Suponha que
$f(x+h)=\sum_{\vert\lambda\vert=0}^{+\infty}a_\lambda h^\lambda$
para todo $h\in\R^m$ numa vizinhança da origem, para alguma
família de vetores $a_\lambda\in\R^n$, onde $\lambda$ percorre o conjunto dos
multi-índices de dimensão $m$. Mostre que $f$ é de classe $C^\infty$
numa vizinhança aberta de $x$ em $U$ e que
$a_\lambda={1\over\lambda!}{\partial^{\vert\lambda\vert}f\over
\partial x^\lambda}(x)$ para todo $\lambda$ ({\sl dica}: use o
Exercício~26 para justificar o fato que uma série de potências
pode ser diferenciada termo a termo --- calcule
${\partial^{\vert\lambda\vert}f\over\partial x^\lambda}(x)$
diferenciando a série de potências
$\sum_{\vert\lambda\vert=0}^{+\infty}a_\lambda h^\lambda$ com
respeito a $h$ no ponto $h=0$ da maneira adequada).

\vfil\eject

\centerline{\bf Aula número $16$ (08/05)}
\bigskip
\bigskip

A aula começa cobrindo os tópicos: ``seqüências de funções
uniformemente de Cauchy'', ``diferenciação termo a termo'' e
``funções real-analíticas'', originalmente destinados à aula número
15.

\medskip

\item{(1)} {\bf Uma função de classe ${\bf C^\infty}$ que não é real-analítica}.

\smallskip

Vamos nesta seção estudar um exemplo muito interessante: uma
função $\xi:\R\to\R$ de classe $C^\infty$ que se anula em
$\left(-\infty,0\right]$ e que não é identicamente nula. Em
particular, esta função não pode ser real-analítica (pois duas
funções real-analíticas num aberto conexo que coincidem num aberto não vazio são
iguais). A existência dessa função não é apenas uma curiosidade:
ela será muito importante quando estudarmos o conceito de {\sl
partição da unidade}.

Para mostrar que a função $\xi$ que definiremos adiante é mesmo de
classe $C^\infty$, precisaremos de um teorema que apresentamos a
seguir. Esse teorema é na verdade interessante em si mesmo,
independentemente do seu uso nesta seção --- também a técnica usada na
sua demonstração é curiosa.

\smallskip

\proclaim Teorema. Sejam $U\subset\R^m$ um aberto, $x\in U$ um
ponto, $f:U\to\R^n$ uma função contínua, diferenciável no aberto
$U\setminus\{x\}$. Se o limite $\lim_{y\to x}\dd f(y)$ existe
(igual a $T\in\Lin(\R^m,\R^n)$, digamos) então $f$ é diferenciável
no ponto $x$ e $\dd f(x)=T$.

\Prova Para $h\in\R^m$ suficientemente próximo da origem, temos
que o segmento $[x,x+h]$ está contido em $U$ e portanto podemos
aplicar a desigualdade do valor médio para a função $f-T$ em tal
segmento obtendo:
$$\big\Vert
f(x+h)-f(x)-T(h)\big\Vert\le\sup_{y\in(x,x+h)}\big\Vert\dd
f(y)-T\big\Vert\,\Vert h\Vert;$$ o uso da desigualdade do valor
médio é justificado pelo fato que $f-T$ é contínua em $[x,x+h]$ e
diferenciável em $(x,x+h)$ (não sabemos ainda que $f-T$ é
diferenciável em $x$, mas isso não é um problema). Definindo então
$r$ pela igualdade $f(x+h)=f(x)+T(h)+r(h)$ então, como $\lim_{y\to
x}\dd f(y)=T$, dado $\varepsilon>0$, podemos encontrar $\delta>0$
tal que $0<\Vert y-x\Vert<\delta$ implica $\big\Vert\dd
f(y)-T\big\Vert<\varepsilon$ e portanto:
$$\left\Vert{r(h)\over\Vert h\Vert}\right\Vert\le\varepsilon,$$
sempre que $\Vert h\Vert<\delta$. Logo
$\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$, o que completa a
demonstração.\fimprova

\smallskip

\noindent{\tt Observação}. Duas coisas são curiosas na
demonstração acima: em primeiro lugar, usamos a desigualdade do
valor médio com toda a sua força, i.e., levamos em conta o fato
que a hipótese da desigualdade do valor médio exige
diferenciabilidade da função {\sl apenas no segmento aberto}. É
interessante também a técnica de aplicar a desigualdade do valor
médio para $f-T$. Em geral, quando $f$ é uma função diferenciável
numa vizinhança de um ponto $x$, pode ser útil aplicar a
desigualdade do valor médio para a função $y\mapsto f(y)-\dd
f(x)\cdot y$ obtendo a estimativa:
$$\big\Vert f(x+h)-f(x)-\dd
f(x)\cdot h\big\Vert\le\sup_{y\in(x,x+h)}\big\Vert\dd f(y)-\dd
f(x)\big\Vert\,\Vert h\Vert,$$ que é mais poderosa que a
estimativa $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$, dada pela
definição de função diferenciável.

\smallskip

\noindent{\tt Observação}. Apesar de termos demonstrado o teorema
acima para funções de várias variáveis, só usaremo-lo para funções
de uma variável.

\smallskip

Definimos agora nossa função $\xi:\R\to\R$ fazendo:
$$\xi(x)=\cases{e^{-{1\over x}},&$x>0$,\cr
0,&$x\le0$.\cr}$$ Obviamente $\xi$ é de classe $C^\infty$ no
aberto $\R\setminus\{0\}$; o problema é mostrar que $\xi$ é
infinitamente diferenciável no ponto $0$. Em primeiro lugar,
observe que $\lim_{x\to0^+}e^{-{1\over x}}=0$, de modo que $\xi$ é
contínua em $0$. Além do mais, $\xi'(x)=0$ para $x<0$ e:
$$\xi'(x)={1\over x^2}\,e^{-{1\over x}},$$
para todo $x>0$. Daí $\lim_{x\to0}\xi'(x)=0$ e pelo teorema
provado acima, temos que $\xi$ é diferenciável em $x=0$ e
$\xi'(0)=0$. Como $\xi'$ é contínua e diferenciável em
$\R\setminus\{0\}$, podemos agora proceder de modo análogo para
mostrar que $\xi$ é duas vezes diferenciável em $x=0$ e que
$\xi''(0)=0$. Para provar que $\xi$ é de classe $C^\infty$, usamos
indução. Suponha que $\xi$ é de classe $C^k$ para algum $k\ge1$ e
que a $k$-ésima derivada $\xi^{(k)}$ de $\xi$ seja da forma:
$$\xi^{(k)}(x)=\cases{{p(x)\over q(x)}\,e^{-{1\over x}},&$x>0$,\cr
\noalign{\medskip}0,&$x\le0$,\cr}$$ onde $p,q:\R\to\R$ são
polinômios. Daí $\xi^{(k)}$ é uma função contínua, diferenciável
em $\R\setminus\{0\}$, $\xi^{(k+1)}(x)=0$ para $x<0$ e:
$$\xi^{(k+1)}(x)={p'(x)q(x)-p(x)q'(x)\over q(x)^2}\,e^{-{1\over
x}}+{p(x)\over x^2q(x)}\,e^{-{1\over x}}={\widetilde
p(x)\over\widetilde q(x)}\,e^{-{1\over x}},\quad x>0,$$ onde
$\widetilde p$ e $\widetilde q$ são polinômios. Por um argumento
elementar de Cálculo I, segue que:
$$\lim_{x\to0}\xi^{(k+1)}(x)=0;$$
pelo teorema acima, $\xi$ é $k+1$ vezes diferenciável em $\R$ e
$\xi^{(k+1)}(0)=0$ (e logo $\xi$ é de classe $C^{k+1}$). Por
indução em $k$, concluímos que $\xi$ é de classe $C^\infty$.

\smallskip

\noindent{\tt Observação}. Note que {\sl todos os coeficientes da
série de Taylor de $\xi$ em torno de $0$ são nulos}, enquanto
$\xi$ não é nula em nenhuma vizinhança de $0$, i.e., a série de
Taylor de $\xi$ em torno de $0$ converge, mas não para o valor de
$\xi$.

\vfil\eject

\centerline{\titfont Curvas}

\bigskip

Antes de continuar nosso estudo de cálculo diferencial, vamos
estudar um pouco de curvas em $\R^n$. Na seção 2 a seguir faremos
uma exposição da noção de comprimento de arco. Em seguida,
relacionaremos o comprimento de arco com a integral da norma do
vetor tangente, no caso de curvas de classe $C^1$. Para isso,
precisaremos fazer uma rápida recordação da definição de integral
de Riemann na seção 3 --- não faremos um desenvolvimento completo
da teoria de integração aqui, já que o mesmo pertence a outra
parte do curso. Vamos apelar para o fato que estudantes neste
estágio já devem ter um conhecimento básico de integral de Riemann
(em uma variável, ao menos).

O próximo passo do curso será o estudo da noção de integral de
linha. Primeiro, usaremos o {\sl approach\/} clássico usando a
linguagem de campos vetoriais. Depois passaremos ao {\sl
approach\/} moderno, usando a linguagem de formas diferenciais.
Terminamos o estudo de curvas com a teoria dos campos
conservativos (usando a linguagem de formas diferenciais, onde a
expressão ``campo conservativo'' será trocada por ``$1$-forma
diferencial exata'').

\bigskip

\item{(2)} {\bf Curvas em ${\bf \R^n}$ --- o comprimento de arco}.

\smallskip

Por uma {\sl curva\/} em $\R^n$ significaremos uma aplicação
arbitrária $\gamma:[a,b]\to\R^n$ definida num intervalo fechado
$[a,b]$ (apesar do fato que na maior parte do tempo estaremos
interessados em curvas que são ao menos contínuas). Por uma {\sl
partição\/} do intervalo $[a,b]$ significamos um subconjunto {\sl
finito\/} $P\subset[a,b]$ que contém os extremos $a$ e $b$.
Tipicamente escreveremos $P=\{t_0,\ldots,t_k\}$, deixando
subentendido que $k\ge1$ e que:
$$a=t_0<t_1<\cdots<t_{k-1}<t_k=b.$$
A {\sl variação\/} de uma curva $\gamma:[a,b]\to\R^n$ com respeito
à partição $P=\{t_0,\ldots,t_k\}$ é o número real não negativo
$\V(\gamma;P)$ definido por:
$$\V(\gamma;P)=\sum_{i=0}^{k-1}\big\Vert\gamma(t_{i+1})-\gamma(t_i)\big\Vert.$$

\smallskip

\proclaim Definição. O {\rm comprimento de arco} (ou {\rm variação
total}) de uma curva $\gamma:[a,b]\to\R^n$ é definido por:
$$\V(\gamma)=\sup_P\V(\gamma;P)\in[0,+\infty],$$
onde $P$ percorre o conjunto de {\rm todas} as possíveis partições
de $[a,b]$. Quando $\V(\gamma)<+\infty$ (i.e., quando existe $M>0$
tal que $\V(\gamma;P)\le M$ para toda partição $P$ de $[a,b]$)
então dizemos que a curva $\gamma$ é {\rm retificável} (ou {\rm de
variação limitada}).

\smallskip

\noindent{\tt Observação}. O nome ``variação total'' (no lugar de
comprimento de arco) e ``de variação limitada'' (no lugar de
retificável) é mais usado quando $n=1$; isso provavelmente se deve
ao fato que no caso $n=1$ a imagem intuitiva que temos de uma
``curva'' não se encaixa bem.

\smallskip

\noindent{\tt Observação}. É mais usual definir o comprimento de
uma curva $\gamma:[a,b]\to\R^n$ apenas usando a norma Euclideana
$\norma$, apesar do fato que para o desenvolvimento da teoria que
faremos, $\norma$ pode em princípio denotar qualquer norma. É
fácil mostrar que a condição de retificabilidade {\sl não
depende\/} da norma escolhida, apesar do fato que o comprimento de
arco depende, obviamente (veja os Exercícios~3 e 4).

\smallskip

Se uma partição $P'$ do intervalo $[a,b]$ contém uma partição $P$
de $[a,b]$ então dizemos que $P'$ {\sl refina\/} (ou que é um {\sl
refinamento}) de $P$. É fácil ver usando a desigualdade triangular
que para toda curva $\gamma:[a,b]\to\R^n$ temos:
$$\V(\gamma;P')\ge\V(\gamma;P);$$
de fato, é suficiente mostrar a desigualdade acima quando $P'$ tem
só um ponto a mais que $P$ (o caso geral segue facilmente por
indução). Se $P=\{t_0,\ldots,t_k\}$ e $P'=P\cup\{t\}$ com
$t_i<t<t_{i+1}$ então a parcela
$\big\Vert\gamma(t_{i+1})-\gamma(t_i)\big\Vert$ em $\V(\gamma;P)$
é substituída pela soma
$\big\Vert\gamma(t_{i+1})-\gamma(t)\big\Vert+\big\Vert\gamma(t)-\gamma(t_i)\big\Vert$
em $\V(\gamma;P')$ e obviamente tal soma é maior ou igual à
parcela original $\big\Vert\gamma(t_{i+1})-\gamma(t_i)\big\Vert$.

\smallskip

A observação acima mostra que para verificar a retificabilidade de
$\gamma$ e para calcular o seu comprimento é suficiente considerar
partições {\sl suficientemente finas}, i.e., se $P_0$ é uma
partição fixada de $[a,b]$ então:
$$\V(\gamma)=\sup_{P\supset P_0}\V(\gamma;P).$$

\smallskip

\noindent{\tt Observação}. {\sl A reta é o caminho mais curto
entre dois pontos}, i.e., se $\gamma:[a,b]\to\R^n$ é uma curva
então $\big\Vert\gamma(b)-\gamma(a)\big\Vert\le\V(\gamma)$. Isso
segue trivialmente da observação que
$\big\Vert\gamma(b)-\gamma(a)\big\Vert=\V(\gamma;P)$, onde
$P=\{a,b\}$.

\smallskip

Vamos provar algumas propriedades fáceis do comprimento de arco.

\proclaim Teorema. Se $\gamma:[a,b]\to\R^n$ é retificável e se
$[c,d]\subset[a,b]$ é um subintervalo então $\gamma\vert_{[c,d]}$
também é retificável e $\V(\gamma\vert_{[c,d]})\le\V(\gamma)$.

\Prova Se $P$ é uma partição de $[c,d]$ então $P'=P\cup\{a,b\}$ é
uma partição de $[a,b]$ e obviamente:
$$\V(\gamma\vert_{[c,d]};P)\le\big\Vert\gamma(c)-\gamma(a)\big\Vert+\V(\gamma\vert_{[c,d]};P)+
\big\Vert\gamma(b)-\gamma(d)\big\Vert=\V(\gamma;P')\le\V(\gamma);$$
a conclusão segue tomando o supremo com respeito a $P$ na
desigualdade $\V(\gamma\vert_{[c,d]};P)\le\V(\gamma)$.\fimprova

\smallskip

\proclaim Teorema. (aditividade por concatenação do comprimento de
arco) Seja $\gamma:[a,b]\to\R^n$ uma curva e $c\in(a,b)$ um ponto
qualquer. Se $\gamma\vert_{[a,c]}$ e $\gamma\vert_{[c,b]}$ são
retificáveis então $\gamma$ também é retificável e:
$$\V(\gamma)=\V(\gamma\vert_{[a,c]})+\V(\gamma\vert_{[c,b]}).$$

\Prova Para calcular o comprimento de $\gamma$ é suficiente usar
partições que contém $c$, ou seja:
$$\V(\gamma)=\sup_{c\in P}\V(\gamma;P);$$
mas toda partição $P$ de $[a,b]$ contendo $c$ é da forma $P=P'\cup
P''$, onde $P'$ e $P''$ são respectivamente partições de $[a,c]$ e
de $[c,b]$. Reciprocamente, se $P'$ é uma partição de $[a,c]$ e
$P''$ é uma partição de $[c,b]$ então $P'\cup P''$ é uma partição
de $[a,b]$ contendo $c$ e:
$$\V(\gamma;P'\cup P'')=\V(\gamma\vert_{[a,c]};P')+\V(\gamma\vert_{[c,b]};P'');$$
tomando supremos em $P'$ e $P''$ na igualdade acima obtemos a
conclusão desejada.\fimprova

\smallskip

Recordamos que uma aplicação $\sigma:[c,d]\to\R$ é dita {\sl
monótona\/} quando for crescente ou decrescente ({\sl crescente},
aqui, significa apenas $x<y\Rightarrow\sigma(x)\le\sigma(y)$,
i.e., crescente {\sl não\/} significa estritamente crescente;
comentário análogo vale para o termo decrescente).

\proclaim Teorema. (invariância do comprimento de arco por
reparametrização) Seja $\gamma:[a,b]\to\R^n$ uma curva e
$\sigma:[c,d]\to[a,b]$ uma aplicação monótona e sobrejetora. Então
$\gamma$ é retificável se e somente se $\gamma\circ\sigma$ o for e
vale a igualdade:
$$\V(\gamma)=\V(\gamma\circ\sigma).$$

\Prova Vamos supor para fixar as idéias que $\sigma$ seja
crescente; o caso em que $\sigma$ é decrescente é análogo. Como
$\sigma$ é crescente e sobrejetora, obviamente vale $\sigma(c)=a$
e $\sigma(d)=b$. Daí, se $P$ é uma partição de $[c,d]$ então
$\sigma(P)$ é uma partição de $[a,b]$ e, reciprocamente (como
$\sigma$ é sobrejetora), toda partição $P'$ de $[a,b]$ é da forma
$\sigma(P)$ para alguma partição $P$ de $[c,d]$. Além do mais, é
fácil ver que se $P$ é uma partição de $[c,d]$ e $P'=\sigma(P)$
então $\V(\gamma\circ\sigma;P)=\V(\gamma;P')$; de fato, escrevendo
$P=\{t_0,\ldots,t_k\}$ com $c=t_0<\cdots<t_k=d$ então
$P'=\{\sigma(t_0),\ldots,\sigma(t_k)\}$ com
$a=\sigma(t_0)\le\cdots\le\sigma(t_k)=b$ e portanto:
$$\V(\gamma\circ\sigma;P)=\sum_{i=0}^{k-1}\big\Vert(\gamma\circ\sigma)(t_{i+1})-(\gamma\circ\sigma)(t_i)\big\Vert
=\sum_{i=0}^{k-1}\big\Vert\gamma\big(\sigma(t_{i+1})\big)-\gamma\big(\sigma(t_i)\big)\big\Vert=\V(\gamma;P');$$
a última igualdade na fórmula acima é de fato correta apesar de
termos possivelmente $\sigma(t_i)=\sigma(t_{i+1})$ para alguns
$i$'s (i.e., na somatória antes da última igualdade acima temos alguns termos nulos
a mais que na definição
padrão de $\V(\gamma;P')$). A conclusão segue então tomando o
supremo em $P$ na igualdade
$\V(\gamma\circ\sigma;P)=\V\big(\gamma;\sigma(P)\big)$.\fimprova

\smallskip

\noindent{\tt Observação}. Se $\sigma:[c,d]\to[a,b]$ é monótona,
contínua e se $\sigma\big(\{c,d\}\big)=\{a,b\}$ então $\sigma$ é
sobrejetora, pelo teorema do valor intermediário, de modo que as
hipóteses do teorema acima são satisfeitas. Na verdade, se
$\sigma:[c,d]\to[a,b]$ é monótona e sobrejetora então $\sigma$ é
automaticamente contínua (veja {\sl Curso de Análise, vol.\ I,
Elon Lages Lima}, pg.\ 182), de modo que não perdemos nada em
supor $\sigma$ contínua desde o começo no teorema acima.

\smallskip

\noindent{\tt Exemplo}. Considere a função $f:[0,1]\to\R$ dada
por:
$$f(t)=\cases{t\cos{1\over t},&$t\in\left(0,1\right]$,\cr
0,&$t=0$.\cr}$$ Temos que $f$ é contínua. Vamos mostrar que $f$
não é de variação limitada. De fato, dado um inteiro $r\ge1$,
considere a partição $P$ de $[0,1]$ definida por:
$$P=\{0,1\}\cup\bigcup_{k=1}^r\left\{{1\over2k\pi},{1\over2k\pi+{\pi\over2}}\right\}.$$
Temos:
$$\V(f;P)\ge\sum_{k=1}^r\left\vert
f\left({1\over2k\pi}\right)-f\left({1\over2k\pi+{\pi\over2}}\right)\right\vert
=\sum_{k=1}^r{1\over2k\pi}.$$ Como a série
$\sum_{k=1}^{+\infty}{1\over2k\pi}$ é divergente, segue que
$\V(f)=\sup_P\V(f;P)=+\infty$ e portanto $f$ não é de variação
limitada.

\smallskip

\proclaim Definição. Uma curva $\gamma:[a,b]\to\R^n$ é dita {\rm
parametrizada por comprimento de arco} quando for retificável e
para todo $t\in\left(a,b\right]$ tivermos
$\V(\gamma\vert_{[a,t]})=t-a$.

Usando a aditividade por concatenação do comprimento de arco, é
fácil ver que uma curva $\gamma:[a,b]\to\R^n$ é parametrizada por
comprimento de arco se e somente se for retificável e para todos
$t_1,t_2\in[a,b]$ com $t_1<t_2$ tivermos
$\V(\gamma\vert_{[t_1,t_2]})=t_2-t_1$. Observe também que se
$\gamma:[a,b]\to\R^n$ é parametrizada por comprimento de arco então vale a
desigualdade:
$$\big\Vert\gamma(t_1)-\gamma(t_2)\big\Vert\le\V(\gamma\vert_{[t_1,t_2]})=t_2-t_1,$$
para todos $t_1,t_2\in[a,b]$ com $t_1<t_2$. Segue em particular que $\gamma$ é
Lipschitziana e portanto {\sl contínua}.

\medskip

\item{(3)} {\bf O comprimento de arco para curvas de classe ${\bf C^1}$ usando integrais}.

\smallskip

Vamos começar com uma rápida recordação da definição de integral
de Riemann. Não faremos um desenvolvimento completo da teoria de
integração básica nessa parte do curso.

Seja $P=\{t_0,\ldots,t_k\}$ uma partição do intervalo $[a,b]$
(como sempre, convencionamos $a=t_0<\cdots<t_k=b$). Uma {\sl
pontilhação\/} para a partição $P$ é um conjunto
$\tau=\{\tau_0,\ldots,\tau_{k-1}\}$, onde $t_i\le\tau_i\le
t_{i+1}$ para $i=0,\ldots,k-1$. O par $(P,\tau)$ será chamado uma
{\sl partição pontilhada\/} de $[a,b]$. Se $f:[a,b]\to\R$ é uma
função então a {\sl soma de Riemann\/} de $f$ com respeito a
$(P,\tau)$ é definida por:
$$S(f;P,\tau)=\sum_{i=0}^{k-1}f(\tau_i)(t_{i+1}-t_i).$$
A {\sl norma\/} $\Vert P\Vert$ da partição $P$ é o maior dos
comprimentos dos intervalos $[t_i,t_{i+1}]$, i.e., $\Vert
P\Vert=\max_{i=0}^{k-1}(t_{i+1}-t_i)$.

\smallskip

\proclaim Definição. Seja $f:[a,b]\to\R$ uma função limitada.
Dizemos que $f$ é {\rm Riemann integrável} quando existe o
``limite'' $\lim_{\Vert P\Vert\to0}S(f;P,\tau)$, i.e., quando existe
um número real $I\in\R$ tal que para todo $\varepsilon>0$, existe
$\delta>0$, de modo que $\big\vert
S(f;P,\tau)-I\big\vert<\varepsilon$, para toda partição $P$ de
$[a,b]$ com $\Vert P\Vert<\delta$ e toda pontilhação
$\tau$ de $P$. O número $I$ (que, como é fácil ver, é
único, quando existe) é chamado a {\rm integral de Riemann} de $f$
e é denotado por:
$$I=\int_a^bf=\int_a^bf(x)\,\dd x.$$

\smallskip

\noindent{\tt Observação}. Apesar de termos considerado apenas
funções a valores reais na definição acima, não haveria mal nenhum
em considerar $f:[a,b]\to\R^n$ (na verdade, quando estudamos o
resto integral da Fórmula de Taylor, usamos a integral de uma
função a valores em $\R^n$). Definindo $\int_a^bf$ para
$f:[a,b]\to\R^n$ de modo idêntico ao feito acima, é fácil ver que
$f=(f^1,\ldots,f^n)$ é Riemann integrável se e somente se cada
$f^i$, $i=1,\ldots,n$, é Riemann integrável; nesse caso, a
$i$-ésima coordenada de $\int_a^bf\in\R^n$ é $\int_a^bf^i$ para
$i=1,\ldots,n$.

\smallskip

\noindent{\tt Observação}. A integral de Riemann só é definida da
maneira acima {\sl para funções limitadas}. A integral de Riemann
para funções ilimitadas (chamada {\sl integral imprópria de
Riemann}) deve ser definida separadamente, como um limite de integrais
próprias. Não teremos nenhum uso
aqui para integrais impróprias de Riemann (uma das {\sl muitas\/} vantagens
da integral de Lebesgue, é a não necessidade de distinguir
integrais próprias de impróprias).

\smallskip

Agora vamos ao resultado central desta seção.

\proclaim Teorema. Seja $\gamma:[a,b]\to\R^n$ uma curva de classe
$C^1$. Então $\gamma$ é retificável e seu comprimento é dado por:
$$\V(\gamma)=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t.$$

\Prova Como $\gamma':[a,b]\to\R^n$ é contínua no compacto $[a,b]$,
existe $M>0$ tal que $\big\Vert\gamma'(t)\big\Vert\le M$ para todo
$t\in[a,b]$. Daí, se $P=\{t_0,\ldots,t_k\}$ é uma partição de
$[a,b]$, a desigualdade do valor médio nos dá:
$$\V(\gamma;P)=\sum_{i=0}^{k-1}\big\Vert\gamma(t_{i+1})-\gamma(t_i)\big\Vert\le\sum_{i=0}^{k-1}M(t_{i+1}-t_i)
=M(b-a).$$ Daí $\gamma$ é retificável e $\V(\gamma)\le M(b-a)$.
Falta mostrar agora que $\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd
t=\V(\gamma)$. Seja dado então $\varepsilon>0$. Como $\gamma$ é
retificável, existe uma partição $P_0=\{t_0,\ldots,t_k\}$ de
$[a,b]$ tal que:
$$\V(\gamma)-{\varepsilon\over3}<\V(\gamma;P_0)\le\V(\gamma).$$
Lembrando que uma função contínua num compacto é uniformemente
contínua, podemos escolher $\delta>0$ satisfazendo as três
seguintes condições:
\smallskip
\item{$\bullet$} $\delta<\min_{i=0}^{k-1}(t_{i+1}-t_i)$;
\item{$\bullet$} para todos $t,s\in[a,b]$, $\vert
t-s\vert<\delta\Rightarrow\big\Vert\gamma(t)-\gamma(s)\big\Vert<{\varepsilon\over9k}$;
\item{$\bullet$} para todos $t,s\in[a,b]$, $\vert
t-s\vert<\delta\Rightarrow\big\Vert\gamma'(t)-\gamma'(s)\big\Vert<{\varepsilon\over3(b-a)}$.
\smallskip
\noindent Seja agora $P=\{s_0,\ldots,s_l\}$ uma partição de
$[a,b]$ com $\Vert P\Vert<\delta$ e seja
$\tau=\{\tau_0,\ldots,\tau_{l-1}\}$ uma pontilhação para $P$. Para
completar a demonstração, vamos mostrar que:
$$\big\vert S(f;P,\tau)-\V(\gamma)\big\vert<\varepsilon,$$
onde $f:[a,b]\to\R$ é definida por
$f(t)=\big\Vert\gamma'(t)\big\Vert$. Dividimos o restante da
demonstração em três partes.

\goodbreak
\smallskip

\item{(i)} $\big\vert
S(f;P,\tau)-\V(\gamma;P)\big\vert\le{\varepsilon\over3}$;

\smallskip

fixado $i\in\{0,\ldots,l-1\}$, aplicamos a desigualdade do valor
médio para a função $t\mapsto\gamma(t)-\gamma'(\tau_i)t$ no
intervalo $[s_i,s_{i+1}]$ obtendo:
$$\eqalign{\big\Vert\gamma(s_{i+1})-\gamma(s_i)-\gamma'(\tau_i)(s_{i+1}-s_i)\big\Vert&\le\sup_{t\in(s_i,s_{i+1})}
\big\Vert\gamma'(t)-\gamma'(\tau_i)\big\Vert\,(s_{i+1}-s_i)\cr
&\le{\varepsilon\over 3(b-a)}(s_{i+1}-s_i),\cr}$$ já que $\vert
t-\tau_i\vert<\delta$ para todo $t\in(s_i,s_{i+1})$. Concluímos
então que:
$$\eqalign{&\big\vert\V(\gamma;P)-S(f;P,\tau)\big\vert\le\sum_{i=0}^{l-1}\Big\vert\big\Vert\gamma(s_{i+1})-\gamma(s_i)
\big\Vert-\big\Vert\gamma'(\tau_i)\big\Vert(s_{i+1}-s_i)\Big\vert\cr
&\qquad\le\sum_{i=0}^{l-1}\big\Vert\gamma(s_{i+1})-\gamma(s_i)-\gamma'(\tau_i)(s_{i+1}-s_i)\big\Vert
\le{\varepsilon\over3(b-a)}\sum_{i=0}^{l-1}(s_{i+1}-s_i)={\varepsilon\over3}.\cr}$$
Isso completa a demonstração da parte (i).

\goodbreak
\smallskip

\item{(ii)} $\big\vert\V(\gamma;P)-\V(\gamma;P\cup
P_0)\big\vert<{\varepsilon\over3}$;

\smallskip

como $\Vert P\Vert<\delta<\min_{i=0}^{k-1}(t_{i+1}-t_i)$, segue que cada
intervalo da partição $P$ contém no máximo um ponto da partição
$P_0$ em seu interior. Definimos então:
$$\eqalign{%
I&=\Big\{i\in\{0,\ldots,l-1\}:(s_i,s_{i+1})\cap
P_0=\emptyset\Big\},\cr
I'&=\Big\{i\in\{0,\ldots,l-1\}:(s_i,s_{i+1})\cap
P_0\ne\emptyset\Big\},\cr}$$ e para cada $i\in I'$ denotamos por
$t_{\rho(i)}$ ($\rho(i)\in\{1,\ldots,k-1\}$) o único ponto da
partição $P_0$ que pertence ao intervalo aberto $(s_i,s_{i+1})$.
Note que $I'$ tem no máximo $k-1$ elementos. Temos:
$$\displaylines{%
\V(\gamma;P\cup P_0)=\sum_{i\in
I}\big\Vert\gamma(s_{i+1})-\gamma(s_i)\big\Vert+\sum_{i\in
I'}\big\Vert\gamma(s_{i+1})-\gamma(t_{\rho(i)})\big\Vert+\big\Vert\gamma(t_{\rho(i)})-\gamma(s_i)\big\Vert,\cr
\V(\gamma;P)=\sum_{i\in
I}\big\Vert\gamma(s_{i+1})-\gamma(s_i)\big\Vert+\sum_{i\in
I'}\big\Vert\gamma(s_{i+1})-\gamma(s_i)\big\Vert,\cr}$$ e
portanto:
$$\V(\gamma;P\cup P_0)-\V(\gamma;P)=\sum_{i\in I'}\big\Vert\gamma(s_{i+1})-\gamma(t_{\rho(i)})\big\Vert+\big\Vert\gamma(t_{\rho(i)})-\gamma(s_i)\big\Vert
-\big\Vert\gamma(s_{i+1})-\gamma(s_i)\big\Vert.$$ Como $\vert
s_{i+1}-t_{\rho(i)}\vert<\delta$, $\vert
t_{\rho(i)}-s_i\vert<\delta$ e $\vert s_{i+1}-s_i\vert<\delta$, o
termo geral da somatória acima tem módulo menor que
${\varepsilon\over3k}$; como a somatória tem no máximo $k$ termos,
concluímos que:
$$\V(\gamma;P)\le\V(\gamma;P\cup
P_0)<\V(\gamma;P)+{\varepsilon\over3},$$ o que completa a
demonstração da parte (ii).

\smallskip

\item{(iii)} $\big\vert\V(\gamma;P\cup
P_0)-\V(\gamma)\big\vert<{\varepsilon\over3};$

\smallskip

Basta observar que:
$$\V(\gamma)-{\varepsilon\over3}<\V(\gamma;P_0)\le\V(\gamma;P\cup
P_0)\le\V(\gamma).$$ Isso completa a demonstração da parte (iii) e
a demonstração do teorema.\fimprova

\medskip

\proclaim Definição. Uma curva $\gamma:[a,b]\to\R^n$ é dita {\rm
de classe $C^p$ por partes} ($1\le p\le\infty$) quando existe uma
partição $P=\{t_0,\ldots,t_k\}$ do intervalo $[a,b]$ tal que
$\gamma\vert_{[t_i,t_{i+1}]}$ é de classe $C^p$ para todo
$i=0,\ldots,k-1$.

Observe que se $\gamma$ é de classe $C^p$ por partes então
$\gamma$ é contínua em $[a,b]$ e (como $p\ge1$) admite derivadas
laterais em cada ponto $t_i$ da partição. Nos pontos $t\in[a,b]$ que não estão
em $P$ (e nas extremidades $t=a$ e $t=b$)
a curva $\gamma$ é diferenciável.

\smallskip

\proclaim Corolário. Se $\gamma:[a,b]\to\R^n$ é de classe $C^1$
por partes então $\gamma$ é retificável e seu comprimento é dado
por:
$$V(\gamma)=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t.$$

\smallskip

O integrando acima não é bem definido num número finito de pontos,
mas isso não é um problema: sabe-se que a integral de uma função
não se altera quando alteramos a função num número finito de
pontos e portanto podemos atribuir um valor arbitrário ao
integrando em questão quando o mesmo não estiver bem definido.

\smallskip

\Prova Seja $P=\{t_0,\ldots,t_k\}$ uma partição de $[a,b]$ tal que
$\gamma\vert_{[t_i,t_{i+1}]}$ é de classe $C^1$ para
$i=0,\ldots,k-1$. Pelo teorema anterior e pela aditividade do
comprimento de arco (e usando uma propriedade elementar da
integral de Riemann):
$$\V(\gamma)=\sum_{i=0}^{k-1}\V(\gamma\vert_{[t_i,t_{i+1}]})=\sum_{i=0}^{k-1}\int_{t_i}^{t_{i+1}}
\big\Vert\gamma'(t)\big\Vert\,\dd
t=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t.\fimprova$$

\smallskip

\proclaim Corolário. Se $\gamma:[a,b]\to\R^n$ é uma curva de
classe $C^1$ por partes então $\gamma$ é parametrizada por
comprimento de arco se e somente se
$\big\Vert\gamma'(t)\big\Vert=1$ para todo $t\in[a,b]$ tal que
$\gamma$ é diferenciável em $t$.

\Prova Segue do Teorema Fundamental do Cálculo e da igualdade:
$$\V(\gamma\vert_{[a,t]})=\int_a^t\big\Vert\gamma'(s)\big\Vert\,\dd
s,$$ para todo $t\in\left(a,b\right]$.\fimprova

\smallskip

\noindent{\tt Observação}. Para cultura geral: a fórmula
$\V(\gamma)=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t$ vale numa classe de
curvas maior do que aquela das curvas $C^1$ por partes. Essa é a classe das
curvas chamadas {\sl absolutamente contínuas\/} (veja o último capítulo do
livro de Teoria da Medida do Pedro Jesus Fernandez, se tiver curiosidade);
nesse contexto, a integral $\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t$ deve
ser entendida no sentido de Lebesgue. A classe de curvas absolutamente
contínuas inclui até mesmo a classe das curvas Lipschitzianas. Mostra-se então que
uma curva absolutamente contínua $\gamma:[a,b]\to\R^n$ é parametrizada por
comprimento de arco se e
somente se a igualdade $\big\Vert\gamma'(t)\big\Vert=1$ vale {\sl para quase
todo $t\in[a,b]$}, i.e., quando tal igualdade é falsa no máximo num
subconjunto de $[a,b]$ que tem {\sl medida de Lebesgue zero}.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Comprimento de arco}.

\medskip

\item{1.} Seja $\gamma:[a,b]\to\R^n$ uma curva. Mostre que
$\V(\gamma)\ge0$ e que $\V(\gamma)=0$ se e somente se $\gamma$ é
constante.

\smallskip

\item{2.} Seja $f:[a,b]\to\R$ uma função monótona. Mostre que $f$
é de variação limitada e que $\V(f)=\big\vert f(b)-f(a)\big\vert$.

\smallskip

\item{3.} Mostre que a condição ``$\gamma:[a,b]\to\R^n$ é
retificável'' não depende da norma usada em $\R^n$.

\smallskip

\item{4.} Seja $\norma_1$ a norma em $\R^n$ definida por $\Vert
x\Vert_1=\sum_{i=1}^n\vert x_i\vert$. Dada uma curva
$\gamma:[a,b]\to\R^n$ e uma partição $P$ de $[a,b]$, denote por
$\V_1(\gamma;P)$ a variação de $\gamma$ com respeito a $P$ definida usando
a norma $\norma_1$ e por $\V_1(\gamma)=\sup_P\V_1(\gamma;P)$ o comprimento de
arco de
$\gamma$ definido usando a norma $\norma_1$. Mostre que:
\smallskip
\itemitem{(a)} se $\gamma=(\gamma^1,\ldots,\gamma^n)$ com cada
$\gamma^i:[a,b]\to\R$ então
$\V_1(\gamma;P)=\sum_{i=1}^n\V_1(\gamma^i;P)$ (onde a norma usada
em $\R$ é o módulo usual);
\itemitem{(b)} conclua do item (a) que $\gamma$ é retificável (com
respeito a $\norma_1$ ou a qualquer outra norma --- vide
Exercício~3) se e somente se cada $\gamma^i$, $i=1,\ldots,n$, é de
variação limitada;
\itemitem{(c)} se $\gamma$ é retificável mostre que
$\V_1(\gamma)=\sum_{i=1}^n\V_1(\gamma^i)$ ({\sl dica}: você deve precisar do
seguinte fato: se $P_1$, \dots, $P_n$ são partições de $[a,b]$ então
$P=\bigcup_{i=1}^nP_i$ é uma partição de $[a,b]$ que refina todas as $P_i$'s
simultaneamente);
\itemitem{(d)} se $\V(\gamma)$ denota o comprimento de arco de
$\gamma$ com respeito à norma Euclideana, dê um exemplo para
mostrar que $\V(\gamma)$ {\sl não pode ser calculado a partir dos
números $\V(\gamma^i)$, $i=1,\ldots,n$}, i.e., procure curvas
retificáveis $\gamma,\mu:[a,b]\to\R^n$ com
$\V(\gamma^i)=\V(\mu^i)$ para todo $i=1,\ldots,n$, mas com
$\V(\gamma)\ne\V(\mu)$ ({\sl dica}: é fácil achar um exemplo com
$n=2$ e com as aplicações $\gamma^i$, $\mu^i$, $i=1,2$ todas
monótonas).

\smallskip

\item{5.} Seja $\gamma:[a,b]\to\R^n$ uma curva retificável que é
contínua no instante $t=a$. O objetivo deste exercício é mostrar
que $\lim_{t\to a}\V(\gamma\vert_{[a,t]})=0$.
\smallskip
\itemitem{(a)} Seja
$c=\inf_{t\in\left(a,b\right]}\V(\gamma\vert_{[a,t]})\ge0$ e
suponha por absurdo que $c>0$. Dado $\varepsilon>0$ arbitrário,
escolha $t\in\left(a,b\right]$ com
$\V(\gamma\vert_{[a,t]})<c+\varepsilon$. Mostre que para todo
$s\in(a,t)$ temos $\V(\gamma\vert_{[s,t]})<\varepsilon$.
\itemitem{(b)} Escolha agora $t\in\left(a,b\right]$ com
$\V(\gamma\vert_{[a,t]})<c+\varepsilon$ e tal que
$\big\Vert\gamma(s)-\gamma(a)\big\Vert<\varepsilon$ para todo
$s\in[a,t]$. Mostre que para toda partição $P$ de $[a,t]$ temos
$\V(\gamma\vert_{[a,t]};P)<2\varepsilon$ ({\sl dica}: trate
separadamente o primeiro termo de $\V(\gamma\vert_{[a,t]};P)$).
\itemitem{(c)} Conclua do item (b) que
$\V(\gamma\vert_{[a,t]})\le2\varepsilon$ e obtenha uma contradição
(de modo que $c=0$).
\itemitem{(d)} Observando que a aplicação $\left(a,b\right]\ni
t\mapsto\V(\gamma\vert_{[a,t]})\in\R$ é crescente, conclua que
$\lim_{t\to a}\V(\gamma\vert_{[a,t]})=0$.

\eject

\item{6.} Seja $\gamma:[a,b]\to\R^n$ uma curva retificável e
defina $V:[a,b]\to\R$ fazendo $V(t)=\V(\gamma\vert_{[a,t]})$ para
$t\in\left(a,b\right]$ e $V(a)=0$. Mostre que os pontos de
continuidade de $\gamma$ coincidem com os pontos de continuidade
de $V$ ({\sl dica}: se $V$ é contínua em $t\in[a,b]$, use a
desigualdade
$\big\Vert\gamma(t_1)-\gamma(t_2)\big\Vert\le\big\vert
V(t_1)-V(t_2)\big\vert$ para mostrar que $\gamma$ também é
contínua em $t$. Se $\gamma$ é contínua em $t$, aplique o resultado
do Exercício~5 para a curva $\gamma\vert_{[t,b]}$ para concluir que $V$ é
contínua à direita em $t$; repita o raciocínio para a {\sl
reparametrização reversa\/} $[-b,-a]\ni s\mapsto\gamma(-s)\in\R^n$
de $\gamma$ para concluir que $V$ é contínua à esquerda em $t$).

\smallskip

\item{7.} ({\sl toda curva contínua é reparametrização de uma curva
parametrizada por comprimento de arco})
Seja $\gamma:[a,b]\to\R^n$ uma curva contínua e retificável.
Mostre que existe um único par $(\sigma,\tilde\gamma)$, onde
$\sigma:[a,b]\to[0,L]$ e $\tilde\gamma:[0,L]\to\R^n$ são
aplicações satisfazendo as seguintes condições:
\smallskip
\itemitem{$\bullet$} $\sigma(a)=0$ e $\sigma(b)=L$;
\itemitem{$\bullet$} $\sigma$ é contínua e monótona;
\itemitem{$\bullet$} $\gamma=\tilde\gamma\circ\sigma$;
\itemitem{$\bullet$} $\tilde\gamma$ é parametrizada por
comprimento de arco.
\smallskip
\noindent({\sl dica}: para mostrar a unicidade de $\sigma$,
considere as igualdades:
$$\sigma(t)=\V(\tilde\gamma\vert_{[0,\sigma(t)]})=\V(\tilde\gamma\circ\sigma\vert_{[a,t]})
=\V(\gamma\vert_{[a,t]}),$$ que mostram que $\sigma$ deve coincidir
com a função $V$ definida no Exercício~6. Para a existência,
defina $\sigma=V$ e mostre que:
\smallskip
\itemitem{(i)} {\sl $\sigma:[a,b]\to[0,L]$ é sobrejetora, onde $L=\V(\gamma)$};
\itemitem{(ii)} {\sl para $s,t\in[a,b]$,
$\sigma(s)=\sigma(t)\Rightarrow\gamma(s)=\gamma(t)$}. \smallskip
\noindent Conclua que existe uma única aplicação $\tilde\gamma:[0,L]\to\R^n$
tal que $\tilde\gamma\circ\sigma=\gamma$, i.e., $\tilde\gamma$
deve preencher o lugar da flecha pontilhada no diagrama
comutativo:
$$\xymatrix{%
[a,b]\ar[r]^\gamma\ar[d]_\sigma&\R^n\\
[0,L]\ar@.[ur]_{\tilde\gamma}}$$ Use novamente a invariância do
comprimento de arco por reparametrizações para concluir que
$\tilde\gamma$ é de fato parametrizada por comprimento de arco).

\smallskip

\item{8.} Dadas curvas retificáveis $\gamma,\mu:[a,b]\to\R^n$,
mostre que:
\smallskip
\itemitem{(a)} a soma $\gamma+\mu$ é retificável e
$\V(\gamma+\mu)\le\V(\gamma)+\V(\mu)$ ({\sl dica}: mostre que
$\V(\gamma+\mu;P)\le\V(\gamma;P)+\V(\mu;P)$ para toda partição $P$
de $[a,b]$);
\itemitem{(b)} se $c\in\R$ então a curva $c\gamma$ (definida por
$(c\gamma)(t)=c\gamma(t)$) é retificável e $\V(c\gamma)=\vert
c\vert\V(\gamma)$ ({\sl dica}: mostre que $\V(c\gamma;P)=\vert
c\vert\V(\gamma;P)$ para toda partição $P$ de $[a,b]$);
\itemitem{(c)} o conjunto ${\rm Ret}([a,b],\R^n)$ formado por todas
as curvas retificáveis $\gamma:[a,b]\to\R^n$ é um subespaço do
espaço vetorial real de todas as curvas $\gamma:[a,b]\to\R^n$ e a
aplicação $\gamma\mapsto\big\Vert\gamma(a)\big\Vert+\V(\gamma)$
define uma norma em ${\rm Ret}([a,b],\R^n)$;
\itemitem{(d)} se $\sigma:[a,b]\to[a,b]$ é crescente e sobrejetora
então a aplicação $${\rm
Ret}([a,b],\R^n)\ni\gamma\longmapsto\gamma\circ\sigma\in{\rm
Ret}([a,b],\R^n)$$ é uma isometria linear.

\vfil\eject

\centerline{\bf Aula número $17$ (10/05)}
\bigskip
\bigskip

A aula começa cobrindo as seções~2 e 3 da aula número 16 (08/05),
sobre comprimento de arco e sua relação com a integral da norma do
vetor tangente (para curvas de classe $C^1$ por partes).

\medskip

\item{(1)} {\bf Integral de um campo vetorial ao longo de uma
curva}.

\smallskip

\proclaim Definição. Um {\rm campo vetorial} em $\R^n$ (definido
num subconjunto $S\subset\R^n$) é uma função $X:S\to\R^n$. Se
$X:S\to\R^n$ é um campo vetorial contínuo (i.e., a aplicação
$X:S\to\R^n$ é contínua) e se $\gamma:[a,b]\to\R^n$ é uma curva de
classe $C^1$ por partes cuja imagem está contida em $S$ então
definimos a {\rm integral de $X$ ao longo de $\gamma$} através da
integral:
$$\int_\gamma X=\int_a^b\big\langle
X\big(\gamma(t)\big),\gamma'(t)\big\rangle\,\dd t,$$ onde $\pint$
denota o produto interno canônico de $\R^n$.

\smallskip

Se $X=(X^1,\ldots,X^n)$ e $\gamma=(\gamma^1,\ldots,\gamma^n)$ com
cada $X^i:S\to\R$ e cada $\gamma^i:[a,b]\to\R$ então a integral de
$X$ ao longo de $\gamma$ pode ser escrita mais explicitamente na
forma:
$$\int_\gamma
X=\sum_{i=1}^n\int_a^bX^i\big(\gamma^1(t),\ldots,\gamma^n(t)\big)(\gamma^i)'(t)\,\dd
t.$$

\smallskip

\noindent{\tt Observação}. Como $\gamma$ é apenas de classe $C^1$
por partes, o integrando $\big\langle
X\big(\gamma(t)\big),\gamma'(t)\big\rangle$ não está bem definido
num número finito de instantes $t\in[a,b]$ --- isso não é um
problema, pois a integral de uma função não se altera quando
alteramos o valor da função num número finito de pontos. Podemos
então atribuir qualquer valor ao integrando em questão nos pontos
$t$ onde $\gamma$ não é diferenciável.

\smallskip

Vamos começar mostrando a invariância da integral de linha por
reparametrizações.

\proclaim Teorema. Sejam $X:S\subset\R^n\to\R^n$ um campo vetorial
contínuo e $\gamma:[a,b]\to\R^n$ uma curva de classe $C^1$ por
partes com $\gamma([a,b])\subset S$. Dada uma função injetora
$\sigma:[c,d]\to[a,b]$ de classe $C^1$ por partes, então:
$$\vcenter{\halign{${\displaystyle#}$,\quad\hfil&#\hfil\cr
\int_{\gamma\circ\sigma}X=\int_\gamma X&se $\sigma(c)=a$ e
$\sigma(d)=b$,\cr
\noalign{\smallskip}\int_{\gamma\circ\sigma}X=-\int_\gamma X&se
$\sigma(c)=b$ e $\sigma(d)=a$.\cr}}$$

\Prova Seja $P=\{t_0,\ldots,t_k\}$ uma partição de $[a,b]$ tal que
$\gamma\vert_{[t_i,t_{i+1}]}$ é de classe $C^1$ para
$i=0,\ldots,k-1$ e seja $Q=\{s_0,\ldots,s_l\}$ uma partição de
$[c,d]$ tal que $\sigma\vert_{[s_i,s_{i+1}]}$ é de classe $C^1$
para $i=0,\ldots,l-1$. Temos que
$Q\cup\sigma^{-1}(P)=\{u_0,\ldots,u_m\}$ é uma partição de $[a,b]$
(usamos aqui que $\sigma$ é injetora!) e é fácil ver que
$(\gamma\circ\sigma)\vert_{[u_i,u_{i+1}]}$ é de classe $C^1$ para
$i=0,\ldots,m-1$; daí $\gamma\circ\sigma$ também é de classe $C^1$
por partes e faz realmente sentido considerar
$\int_{\gamma\circ\sigma}X$. Temos:
$$\int_{\gamma\circ\sigma}X=\int_c^d\big\langle X\big(\gamma(\sigma(t))\big),
(\gamma\circ\sigma)'(t)\big\rangle\,\dd t;$$ exceto para um número
finitos de instantes $t\in[c,d]$ (mais precisamente, exceto para
$t\in\{u_0,\ldots,u_m\}$) temos
$(\gamma\circ\sigma)'(t)=\gamma'\big(\sigma(t)\big)\sigma'(t)$ e
portanto:
$$\int_{\gamma\circ\sigma}X=\int_c^d\big\langle
X\big(\gamma(\sigma(t))\big),\gamma'\big(\sigma(t)\big)\big\rangle\,\sigma'(t)\,\dd
t=\sum_{i=0}^{m-1}\int_{u_i}^{u_{i+1}}\big\langle
X\big(\gamma(\sigma(t))\big),\gamma'\big(\sigma(t)\big)\big\rangle\,\sigma'(t)\,\dd
t.$$ Fazendo a mudança de variável $\sigma(t)=x$ nas integrais acima
obtemos:
$$\int_{\gamma\circ\sigma}X=\sum_{i=0}^{m-1}\int_{\sigma(u_i)}^{\sigma(u_{i+1})}\big\langle
X\big(\gamma(x)\big),\gamma'(x)\big\rangle\,\dd
x=\int_{\sigma(c)}^{\sigma(d)}\big\langle
X\big(\gamma(x)\big),\gamma'(x)\big\rangle\,\dd x,$$ e a última
integral vale $\pm\int_\gamma X$, dependendo do valor de $\sigma$
nas extremidades $c$ e $d$.\fimprova

\smallskip

\noindent{\tt Observação}. A hipótese que $\sigma:[c,d]\to[a,b]$ é
injetora no teorema acima é usada só para concluir que
$\sigma^{-1}(P)$ é finito. Poderíamos alternativamente supor
apenas que para todo $t\in[a,b]$ no qual $\gamma$ não é
diferenciável temos que o conjunto $\sigma^{-1}(t)\subset[c,d]$ é
finito. Observe que sem a hipótese de injetividade para $\sigma$,
é bem possível que $\sigma$ não seja monótona; isso corresponde à
idéia intuitiva que a reparametrização $\gamma\circ\sigma$ de
$\gamma$ pode fazer ``um pouco de zigue-zague'' que a curva
$\gamma$ original não fazia. Desde que $\sigma(c)=a$ e
$\sigma(d)=b$, i.e., desde que $\gamma\circ\sigma$ ``percorra todo
o percurso de $\gamma(a)$ a $\gamma(b)$'', a integral de linha
$\int_\gamma X$ não se altera; na prática, o que ocorre é que
trechos de $\gamma$ percorridos a primeira vez num certo sentido e
uma segunda vez em outro sentido se cancelam. Note bem que {\sl
esse fenômeno não ocorre no caso do comprimento de arco\/}! No
caso do comprimento de arco os ``zigue-zagues'' {\sl aumentam o
comprimento total}, de modo que a hipótese de monotonicidade de
$\sigma$ é essencial.

\smallskip

\noindent{\tt Observação}. Muitos livros elementares de Cálculo e
de Física--Matemática usam notações como $\int_\gamma
X\cdot\dd\vec{\,r}$ em vez de $\int_\gamma X$. A motivação para
essa notação é a seguinte: pensa-se em $\vec{\,r}$ como o ``vetor
posição da partícula que viaja ao longo de $\gamma$'' e daí
$\dd\vec{\,r}$ é uma espécie de ``vetor infinitesimal de variação
de posição''. Escreve-se então $X\cdot\dd\vec{\,r}$ para denotar o
produto escalar (i.e., o produto interno) do campo $X$ pela
``variação infinitesimal'' $\dd\vec{\,r}$.

\smallskip

\noindent{\tt Observação}. A interpretação física para a integral
de linha definida acima (quando $n\le3$) é a seguinte: pensamos em
$X$ como um {\sl campo de forças}, i.e., em cada ponto $x\in\R^n$
temos um ``vetor força'' $X(x)$. A curva $\gamma$ corresponde à
trajetória de uma partícula (o parâmetro $t$ de $\gamma$ é o
tempo). A integral de linha $\int_\gamma X$ corresponde então ao
{\sl trabalho\/} da força $X$ sobre a partícula que descreve a
trajetória $\gamma$. O trabalho de uma força constante $\vec
F\in\R^3$ sobre uma partícula que descreve um movimento retilínio
do ponto $p\in\R^3$ ao ponto $q\in\R^3$ é dada por $\langle\vec
F,q-p\rangle$ (pois a {\sl componente da força $\vec F$ normal ao
movimento não realiza trabalho}). A integral de linha corresponde
a uma passagem ao limite dessa idéia: escolhemos uma partição
$P=\{t_0,\ldots,t_k\}$ do intervalo $[a,b]$ com norma pequena e
instantes $\tau_i\in[t_i,t_{i+1}]$, $i=0,\ldots,k-1$. Num
``intervalo pequeno'' $[t_i,t_{i+1}]$ aproximamos o movimento da
partícula por um segmento de reta e a força dada pelo campo $X$
por uma força constante; o trabalho realizado por $X$ do instante
$t_i$ ao instante $t_{i+1}$ é aproximado então pelo produto
$\big\langle
X\big(\gamma(\tau_i)\big),\gamma(t_{i+1})-\gamma(t_i)\big\rangle$.
Agora aproximamos o vetor $\gamma(t_{i+1})-\gamma(t_i)$ por
$\gamma'(\tau_i)(t_{i+1}-t_i)$ e somamos em $i=0,\ldots,k-1$.
Fazendo o limite quando $\Vert P\Vert\to0$, obtemos a integral de
linha $\int_\gamma X$ (veja a observação a seguir).

\smallskip

\noindent{\tt Observação}. É muito comum definir a integral de
linha $\int_\gamma X$ usando limites de somas da forma
$\sum_{i=0}^{k-1}\big\langle
X\big(\gamma(\tau_i)\big),\gamma(t_{i+1})-\gamma(t_i)\big\rangle$
quando a norma da partição $P=\{t_0,\ldots,t_k\}$ de $[a,b]$ tende
a zero e $\{\tau_0,\ldots,\tau_{k-1}\}$ é uma pontilhação de $P$.
Mostra-se então que esse limite de somas coincide com a integral
de Riemann $\int_a^b\big\langle
X\big(\gamma(t)\big),\gamma'(t)\big\rangle\,\dd t$ no caso que $X$
é contínuo e $\gamma$ é de classe $C^1$ por partes. Essa definição
da integral de linha usando limites de somas é similar à definição
da {\sl integral de Riemann--Stieltjes\/} que não abordaremos
nesse curso.

\noindent[para quem conhece a definição de integral de
Riemann--Stieltjes, observamos que se $X=(X^1,\ldots,X^n)$ e
$\gamma=(\gamma^1,\ldots,\gamma^n)$ então a integral de linha
$\int_\gamma X$ coincide com a soma
$\sum_{i=1}^n\int_a^bX^i\big(\gamma(t)\big)\,\dd\gamma^i(t)$, onde
$\int_a^bX^i\big(\gamma(t)\big)\,\dd\gamma^i(t)$ denota a integral
de Riemann--Stieltjes de $f=X^i\circ\gamma:[a,b]\to\R$ com
respeito à função $\alpha=\gamma^i:[a,b]\to\R$. Os curiosos podem
consultar o {\sl Curso de Análise vol.\ II\/} do Elon Lages Lima,
pgs.\ 193--204.]

\noindent Nós decidimos usar um caminho mais curto para
simplificar a exposição: definir $\int_\gamma X$ {\sl apenas\/}
quando $X$ é contínuo e $\gamma$ é de classe $C^1$ por partes,
usando diretamente a integral de Riemann. Na prática, raramente se
usa a integral $\int_\gamma X$ em condições mais gerais do que a
que consideramos. Nos Exercícios~10--15 apresentamos um roteiro para os
interessados na definição de $\int_\gamma X$ como limite de somas.

\smallskip

\noindent{\tt Observação}. Como mostramos acima, a integral de
linha $\int_\gamma X$ é invariante por reparametrizações, mas
apenas por reparametrizações que ``preservam o sentido de percurso
de $\gamma$'' --- se a reparametrização ``inverte o sentido do
percurso'', o valor de $\int_\gamma X$ troca de sinal. Por esse
motivo diz-se às vezes que $\int_\gamma X$ é uma {\sl integral
orientada}. Observe que, por outro lado, o comprimento de arco
$\V(\gamma)$ é invariante {\sl tanto por reparametrizações
crescentes como por reparametrizações decrescentes} (não há
mudança de sinal). O comprimento de arco é muitas vezes denotado
em textos elementares de Cálculo e em textos de Física--Matemática
por $\int_\gamma\dd s$ ou por $\int_\gamma\Vert\dd\vec{\,r}\Vert$;
essa notação é motivada pela idéia que $\dd s$ (ou
$\Vert\dd\vec{\,r}\Vert$) denotam o comprimento de uma ``porção
infinitesimal'' de $\gamma$. Considera-se também ``integrais'' da
forma $\int_a^bf\,\dd s$ (denotadas também por
$\int_a^bf\,\Vert\dd\vec{\,r}\Vert$), onde $f:[a,b]\to\R$ é uma
função escalar. Tais integrais podem ser definidas como limites de
somas da forma
$\sum_{i=0}^{k-1}f(\tau_i)\big\Vert\gamma(t_{i+1})-\gamma(t_i)\big\Vert$
quando a norma da partição $P=\{t_0,\ldots,t_k\}$ de $[a,b]$ tende
a zero e $\{\tau_0,\ldots,\tau_{k-1}\}$ é uma pontilhação de $P$.
Mostra-se então que, se $f$ é contínua e $\gamma$ é de classe
$C^1$ por partes, então $\int_\gamma f\,\dd s$ coincide com a
integral de Riemann $\int_a^bf(t)\big\Vert\gamma'(t)\big\Vert\,\dd
t$ (ou, para simplificar a exposição, alguns autores definem
$\int_\gamma f\,\dd s$ apenas quando $f$ é contínua e $\gamma$ é
de classe $C^1$ por partes, usando diretamente a integral de
Riemann $\int_a^bf(t)\big\Vert\gamma'(t)\big\Vert\,\dd t$). Essa
integral $\int_\gamma f\,\dd s$ corresponde fisicamente, por
exemplo, à {\sl massa do ``fio'' $\gamma$ cuja densidade linear de
massa é expressa pela função $f$\/} (o comprimento de arco é
reobtido quando $f\equiv1$). A integral $\int_\gamma f\,\dd s$,
como o comprimento de arco, é invariante tanto por
reparametrizações crescentes como por reparametrizações
decrescentes, não havendo mudança de sinal; diz-se então que tais
integrais são {\sl não-orientadas}. Alguns autores usam também a
terminologia {\sl integral de linha de primeiro tipo\/} para
integrais da forma $\int_\gamma f\,\dd s$ e {\sl integral de linha
de segundo tipo\/} para integrais da forma $\int_\gamma X$.

\smallskip

Para finalizar, vamos enunciar algumas propriedades elementares da integral
de linha cuja demonstração é imediata a partir de propriedades
correspondentes da integral de Riemann.

\proclaim Teorema. Sejam $X,Y:S\subset\R^n\to\R^n$ campos
vetoriais contínuos e $\gamma:[a,b]\to\R^n$ uma curva de classe
$C^1$ por partes. Então:
\smallskip
\itemitem{\rm(a)} $\int_\gamma X+Y=\int_\gamma X+\int_\gamma Y$;
\itemitem{\rm(b)} $\int_\gamma kX=k\int_\gamma X$, para toda
constante $k\in\R$;
\itemitem{\rm(c)} $\int_\gamma
X=\int_{\gamma\vert_{[a,c]}}X+\int_{\gamma\vert_{[c,b]}}X$, para
todo $c$ no intervalo aberto $(a,b)$;
\itemitem{\rm(d)} $\left\vert\int_\gamma
X\right\vert\le\sup_{t\in[a,b]}\big\Vert
X\big(\gamma(t)\big)\big\Vert\,\V(\gamma)$.

Observe que a demonstração do item (d) do teorema acima segue
trivialmente usando a desigualdade de Cauchy--Schwarz e a fórmula
$\V(\gamma)=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t$ para o
comprimento de arco $\V(\gamma)$.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Funções de variação limitada}

\medskip

\item{0.} Seja $f:[a,b]\to\R$ uma função de variação limitada. Defina
$V:[a,b]\to\R$ fazendo $V(t)=\V(f\vert_{[a,t]})$ para $t\in\left(a,b\right]$ e
$V(a)=0$. Mostre que $V$ e $V-f$ são funções crescentes ({\sl dica}:
$\big\vert f(x)-f(y)\big\vert\le\big\vert V(x)-V(y)\big\vert$). Conclua que
uma função $g:[a,b]\to\R$ é de variação limitada se e somente se pode ser
escrita como diferença de duas funções crescentes ({\sl dica}: use os
Exercícios~2 e 8(a) da Aula número 16 --- 08/05).

\medskip

\noindent{\tt Recordação das propriedades elementares da integral
de Riemann}.

\smallskip

Os exercícios que aparecem aqui são para aqueles que querem apreender a
demonstrar algumas propriedades básicas da integral de Riemann definida em
termos de limites de somas de Riemann. Alguns exercícios não são tão fáceis,
mas todos esses resultados podem ser encontrados no {\sl Curso de Análise,
vol.\ I}, do Elon Lages Lima (capítulo sobre integração). Avisamos no entanto
que o Elon utiliza uma definição diferente de integral de Riemann (em termos
de integrais inferiores e superiores), mas depois ele mostra que essa
definição diferente é de fato equivalente àquela adotada aqui.

\smallskip

\item{1.} Sejam $f,g:[a,b]\to\R$ funções limitadas, $k\in\R$ um número real e $(P,\tau)$ uma
partição pontilhada de $[a,b]$. Mostre que:
$$S(f+g;P,\tau)=S(f;P,\tau)+S(g;P,\tau),\quad
S(kf;P,\tau)=kS(f;P,\tau),$$ e que se $f(t)\le g(t)$ para todo
$t\in[a,b]$ então $S(f;P,\tau)\le S(g;P,\tau)$. Conclua que se
$f,g:[a,b]\to\R$ são Riemann integráveis então $f+g$ e $kf$ também
o são e:
$$\int_a^bf+g=\int_a^bf+\int_a^bg,\quad\int_a^bkf=k\int_a^bf;$$
além do mais, se $f(t)\le g(t)$ para todo $t\in[a,b]$, mostre que
$\int_a^bf\le\int_a^bg$. Conclua que se $\vert f\vert$ também é
Riemann integrável então
$\left\vert\int_a^bf\right\vert\le\int_a^b\vert f\vert$ ({\sl
dica}: $-\vert f\vert\le f\le\vert f\vert$).

\smallskip

\noindent{\tt Observação}. Na verdade, se $f$ é integrável então
$\vert f\vert$ é automaticamente integrável (veja observação mais adiante
sobre a condição necessária e suficiente para a Riemann integrabilidade em
termos do conjunto dos pontos de descontinuidade da função).

\smallskip

\item{2.} ({\sl critério de Cauchy para integrabilidade}) Mostre
que uma função limitada $f:[a,b]\to\R$ é Riemann integrável se e
somente se vale a seguinte propriedade: dado $\varepsilon>0$,
existe $\delta>0$ tal que dadas partições pontilhadas arbitrárias
$(P,\tau)$ e $(P',\tau')$ de $[a,b]$ com $\Vert P\Vert<\delta$ e
$\Vert P'\Vert<\delta$ então $\big\vert
S(f;P,\tau)-S(f;P',\tau')\big\vert<\varepsilon$ ({\sl dica}: para
mostrar que $f$ é Riemann integrável, escolha uma seqüência
arbitrária $(P_n,\tau_n)$ de partições pontilhadas com $\Vert
P_n\Vert\to0$ e mostre que a seqüência $S(f;P_n,\tau_n)$ é de
Cauchy em $\R$; mostre que o limite $I\in\R$ dessa seqüência é
igual à integral $\int_a^bf$).

\goodbreak
\smallskip

\item{3.} Se $f:[a,b]\to\R$ é Riemann integrável e
$[c,d]\subset[a,b]$ é um subintervalo, mostre que $f\vert_{[c,d]}$
é Riemann integrável ({\sl dica}: se $(P,\tau)$ é uma partição
pontilhada de $[c,d]$ então $\widetilde P=P\cup\{a,b\}$ e
$\widetilde\tau=\tau\cup\{a,b\}$ definem uma partição pontilhada
$(\widetilde P,\widetilde\tau)$ de $[a,b]$; além do mais, se
$(P,\tau)$ e $(P',\tau')$ são partições pontilhadas de $[c,d]$,
$\widetilde P=P\cup\{a,b\}$, $\widetilde P'=P'\cup\{a,b\}$,
$\widetilde\tau=\tau\cup\{a,b\}$ e
$\widetilde\tau'=\tau'\cup\{a,b\}$ então:
$$S(f;\widetilde P,\widetilde\tau)-S(f;\widetilde
P',\widetilde\tau')=S(f\vert_{[c,d]};P,\tau)-S(f\vert_{[c,d]};P',\tau').$$
Use o critério de Cauchy explicado no Exercício~2).

\smallskip

\item{4.} Sejam $f:[a,b]\to\R$ uma função e $c$ um ponto do
intervalo aberto $(a,b)$, de modo que $f\vert_{[a,c]}$ e
$f\vert_{[c,b]}$ sejam Riemann integráveis. Mostre que $f$ também é
Riemann integrável e que:
$$\int_a^bf=\int_a^cf+\int_c^bf.$$
[{\sl dica}: se $(P,\tau)$ é uma partição pontilhada de $[a,b]$
com $c\in P$ então podemos de maneira óbvia definir partições
pontilhadas $(P_1,\tau_1)$ e $(P_2,\tau_2)$ de $[a,c]$ e de
$[c,b]$ respectivamente (com $P_1=P\cap[a,c]$ e $P_2=P\cap[c,b]$)
de modo que
$S(f;P,\tau)=S(f\vert_{[a,c]};P_1,\tau_1)+S(f\vert_{[c,b]};P_2,\tau_2)$.
Se $P$ é uma partição de $[a,b]$ que não contém $c$ e se $\Vert
P\Vert<\delta$ então $P\cup\{c\}$ é uma partição de $[a,b]$ que
contém $c$ e $\big\vert
S(f;P,\tau)-S(f;P\cup\{c\},\tau')\big\vert<2M\delta$, onde
$M=\sup_{t\in[a,b]}\big\vert f(t)\big\vert$ e $\tau'$ é
uma pontilhação adequada (definida de maneira óbvia) de $P\cup\{c\}$].

\smallskip

\item{5.} Mostre que se $f:[a,b]\to\R$ é nula fora de um
subconjunto finito de $[a,b]$ então $f$ é Riemann integrável e
$\int_a^bf=0$ ({\sl dica}: se o conjunto dos pontos de $[a,b]$
onde $f$ não é nula possui $k$ elementos e $(P,\tau)$ é uma
partição pontilhada de $[a,b]$ com $\Vert P\Vert<\delta$ então
$\big\vert S(f;P,\tau)\big\vert\le2kM\delta$, onde
$M=\sup_{t\in[a,b]}\big\vert f(t)\big\vert$). Conclua que se
$f:[a,b]\to\R$ é Riemann integrável e $g:[a,b]\to\R$ difere de $f$
só num conjunto finito então $g$ também é Riemann integrável e
$\int_a^bg=\int_a^bf$.

\smallskip

\item{6.} Mostre que toda função contínua $f:[a,b]\to\R$ é Riemann
integrável ({\sl dica}: use o Critério de Cauchy da seguinte
forma: se $P$ e $P'$ são partições de $[a,b]$ então $P\cup P'$ é
uma partição que refina $P$ e $P'$ simultaneamente. É suficiente
então para aplicar o critério de Cauchy estimar o valor de
$\big\vert S(f;P,\tau)-S(f;P',\tau')\big\vert$ com $\Vert
P\Vert<\delta$, onde $(P,\tau)$, $(P',\tau')$ são partições
pontilhadas de $[a,b]$ e $P'$ refina $P$, i.e., $P'\supset P$. Use
o fato que $f$ é uniformemente contínua e limitada para escolher
$\delta>0$).

\smallskip

\item{7.} ({\sl o Teorema Fundamental do Cálculo}) Mostre que se
$f:[a,b]\to\R$ é uma função Riemann integrável e se $F:[a,b]\to\R$
é definida por $F(t)=\int_a^tf$ (com $F(a)=0$) então $F$ é contínua e,
para todo $t\in[a,b]$ onde $f$ é contínua, temos que $F$ é
derivável em $t$ e $F'(t)=f(t)$. Conclua que se $f$ é contínua
então existe uma função $G:[a,b]\to\R$ de classe $C^1$ com $G'=f$
e, dada uma tal função, temos $\int_a^bf=G(b)-G(a)$.

\goodbreak
\smallskip

\item{8.} Mostre que se $F:[a,b]\to\R$ é uma função de classe
$C^1$ por partes então $\int_a^bF'=F(b)-F(a)$, onde atribuímos um
valor arbitrário ao integrando no número finito de instantes onde
$F$ não é derivável ({\sl dica}: use os Exercícios~4, 5 e 7).
Usando a convenção $\int_p^pf=0$ e $\int_p^qf=-\int_q^pf$ para
$p>q$, mostre que $\int_p^qF'=F(q)-F(p)$ para todos $p,q\in[a,b]$.

\smallskip

\item{9.} ({\sl mudança de variáveis na integral de Riemann}) Se
$f:[a,b]\to\R$ é uma função contínua e $g:[c,d]\to\R$ é uma função
de classe $C^1$ por partes cuja imagem está contida em $[a,b]$
então:
$$\int_{g(c)}^{g(d)}f(t)\,\dd t=\int_c^df\big(g(x)\big)g'(x)\,\dd
x,$$ onde, como sempre, substituímos $g'(x)$ por um valor
arbitrário no número finito de pontos $x\in[c,d]$ onde $g$ não é
derivável ({\sl dica}: seja $F:[a,b]\to\R$ uma função tal que
$F'=f$ e aplique o teorema fundamental do cálculo para a função
composta $F\circ g:[c,d]\to\R$).

\smallskip

\noindent{\tt Observação}.
Sabe-se que uma função limitada $f:[a,b]\to\R$ é
Riemann integrável se e somente se {\sl o conjunto dos pontos onde $f$ é
descontínua possui medida de Lebesgue zero}. Para os interessados em mais
detalhes, consultem o {\sl Curso de Análise, vol.\ I\/} do Elon Lages Lima
(capítulo sobre integração).

\medskip

\noindent{\tt A integral de linha como limite de somas}.

\smallskip

Esta série de Exercícios é para aqueles que desejam estudar a noção de
integral de linha num contexto mais geral (i.e., para curvas que podem não ser
de classe $C^1$ por partes e campos que podem não ser contínuos). Mais
detalhes podem ser encontrados no {\sl Curso de Análise, vol.\ II} do Elon
Lages Lima (capítulo sobre integrais curvilíneas). Antes de começar a série de
Exercícios, apresentamos a definição geral de integrais de linha da forma
$\int_\gamma X$.

\smallskip

Sejam $X:S\subset\R^n\to\R^n$ um campo vetorial arbitrário e
$\gamma:[a,b]\to\R^n$ uma curva arbitrária cuja imagem está
contida em $S$. Se $P=\{t_0,\ldots,t_k\}$ é uma partição de
$[a,b]$ e $\tau=\{\tau_0,\ldots,\tau_{k-1}\}$ é uma pontilhação
para $P$ então definimos a {\sl soma de Riemann\/}
$S(X,\gamma;P,\tau)$ fazendo:
$$S(X,\gamma;P,\tau)=\sum_{i=0}^{k-1}\big\langle
X\big(\gamma(\tau_i)\big),\gamma(t_{i+1})-\gamma(t_i)\big\rangle.$$
A {\sl integral de $X$ ao longo de $\gamma$\/} é definida como
sendo o número real $I\in\R$ (se existir) tal que dado
$\varepsilon>0$, existe $\delta>0$ tal que $\big\vert
S(X,\gamma;P,\tau)-I\big\vert<\varepsilon$, para toda partição
pontilhada $(P,\tau)$ de $[a,b]$ com $\Vert P\Vert<\delta$. É
fácil ver que $I$ é único se existir; escrevemos $I=\int_\gamma
X$.

\smallskip

\item{10.} Mostre que se $X,Y:S\subset\R^n\to\R^n$ são campos
vetoriais, $\gamma:[a,b]\to\R^n$ é uma curva com
imagem contida em $S$ e se $k\in\R$ é um número real então,
se as integrais $\int_\gamma X$ e $\int_\gamma Y$ existem então
também as integrais $\int_\gamma X+Y$ e $\int_\gamma kX$ existem e
valem as identidades:
$$\int_\gamma X+Y=\int_\gamma X+\int_\gamma Y,\quad\int_\gamma
kX=k\int_\gamma X.$$ ({\sl dica}: a solução é quase igual a do Exercício~1).

\smallskip

\item{11.} ({\sl critério de Cauchy para integral de linha}) Sejam
$X:S\subset\R^n\to\R^n$ um campo vetorial e $\gamma:[a,b]\to\R^n$
uma curva cuja imagem está contida em $S$. Mostre que a integral
$\int_\gamma X$ existe se e somente se vale a seguinte
propriedade: dado $\varepsilon>0$, existe $\delta>0$ tal que dadas
partições pontilhadas $(P,\tau)$, $(P',\tau')$ arbitrárias de
$[a,b]$ com $\Vert P\Vert<\delta$, $\Vert P'\Vert<\delta$ então
$\big\vert
S(X,\gamma;P,\tau)-S(X,\gamma;P',\tau')\big\vert<\varepsilon$
({\sl dica}: a solução é quase igual a do Exercício~2).

\smallskip

\item{12.} Se $X:S\subset\R^n\to\R^n$ é um campo vetorial e
$\gamma:[a,b]\to\R^n$ é uma curva cuja imagem está contida em $S$
então, se a integral $\int_\gamma X$ existe então a integral
$\int_{\gamma\vert_{[c,d]}}X$ também existe para todo subintervalo
$[c,d]\subset[a,b]$ ({\sl dica}: a solução é quase igual a do Exercício~3).
Mostre também que se $c$ é um ponto do intervalo aberto $(a,b)$
então vale a identidade:
$$\int_\gamma
X=\int_{\gamma\vert_{[a,c]}}X+\int_{\gamma\vert_{[c,b]}}X.$$ ({\sl
dica}: a solução é quase igual a do Exercício~4, mas como já estamos supondo
que $\int_\gamma X$ existe, é suficiente considerar partições $P$ de $[a,b]$
que contêm $c$. Observamos que em geral não é possível
concluir que $\int_\gamma X$ existe a partir da existência de
$\int_{\gamma\vert_{[a,c]}}X$ e de $\int_{\gamma\vert_{[c,b]}}X$;
veja o {\sl Curso de Análise vol.\ II\/} do Elon Lages Lima, pg.\
236, Exercício 2.3 para um contra-exemplo no contexto da integral
de Riemann-Stieltjes).

\smallskip

\item{13.} ({\sl invariância por reparametrização}) Sejam
$X:S\subset\R^n\to\R^n$ um campo vetorial e $\gamma:[a,b]\to\R^n$ uma curva
arbitrária com imagem contida em $S$. Mostre que se a integral $\int_\gamma X$
existe e $\sigma:[c,d]\to[a,b]$ é uma função monótona e sobrejetora então a
integral $\int_{\gamma\circ\sigma}X$ também existe e vale a igualdade:
$$\vcenter{\halign{${\displaystyle#}$,\quad\hfil&#\hfil\cr
\int_{\gamma\circ\sigma}X=\int_\gamma X&se $\sigma$ é
crescente,\cr
\noalign{\smallskip}\int_{\gamma\circ\sigma}X=-\int_\gamma X&se
$\sigma$ é decrescente.\cr}}$$ [{\sl dica}: use o fato que uma
função monótona num intervalo é contínua se sua imagem também for
um intervalo (veja {\sl Curso de Análise, vol.\ I}, Elon Lages
Lima, pg.\ 182). Daí $\sigma$ é contínua e portanto uniformemente
contínua. Mostre que se $(P,\tau)$ é uma partição pontilhada de
$[c,d]$ então $P'=\sigma(P)$ é uma partição de $[a,b]$ e podemos
escolher uma pontilhação $\tau'$ de $P'$ (de maneira óbvia) de
modo que $S(X,\gamma\circ\sigma;P,\tau)=\pm S(X,\gamma;P',\tau')$,
sendo que o sinal $+$ aparece para $\sigma$ crescente e o sinal
$-$ aparece para $\sigma$ decrescente (o argumento é similar ao
usado na demonstração da invariância do comprimento de arco por
reparametrizações, mas aqui aparece um sinal de $-$ no caso
decrescente). Para concluir, use a continuidade uniforme de
$\sigma$ para mostrar que $\Vert P\Vert\to0$ implica $\Vert
P'\Vert\to0$, i.e., que dado $\delta'>0$, existe $\delta>0$ de
modo que $\Vert P\Vert<\delta$ implica
$\big\Vert\sigma(P)\big\Vert<\delta'$].

\eject

\item{14.} Mostre que se $X:S\subset\R^n\to\R^n$ é um campo
vetorial contínuo e $\gamma:[a,b]\to\R^n$ é uma curva contínua e
retificável com imagem contida em $S$ então a integral
$\int_\gamma X$ existe e vale a desigualdade:
$$\left\vert\int_\gamma X\right\vert\le\sup_{t\in[a,b]}\big\Vert
X\big(\gamma(t)\big)\big\Vert\,\V(\gamma).$$
({\sl dica}: a idéia da prova é similar a do Exercício~6).

\smallskip

\item{15.} Mostre que se $X:S\subset\R^n\to\R^n$ é um campo
vetorial contínuo e $\gamma:[a,b]\to\R^n$ é uma curva de classe
$C^1$ por partes cuja imagem está contida em $S$ então:
$$\int_\gamma X=\int_a^b\big\langle
X\big(\gamma(t)\big),\gamma'(t)\big\rangle\,\dd t.$$
[{\sl dica}: primeiro reduza o caso geral ao caso que $\gamma$ é de classe
$C^1$. Lembre que você {\sl já sabe\/} que as integrais $\int_\gamma X$ e
$\int_a^b\big\langle X\big(\gamma(t)\big),\gamma'(t)\big\rangle\,\dd t$
existem, de modo que você não precisa se preocupar em usar partições
pontilhadas arbitrárias para mostrar a existência das integrais. Se
$P=\{t_0,\ldots,t_k\}$ é uma partição de $[a,b]$ de norma pequena e se
$\tau=\{\tau_0,\ldots,\tau_{k-1}\}$ é uma pontilhação de $P$, use (para cada
$i=0,\ldots,k-1$) a desigualdade do valor médio para a função
$t\mapsto\gamma(t)-\gamma'(\tau_i)t$ no intervalo $[t_i,t_{i+1}]$ para obter
uma estimativa para o erro ao aproximar $\gamma(t_{i+1})-\gamma(t_i)$ por
$\gamma'(\tau_i)(t_{i+1}-t_i)$. Use o fato que $X\circ\gamma:[a,b]\to\R^n$ é
limitado e o fato que $\gamma':[a,b]\to\R^n$ é uniformemente contínua para
concluir a demonstração].

\vfil\eject

\centerline{\bf Aula número $18$ (15/05)}
\bigskip
\bigskip

A aula começa cobrindo parte da demonstração da fórmula
``$\V(\gamma)=\int_a^b\big\Vert\gamma'(t)\big\Vert\,\dd t$'' (para
$\gamma$ de classe $C^1$ por partes) e todo o material
originalmente planejado para a aula número 17 (10/05).

\medskip

\item{(1)} {\bf Mais álgebra linear: o espaço dual}.

\smallskip

\proclaim Definição. Seja $V$ um espaço vetorial real. O {\rm
espaço dual} de $V$ é o espaço $V^*=\Lin(V,\R)$ formado por todos
os funcionais lineares em $V$, i.e., por todos os operadores
lineares $T:V\to\R$.

Os elementos de $V^*$ são às vezes chamados também de {\sl
co-vetores}.

\smallskip

A teoria do espaço dual é extremamente simplificada quando $V$ tem
dimensão finita. {\sl Suporemos então a partir de agora que todos
os espaços vetoriais considerados são reais e têm dimensão
finita}.

\smallskip

Se ${\frak B}=(b_i)_{i=1}^n$ é uma base para $V$ então para cada
$i=1,\ldots,n$, denotamos por $b^*_i:V\to\R$ a aplicação que
associa a cada vetor $v\in V$ a sua $i$-ésima coordenada na base
${\frak B}$, ou seja:
$$v=\sum_{i=1}^nb_i^*(v)b_i,$$
para todo $v\in V$. É fácil ver que $b^*_i$ é linear e portanto
define um elemento do espaço dual $V^*$. Além do mais, temos:
$$\eqalign{b_i^*(b_j)=\cases{0,&$i\ne j$,\cr
1,&$i=j$,\cr}}$$ para todos $i,j=1,\ldots,n$. Temos o seguinte:
\proclaim Teorema. Se ${\frak B}=(b_i)_{i=1}^n$ é uma base de $V$
então a família ${\frak B}^*=(b_i^*)_{i=1}^n$ é uma base de $V^*$.

\Prova Se $\sum_{i=1}^nk_ib_i^*=0$ com cada $k_i\in\R$ então,
avaliando os dois lados dessa igualdade em $b_j$ obtemos $k_j=0$ e
portanto ${\frak B}^*$ é linearmente independente. Dado agora
$\alpha\in V^*$ temos:
$$\alpha(v)=\alpha\left(\sum_{i=1}^nb_i^*(v)b_i\right)=\left(\sum_{i=1}^n\alpha(b_i)b_i^*\right)(v),$$
para todo $v\in V$ e portanto
$\alpha=\sum_{i=1}^n\alpha(b_i)b_i^*$. A conclusão segue.\fimprova

\smallskip

\proclaim Definição. A base ${\frak B}^*$ de $V^*$ definida acima
é chamada a {\rm base dual} correspondente a ${\frak B}$.

\smallskip

\noindent{\tt Observação}. A demonstração acima nos dá uma
informação adicional importante: {\sl a $i$-ésima coordenada de um
funcional $\alpha\in V^*$ na base dual ${\frak B}^*$ é exatamente
o valor de $\alpha$ no $i$-ésimo vetor da base original ${\frak
B}$}.

\smallskip

Passamos agora a estudar o {\sl espaço bidual\/} de $V$, i.e., o
dual do dual de $V$; vamos denotá-lo por $V^{**}$. A cada $v\in
V$, podemos associar canonicamente um funcional linear em $V^*$
que é o {\sl operador de avaliação em $v$}; vamos denotá-lo
temporariamente por $\hat v$. Temos:
$$\hat v(\alpha)=\alpha(v),$$
para todo $\alpha\in V^*$. É fácil ver que $\hat v$ é realmente
linear e portanto define um elemento de $V^{**}$. Além do mais, a
aplicação:
$$V\ni v\longmapsto\hat v\in V^{**}$$
é linear. Vamos mostrar que tal aplicação é um isomorfismo. Em
primeiro lugar, se $\hat v=0$ então $\hat v(\alpha)=\alpha(v)=0$
para todo $\alpha\in V^*$ e portanto $v=0$ (se $v\ne0$, sempre
podemos encontrar $\alpha\in V^*$ com $\alpha(v)\ne0$ --- por
exemplo, $b_i^*(v)\ne0$ para algum $i=1,\ldots,n$). Mostramos
então que $v\mapsto\hat v$ é injetora. Como
$\Dim(V^{**})=\Dim(V^*)=\Dim(V)$, demonstramos o seguinte:
\proclaim Teorema. A aplicação $V\ni v\mapsto\hat v\in V^{**}$ é
um isomorfismo.\fimprova

\smallskip

É muito comum usar o isomorfismo $v\mapsto\hat v$ para {\sl
identificar\/} $V$ com seu bidual $V^{**}$, i.e., denota-se o
funcional de avaliação em $v$ com o próprio símbolo $v$, em vez de
$\hat v$. Essa identificação corresponde intuitivamente à seguinte
idéia: se $v\in V$ e $\alpha\in V^*$ então ``não distinguimos
entre aplicar $\alpha$ em $v$ ou aplicar $v$ em $\alpha$'', i.e.,
escrevemos $\alpha(v)=v(\alpha)$; o vetor $v$ identifica-se com a
operação $\hat v$ que leva $\alpha$ em $\alpha(v)$.

\smallskip

\noindent{\tt Observação}. A existência de um isomorfismo canônico
$V\cong V^{**}$ num certo sentido explica a terminologia ``espaço
dual''. Em geral, diz-se que ``dois tipos de objetos são duais''
quando ``obtem-se um escalar ao juntar os dois''. A idéia de
identificar $V$ com $V^{**}$ corresponde em pensar na operação de
juntar $v$ e $\alpha$ como uma operação na qual $v$ e $\alpha$ tem
um papel de caráter similar, i.e., não se distingue o vetor da
função.

\smallskip

\noindent{\tt Observação}. Se ${\frak B}=(b_i)_{i=1}^n$ é uma base
de $V$ então, como já observamos, as coordenadas de um funcional
$\alpha\in V^*$ na base dual ${\frak B}^*=(b_i^*)_{i=1}^n$ são
exatamente as avaliações de $\alpha$ nos vetores $b_i$ de ${\frak
B}$. Uma releitura desse fato nos diz o seguinte: a {\sl base
bidual\/} associada a ${\frak B}$, i.e., a base ${\frak
B}^{**}=(b_i^{**})_{i=1}^n$ dual de ${\frak B}^*$ coincide com a
base original ${\frak B}$ de $V$, se identificarmos $V$ com
$V^{**}$ da maneira descrita acima.

\smallskip

Assim como o espaço bidual $V^{**}$, é claro que o espaço dual
$V^*$ também é isomorfo a $V$, já que $\Dim(V)=\Dim(V^*)$; por
exemplo, se ${\frak B}=(b_i)_{i=1}^n$ é uma base de $V$ então
podemos definir um isomorfismo de $V$ sobre $V^*$ que leva $b_i$
em $b_i^*$ para todo $i=1,\ldots,n$. Ocorre no entanto uma
diferença importante entre as afirmações ``$V$ é isomorfo a
$V^*$'' e ``$V$ é isomorfo a $V^{**}$'': o isomorfismo que
construímos entre $V$ e $V^*$ {\sl não é canônico}, i.e., {\sl
depende da base ${\frak B}$ escolhida para defini-lo}, enquanto
que a definição do isomorfismo $V\cong V^{**}$ não depende de
escolha alguma.

\smallskip

\noindent{\tt Observação}. A noção de {\sl isomorfismo canônico\/}
pode ser formalizada dentro do contexto da {\sl teoria das
categorias}; na terminologia técnica de tal teoria diria-se que
$V\mapsto V$ e $V\mapsto V^{**}$ são {\sl funtores naturalmente
isomorfos}. O diagrama comutativo que aparece no Exercício~7
fornece justamente a definição técnica de naturalidade de
transformações entre funtores na teoria das categorias!

\smallskip

Para finalizar, observamos que {\sl a escolha de um produto
interno em $V$ determina um isomorfismo de $V$ sobre $V^*$}. Em
geral, se $B:V\times V\to\R$ é uma forma bilinear, sabemos que a
aplicação $T:V\to\Lin(V,\R)=V^*$ definida por $T(v)(w)=B(v,w)$ é
linear. Denotaremos o funcional linear $T(v)$ por $B(v,\cdot)$.
Temos o seguinte: \proclaim Teorema. Se $\pint$ é um produto
interno em $V$ então a aplicação linear:
$$V\ni v\longmapsto\langle v,\cdot\rangle\in V^*$$
correspondente à forma bilinear $\pint$ é um isomorfismo.

\Prova Como $\Dim(V)=\Dim(V^*)$, basta mostrar que a aplicação
linear em questão é injetora. Se $v\in V$ é tal que o funcional
linear $\langle v,\cdot\rangle$ é nulo então em particular
$\langle v,v\rangle=0$ e portanto $v=0$ (já que $\pint$ é definida
positiva). Isso mostra que $v\mapsto\langle v,\cdot\rangle$ tem
núcleo zero e completa a demonstração.\fimprova

\smallskip

Recorde que uma base $(b_i)_{i=1}^n$ de $V$ é dita {\sl
ortonormal\/} com respeito a um produto interno $\pint$ quando
$\langle b_i,b_j\rangle=0$ para $i\ne j$ e $\langle
b_i,b_i\rangle=1$ para todos $i,j=1,\ldots,n$. O seguinte teorema
nos permite identificar o isomorfismo $v\mapsto\langle
v,\cdot\rangle$ mais concretamente em termos de bases ortonormais
de $V$.

\proclaim Teorema. Seja $\pint$ um produto interno em $V$ e
${\frak B}=(b_i)_{i=1}^n$ uma base ortonormal. Então o isomorfismo
$V\to V^*$ determinado por $\pint$ é o único isomorfismo que leva
$b_i$ sobre $b_i^*$ para $i=1,\ldots,n$. Em particular, as
coordenadas de um vetor $v\in V$ com respeito à base ${\frak B}$
coincidem com as coordenadas de $\alpha=\langle v,\cdot\rangle\in
V^*$ com respeito à base dual ${\frak B}^*$.

\Prova Devemos mostrar que $\langle b_i,\cdot\rangle=b_i^*$ para
todo $i=1,\ldots,n$; basta mostrar então que esses funcionais
coincidem sobre uma base de ${\frak B}$, i.e., basta ver que
$\langle b_i,b_j\rangle=b_i^*(b_j)$ para $j=1,\ldots,n$. Isso
segue diretamente da definição de base ortonormal e da definição
de base dual. Quanto à útlima afirmação, se o isomorfismo
$v\mapsto\langle v,\cdot\rangle$ leva a base ${\frak B}$ sobre a
base ${\frak B}^*$ então as coordenadas de $v$ na base ${\frak B}$
coincidem com as coordenadas de $\langle v,\cdot\rangle$ na base
${\frak B}^*$.\fimprova

\smallskip

\noindent{\tt Exemplo}. Considerando o $\R^n$ munido de seu
produto interno canônico então a base canônica é ortonormal e
portanto o isomorfismo $\R^n\ni v\mapsto\langle
v,\cdot\rangle\in{\R^n}^*$ induzido pelo produto interno canônico
leva o vetor $v=(v_1,\ldots,v_n)\in\R^n$ no funcional linear
$\alpha\in{\R^n}^*$ que possue coordenadas $(v_1,\ldots,v_n)$ na
base dual da base canônica. Esse funcional é dado por:
$$\alpha(x)=\sum_{i=1}^nv_ix_i=\pmatrix{v_1&v_2&\cdots&v_n\cr}\pmatrix{x_1\cr
x_2\cr\vdots\cr x_n\cr},$$ para todo $x\in\R^n$ e portanto ele é
representado pela matriz linha $\big(v_1\ v_2\ \cdots\ v_n\big)$.
Observe que o $i$-ésimo vetor da base dual da base canônica de
$\R^n$ é simplesmente o $i$-ésimo operador de projeção
$(x_1,\ldots,x_n)\mapsto x_i$.

\goodbreak
\medskip

\item{(2)} {\bf ${\bf 1}$-formas em ${\bf \R^n}$}.

\smallskip

\proclaim Definição. Uma {\rm $1$-forma} em $\R^n$ (definida num
subconjunto $S\subset\R^n$) é uma aplicação $\omega:S\to{\R^n}^*$
que a cada ponto $x\in S$ associa um funcional linear
$\omega(x):\R^n\to\R$.

\smallskip

Vocês devem manter em mente a analogia entre a definição acima e a
definição de campo vetorial: um campo vetorial associa a cada
ponto de $S\subset\R^n$ um vetor, i.e., um elemento de $\R^n$; uma
$1$-forma associa a cada ponto de $S\subset\R^n$ um co-vetor,
i.e., um elemento de ${\R^n}^*$. Vamos agora entender a definição
acima de maneira mais concreta.

\smallskip

Denotaremos, como sempre, por $(e_i)_{i=1}^n$ a base canônica de
$\R^n$. De acordo com a notação da seção anterior, deveríamos
denotar a base dual da base canônica de $\R^n$ por
$(e_i^*)_{i=1}^n$; como observamos no último exemplo daquela
seção, $e_i^*:\R^n\to\R$ é simplesmente o $i$-ésimo operador de
projeção $\R^n\ni x\mapsto x_i\in\R$. No contexto do cálculo com
$1$-formas, no entanto, notações como $(e_i^*)_{i=1}^n$ para a
base dual da base canônica {\sl não são usuais}; em vez, o
$i$-ésimo vetor da base dual da base canônica de $\R^n$ é
usualmente denotado por $\dd x_i$. Embora em princípio não seja
necessário justificar a introdução de uma notação, existe uma
``explicação'' padrão para o uso da notação $(\dd x_i)_{i=1}^n$
para a base dual de $(e_i)_{i=1}^n$. A explicação é a seguinte: se
denotamos por $x=(x_1,\ldots,x_n)$ um vetor de $\R^n$, então é
perfeitamente natural denotar o $i$-ésimo operador de projeção
$x\mapsto x_i$ (i.e., o $i$-ésimo vetor da base dual da base
canônica) simplesmente pelo símbolo $x_i$. Como tal operador de
projeção é linear, seu diferencial em qualquer ponto é igual a ele
próprio; é razoável então denotar também por $\dd x_i$ o próprio
operador de projeção $x\mapsto x_i$ (embora $\dd x_i(x)$ seria
possivelmente uma notação mais correta).

Se $\omega:S\subset\R^n\to{\R^n}^*$ é uma $1$-forma então para
cada $x\in S$ o co-vetor $\omega(x)\in{\R^n}^*$ pode ser escrito
de modo único como combinação linear dos vetores da base dual da
base canônica de $\R^n$. Temos então que a $1$-forma $\omega$ pode
ser escrita de modo único sob a forma:
$$\omega(x)=\sum_{i=1}^na_i(x)\dd x_i,$$
onde $a_i:S\to\R$, $i=1,\ldots,n$, são funções a valores reais.

\smallskip

\noindent{\tt Exemplo}. Se $f:U\subset\R^n\to\R$ é uma função
diferenciável num aberto $U\subset\R^n$ então o diferencial de $f$
é uma $1$-forma $\dd f:U\to{\R^n}^*$. A $i$-ésima coordenada de
$\dd f(x)$ na base $(\dd x_i)_{i=1}^n$ é $\dd f(x)\cdot
e_i={\partial f\over\partial x_i}(x)$; temos então:
$$\dd f(x)=\sum_{i=1}^n{\partial f\over\partial x_i}(x)\,\dd x_i,$$
para todo $x\in U$.

\smallskip

Como já observamos na seção anterior, o produto interno canônico
$\pint$ de $\R^n$ induz um isomorfismo $v\mapsto\langle
v,\cdot\rangle$ de $\R^n$ sobre ${\R^n}^*$. Como a base canônica
de $\R^n$ é ortonormal, sabemos que esse isomorfismo leva o vetor
$v=(v_1,\ldots,v_n)\in\R^n$ sobre o co-vetor $\sum_{i=1}^nv_i\dd
x_i$. Esse isomorfismo entre $\R^n$ e ${\R^n}^*$ fornece uma
correspondência biunívoca entre campos vetoriais
$X:S\subset\R^n\to\R^n$ e $1$-formas
$\omega:S\subset\R^n\to{\R^n}^*$ de modo que
$\omega(x)=\big\langle X(x),\cdot\big\rangle$ para todo $x\in S$.
Em termos das bases canônicas temos:
$$X(x)=\sum_{i=1}^nX_i(x)e_i,\quad\omega(x)=\sum_{i=1}^nX_i(x)\dd
x_i,$$ para todo $x\in S$.

\smallskip

\noindent{\tt Exemplo}. Se $f:U\subset\R^n\to\R$ é uma função
diferenciável no aberto $U\subset\R^n$ então o campo vetorial
associado à $1$-forma $\dd f:U\to{\R^n}^*$ é conhecido como o {\sl
gradiente\/} de $f$ e é denotado por $\nabla f$. Temos:
$$\nabla f(x)=\sum_{i=1}^n{\partial f\over\partial x_i}(x)e_i,$$
para todo $x\in U$. Além do mais, obtemos também a familiar
fórmula:
$${\partial f\over\partial v}(x)=\dd f(x)\cdot v=\big\langle\nabla
f(x),v\big\rangle,\quad x\in U,\ v\in \R^n,$$ para as derivadas
direcionais ${\partial f\over\partial v}(x)$ de $f$. Se $\nabla
f(x)\ne0$, uma aplicação direta da desigualdade de Cauchy--Schwarz
mostra que a direção $v={\nabla f(x)\over\Vert\nabla f(x)\Vert}$
maximiza o valor da derivada direcional ${\partial f\over\partial
v}(x)$ para $\Vert v\Vert=1$.

\medskip

\item{(3)} {\bf Integração de ${\bf 1}$-formas}.

\smallskip

\proclaim Definição. Seja $\omega:S\subset\R^n\to{\R^n}^*$ uma
$1$-forma contínua definida num subconjunto $S\subset\R^n$. Se
$\gamma:[a,b]\to\R^n$ é uma curva de classe $C^1$ por partes cuja
imagem está contida em $S$ então a {\rm integral de $\omega$ ao
longo de $\gamma$} é definida por:
$$\int_\gamma\omega=\int_a^b\omega\big(\gamma(t)\big)\cdot\gamma'(t)\,\dd
t.$$

\smallskip

Obviamente, se $\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$ e
$X(x)=\sum_{i=1}^na_i(x)e_i$ é o campo vetorial $X:S\to\R^n$
associado a $\omega$ pelo isomorfismo canônico de $\R^n$ sobre
${\R^n}^*$ então:
$$\int_\gamma\omega=\sum_{i=1}^n\int_a^ba_i\big(\gamma(t)\big)(\gamma^i)'(t)\,\dd
t=\int_\gamma X.$$

\smallskip

Na verdade, não existe diferença nenhuma entre a teoria de
integração de campos vetoriais sobre curvas em $\R^n$ e a teoria
de integração de $1$-formas sobre curvas em $\R^n$ --- é só uma
questão de trocar $\sum_{i=1}^na_i(x)\dd x_i$ por
$\sum_{i=1}^na_i(x)e_i$. Em particular, todos os resultados que
mostramos sobre integração de campos vetoriais em curvas
(invariância por reparametrização, aditividade por concatenação,
etc) admitem versões análogas na teoria de integração de
$1$-formas em curvas.

\smallskip

\noindent{\tt Observação}. Em vista do comentário acima, surge uma
pergunta: ``para que estudar integral de linha na linguagem de
$1$-formas?'' --- em primeiro lugar, observe que a definição da
integral $\int_\gamma\omega$ é ``mais natural'' que a definição da
integral $\int_\gamma X$ pois a integração de $1$-formas não faz
referência ao produto interno de $\R^n$. Na verdade, a grande
diferença só é sentida quando se estuda integral de linha para
curvas em {\sl variedades diferenciáveis}: lá o produto interno em
geral não está disponível e {\sl apenas a integração de $1$-formas
faz sentido}!

\smallskip

\noindent{\tt Observação}. Muitos livros elementares de Cálculo
usam notações como $\int_\gamma P\dd x+Q\dd y$ significando apenas
a integral do campo vetorial $(P,Q)$ ao longo da curva $\gamma$.
Nesse caso a notação $P\dd x+Q\dd y$ é usada sem a introdução
formal da noção de forma diferencial. É interessante observar que
com a escolha da notação $\dd x_i$ para os vetores da base dual da
base canônica, a notação clássica $\int_\gamma P\dd x+Q\dd y$ dos
livros de Cálculo elementares é recuperada agora com um
significado matemático formal.

\smallskip

\noindent{\tt Observação}. Assim como a integral de campos
vetoriais, a integração de $1$-formas também pode ser definida em
termos de limites de somas, caso seja desejável generalizar o
significado de $\int_\gamma\omega$ para $1$-formas $\omega$ que
não são contínuas e curvas $\gamma$ que não são de classe $C^1$
por partes (veja os Exercícios~9--12).

\smallskip

\noindent{\tt Observação}. Para quem está curioso com o nome
``$1$-forma'', mencionamos agora (mas só estudaremos em detalhes
depois) que uma $k$-forma em $\R^n$ é uma função que a cada ponto
de um subconjunto $S\subset\R^n$ associa uma {\sl aplicação
$k$-linear anti-simétrica em $\R^n$} (note que no caso $k=1$ a
anti-simetria é redundante). A teoria de $k$-formas aparecerá
naturalmente quando estudarmos integral de superfície.

\medskip

\item{(4)} {\bf Formas fechadas e exatas}.

\smallskip

\proclaim Definição. Uma $1$-forma contínua $\omega:U\to\R^n$
definida num aberto $U\subset\R^n$ é dita {\rm exata} quando
existe uma função $f:U\to\R$ de classe $C^1$ tal que $\dd
f=\omega$.

\smallskip

\noindent{\tt Observação}. Se $X:U\to\R^n$ é o campo vetorial
associado a uma $1$-forma $\omega:U\to{\R^n}^*$ então uma função
$f:U\to\R$ de classe $C^1$ com $\dd f=\omega$ é chamada um {\sl
potencial\/} para $X$ (observe que $\nabla f=X$). Na terminologia
dos cursos elementares de Cálculo, um campo vetorial que admite um
potencial (i.e., um campo vetorial que corresponde a uma $1$-forma
exata) é chamado um {\sl campo conservativo}.

\smallskip

\proclaim Teorema. Seja $f:U\to\R$ uma função de classe $C^1$
definida num aberto $U\subset\R^n$ e seja $\gamma:[a,b]\to\R^n$
uma curva de classe $C^1$ por partes cuja imagem está contida em
$U$. Então:
$$\int_\gamma\dd f=f\big(\gamma(b)\big)-f\big(\gamma(a)\big).$$

\Prova É só calcular:
$$\int_\gamma\dd f=\int_a^b\dd
f\big(\gamma(t)\big)\cdot\gamma'(t)\,\dd
t=\int_a^b(f\circ\gamma)'(t)\,\dd t;$$ a conclusão segue do
Teorema Fundamental do Cálculo.\fimprova

\smallskip

\proclaim Corolário. Se $\omega:U\to{\R^n}^*$ é uma $1$-forma
exata então a integral $\int_\gamma\omega$ não depende da curva
$\gamma$, mas {\rm somente de suas extremidades}; mais
precisamente, se $\gamma:[a,b]\to\R^n$, $\mu:[a',b']\to\R^n$ são
curvas de classe $C^1$ por partes com imagem em $U$ tais que
$\gamma(a)=\mu(a')$ e $\gamma(b)=\mu(b')$ então:
$$\int_\gamma\omega=\int_\mu\omega.\fimprova$$

\smallskip

Vamos agora estudar o problema clássico da dependência do caminho
nas integrais de linha, i.e., o problema de determinar se uma
integral da forma $\int_\gamma\omega$ depende da curva $\gamma$ ou
só de suas extremidades $\gamma(a)$ e $\gamma(b)$. Para começar,
temos o seguinte resultado muito simples: \proclaim Teorema. Seja
$\omega:U\to{\R^n}^*$ uma $1$-forma contínua definida num aberto
$U\subset\R^n$. As seguinte condições são equivalentes:
\smallskip
\itemitem{\rm(a)} dadas curvas arbitrárias $\gamma:[a,b]\to\R^n$,
$\mu:[a',b']\to\R^n$ de classe $C^k$ por partes ($1\le
k\le\infty$) com imagens contidas em $U$ e $\gamma(a)=\mu(a')$,
$\gamma(b)=\mu(b')$ então $\int_\gamma\omega=\int_\mu\omega$;
\itemitem{\rm(b)} dada uma curva $\gamma:[a,b]\to\R^n$ de classe $C^k$ por partes com imagem contida em $U$
então, se $\gamma$ é {\rm fechada} (i.e., se
$\gamma(a)=\gamma(b)$), então $\int_\gamma\omega=0$.

\Prova Suponha (a). Se $\gamma:[a,b]\to\R^n$ é uma curva fechada
de classe $C^k$ por partes com imagem contida em $U$ então temos
$\int_\gamma\omega=\int_\mu\omega$, onde $\mu:[a,b]\to\R^n$ é a
curva constante e igual a $\gamma(a)$ (observe que $\gamma$ e
$\mu$ tem as mesmas extremidades). Como obviamente
$\int_\mu\omega=0$, (b) segue. Suponha agora (b). Se
$\gamma:[a,b]\to\R^n$ e $\mu:[a',b']\to\R^n$ são curvas de classe
$C^k$ por partes com imagens contidas em $U$ e as mesmas
extremidades, então podemos definir uma curva de classe $C^k$ por
partes $\rho:[0,1]\to\R^n$ tal que
$\rho\vert_{\big[0,{1\over2}\big]}$ é uma reparametrização
crescente de $\gamma$ e $\rho\vert_{\big[{1\over2},1\big]}$ é uma
reparametrização decrescente de $\mu$. Como $\rho$ é uma curva
fechada, temos:
$$0=\int_\rho\omega=\int_\gamma\omega-\int_\mu\omega;$$
isso prova (a) e completa a demonstração.\fimprova

\smallskip

Já estabelecemos que para $1$-formas exatas $\omega$ a integral de
linha $\int_\gamma\omega$ não depende da curva $\gamma$, mas só de
suas extremidades. Mostramos agora a recíproca: \proclaim Teorema.
Seja $\omega:U\to{\R^n}^*$ uma $1$-forma contínua definida num
aberto $U\subset\R^n$. Suponha que para quaisquer curvas
$\gamma:[a,b]\to\R^n$, $\mu:[a',b']\to\R^n$ de classe $C^\infty$
por partes, com imagens contidas em $U$ e $\gamma(a)=\mu(a')$,
$\gamma(b)=\mu(b')$ temos $\int_\gamma\omega=\int_\mu\omega$.
Então $\omega$ é exata.

\Prova Não há perda de generalidade em supor que $U$ é conexo. De
fato, se $U$ não é conexo então escrevemos $U=\bigcup_{i\in I}U_i$
como união de suas {\sl componentes conexas\/} (recorde
Exercício~7, Aula número~7 --- 27/03). Daí cada $U_i$ é um aberto
conexo e a união $U=\bigcup_{i\in I}U_i$ é disjunta. Se pudermos
para cada $i\in I$ definir uma função $f_i:U_i\to\R$ de classe
$C^1$ com $\dd f_i=\omega\vert_{U_i}$ então a conclusão será
obtida definindo $f:U\to\R$ fazendo $f\vert_{U_i}=f_i$ para todo
$i\in I$.

Suponha então que $U$ é um aberto conexo e vamos construir
$f:U\to\R$ de classe $C^1$ com $\dd f=\omega$. Fixe
arbitrariamente $x_0\in U$ e defina:
$$f(x)=\int_\gamma\omega,$$
onde $\gamma:[a,b]\to\R^n$ é uma curva arbitrária de classe
$C^\infty$ por partes com imagem em $U$ e $\gamma(a)=x_0$,
$\gamma(b)=x$. Observe que por hipótese a integral
$\int_\gamma\omega$ de fato não depende da escolha de $\gamma$.
Obviamente devemos nos preocupar com a {\sl existência\/} de ao
menos uma tal curva $\gamma$, mas tal existência é demonstrada com
um argumento rotineiro de conexidade (veja Exercício~8). Escreva
$\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$; fixado $x\in U$ e
$i=1,\ldots,n$, vamos mostrar que a derivada parcial ${\partial
f\over\partial x_i}(x)$ existe e é igual a $\omega(x)\cdot
e_i=a_i(x)$. Isso basta para concluir que $f$ é de classe $C^1$ e
que $\dd f=\omega$ (pois concluiremos que $f$ tem derivadas
parciais contínuas!). Escolha $\varepsilon>0$ tal que $x+te_i\in
U$ para todo $t\in(-\varepsilon,\varepsilon)$. Fixe uma curva
qualquer $\gamma$ de classe $C^\infty$ por partes ligando $x_0$ a
$x$ e considere a curva $\mu:[0,1]\to\R^n$ definida por
$\mu(s)=x+ste_i$ para todo $s\in[0,1]$. Podemos escolher uma curva
$\rho:[0,1]\to\R^n$ de classe $C^\infty$ por partes que é obtida
concatenando reparametrizações crescentes de $\gamma$ e $\mu$,
i.e., $\rho$ é tal que $\rho\vert_{\big[0,{1\over2}\big]}$ é uma
reparametrização crescente de $\gamma$ e
$\rho\vert_{\big[{1\over2},1\big]}$ é uma reparametrização
crescente de $\mu$. Daí $\rho$ é uma curva de classe $C^\infty$
por partes com imagem contida em $U$ tal que $\rho(0)=x_0$,
$\rho(1)=x+te_i$; logo:
$$\eqalign{f(x+te_i)&=\int_\rho\omega=\int_\gamma\omega+\int_\mu\omega
=\int_\gamma\omega+\int_0^1\omega(x+ste_i) \cdot(te_i)\,\dd s\cr
&=\int_\gamma\omega+\int_0^1a_i(x+ste_i)t\,\dd s,\cr}$$ para todo
$t\in(-\varepsilon,\varepsilon)$. Fazendo a mudança de variável
$st=u$ na última integral obtemos:
$$f(x+te_i)=\int_\gamma\omega+\int_0^ta_i(x+ue_i)\,\dd u,$$
para todo $t\in(-\varepsilon,\varepsilon)$. Derivando a igualdade
acima em $t=0$ e usando o Teorema Fundamental do Cálculo (note que
o termo $\int_\gamma\omega$ não depende de $t$!) obtemos:
$${\partial f\over\partial x_i}(x)=\left.{\dd\over\dd
t}f(x+te_i)\right\vert_{t=0}=a_i(x),$$ o que completa a
demonstração.\fimprova

\smallskip

Resumimos os resultados mostrados até agora omitindo os detalhes
técnicos do enunciado para uma visualização melhor dos fatos. {\sl
Temos que as seguintes afirmações são equivalentes sobre uma
$1$-forma contínua $\omega$ num aberto de $\R^n$}:
\smallskip
\item{(i)} {\sl $\omega$ é exata};
\item{(ii)} {\sl a integral $\omega$ sobre curvas não depende do
caminho};
\item{(iii)} {\sl a integral de $\omega$ sobre curvas fechadas é
zero}.

\smallskip

Precisamos agora de métodos mais diretos para determinar se uma
$1$-forma é ou não exata. Observe que se uma $1$-forma
$\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$ de classe $C^1$ for exata
então existe uma função $f$ de classe $C^2$ tal que ${\partial
f\over\partial x_i}(x)=a_i(x)$ para todo $x$. Aplicando o Teorema
de Schwarz obtemos:
$${\partial a_i\over\partial x_j}(x)={\partial^2f\over\partial
x_j\partial x_i}(x)={\partial^2f\over\partial x_i\partial
x_j}(x)={\partial a_j\over\partial x_i}(x),$$ para todo $x$. Isso
motiva a seguinte: \proclaim Definição. Uma $1$-forma
$\omega:U\to{\R^n}^*$ de classe $C^1$ num aberto $U\subset\R^n$ é
dita {\rm fechada} se:
$${\partial a_i\over\partial x_j}(x)={\partial a_j\over\partial
x_i}(x),$$ para todo $x\in U$, e todos $i,j=1,\ldots,n$, onde
$\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$.

\smallskip

Do Teorema de Schwarz segue trivialmente o seguinte: \proclaim
Teorema. Toda $1$-forma exata de classe $C^1$ é fechada.\fimprova

\smallskip

A recíproca do teorema acima é falsa, mas não totalmente. Isso
fica pra próxima\dots

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Espaço dual}.

\medskip

Nessa série de exercícios, {\sl todos os espaços vetoriais são
reais e de dimensão finita}.

\smallskip

\item{1.} Sejam $V$, $W$ espaços vetoriais. Se $T:V\to W$ é um
operador linear então o seu {\sl operador transposto\/}
$T^*:W^*\to V^*$ é definido por $T^*(\alpha)=\alpha\circ T$ para
todo $\alpha\in W^*$. Mostre que:
\smallskip
\itemitem{(a)} $T^*$ é bem definido e linear;
\itemitem{(b)} se ${\frak B}$ é uma base de $V$, ${\frak C}$ é uma
base de $W$ e ${\frak B}^*$, ${\frak C}^*$ são as bases duais a
${\frak B}$ e a ${\frak C}$ respectivamente então a matriz
$[T]_{\frak B\frak C}$ que representa $T$ com respeito às bases
${\frak B}$ e ${\frak C}$ é a {\sl transposta\/} da matriz
$[T^*]_{\frak C^*\frak B^*}$ que representa $T^*$ com respeito às
bases duais ${\frak C}^*$ e ${\frak B}^*$;
\itemitem{(c)} $(T_1\circ T_2)^*=T_2^*\circ T_1^*$, se $T_1:V\to W$, $T_2:U\to
V$ são operadores lineares;
\itemitem{(d)} se $\Id$ denota a identidade de $V$ então $\Id^*$ é
a identidade de $V^*$;
\itemitem{(e)} se $T:V\to W$ é um isomorfismo então $T^*$ também
o é e $(T^*)^{-1}=(T^{-1})^*$.

\smallskip

\item{2.} Se $S$ é um subespaço do espaço vetorial $V$ então o
{\sl anulador\/} de $S$ em $V$ é definido por:
$$S^\anul=\big\{\alpha\in V^*:\alpha\vert_S=0\big\}.$$
Mostre que:
\smallskip
\itemitem{(a)} o {\sl operador restrição\/} definido por
$V^*\ni\alpha\mapsto\alpha\vert_S\in S^*$ é linear, sobrejetor e
seu núcleo é $S^\anul$ (em particular, $S^\anul$ é um subespaço de
$V^*$);
\itemitem{(b)} conclua do item (a) que
$\Dim(S)+\Dim(S^\anul)=\Dim(V)$;
\itemitem{(c)} se $A$ é um {\sl subconjunto\/} arbitrário de $V$,
então o conjunto $\big\{\alpha\in V^*:\alpha\vert_A=0\big\}$
coincide com o anulador do subespaço $S$ gerado por $A$ em $V$ (e
portanto é também um subespaço de $V^*$).

\smallskip

\item{3.} Se $V$ é um espaço vetorial e $Z$ é um subespaço do dual
$V^*$ então o {\sl espaço anulado\/} por $Z$ é definido por:
$$Z_\anul=\big\{v\in V:\alpha(v)=0\ \hbox{para todo}\ \alpha\in
Z\big\}.$$ Mostre que:
\smallskip
\itemitem{(a)} $Z_\anul$ é um subespaço de $V$;
\itemitem{(b)} se $S$ é um subespaço de $V$ então $(S^\anul)_\anul=S$,
i.e., o espaço anulado pelo anulador de $S$ é $S$ ({\sl dica}: se
$v\in V$, $v\not\in S$, construa $\alpha\in V^*$ com
$\alpha\vert_S=0$ e $\alpha(v)=1$);
\itemitem{(c)} conclua do item (b) que se $S_1,S_2\subset V$ são
subespaços com $S_1^\anul=S_2^\anul$ então $S_1=S_2$;
\itemitem{(d)} se $Z^\anul\subset V^{**}$ denota o anulador de $Z$
então, identificando $V$ com $V^{**}$ da maneira usual, $Z^\anul$
identifica-se com $Z_\anul$;
\itemitem{(e)} se $S$ é um subespaço de $V$ então, identificando
$V$ e $V^{**}$ da maneira usual, temos que o espaço anulado por
$S\subset V^{**}$ (que é um subespaço de $V^*$) é precisamente o
anulador de $S$;
\itemitem{(f)} conclua dos itens (b), (d) e (e) que $(Z_\anul)^\anul=Z$ ({\sl dica}: interprete a igualdade $(Z^\anul)_\anul=Z$);
\itemitem{(g)} conclua do item (f) que
$\Dim(Z)+\Dim(Z_\anul)=\Dim(V)$;
\itemitem{(h)} conclua do item (f) que se $Z_1,Z_2\subset V^*$ são
subespaços com $(Z_1)_\anul=(Z_2)_\anul$ então $Z_1=Z_2$;
\itemitem{(i)} se $B$ é um {\sl subconjunto\/} arbitrário de $V^*$
então o conjunto: $$\big\{v\in V:\alpha(v)=0,\ \hbox{para todo}\
\alpha\in B\big\}$$ coincide com o espaço anulado pelo subespaço
$Z$ gerado por $B$ em $V^*$ (e portanto é também um subespaço de
$V$).

\smallskip

\item{4.} Se $\alpha_1,\ldots,\alpha_k\in V^*$ são funcionais
lineares e se $S=\bigcap_{i=1}^k\Ker(\alpha_i)$ então todo
$\alpha\in V^*$ pertencente ao anulador de $S$ é uma combinação
linear dos $\alpha_i$'s ({\sl dica}: se $Z\subset V^*$ é o
subespaço gerado pelos $\alpha_i$'s então $S=Z_\anul$; use o item
(f) do Exercício~3).

\smallskip

\item{5.} Se $V$, $W$ são espaços vetoriais e $T:V\to W$ é um
operador linear, mostre que:
\smallskip
\itemitem{(a)} $\Ker(T^*)=\big[\Img(T)\big]^\anul$;
\itemitem{(b)} $\Img(T^*)=\big[\Ker(T)\big]^\anul$ ({\sl dica}: se $\alpha\in\big[\Ker(T)\big]^\anul$,
defina $\beta:\Img(T)\to\R$ {\sl por passagem ao quociente}, i.e.,
de modo a completar a flecha pontilhada no diagrama comutativo:
$$\xymatrix@C+5pt{V\ar[d]_T\ar[dr]^\alpha\\
\Img(T)\ar@.[r]_\beta&\R}$$ escolha uma extensão linear arbitrária
$\tilde\beta:W\to\R$ de $\beta$ e mostre que
$T^*\big(\tilde\beta\big)=\alpha$);
\itemitem{(c)} conclua do item (a) que $T$ é sobrejetora se e
somente se $T^*$ é injetora;
\itemitem{(d)} conclua do item (b) que $T$ é injetora se e somente
se $T^*$ é sobrejetora;
\itemitem{(e)} conclua do item (b) que
$\Dim\big(\Img(T)\big)=\Dim\big(\Img(T^*)\big)$;
\itemitem{(f)} conclua do item (e) acima e do item (b) do Exercício~1
que o ``posto coluna'' de uma matriz (i.e., a dimensão do
subespaço gerado por suas colunas) e o ``posto linha'' dessa
matriz (i.e., a dimensão do subespaço gerado por suas linhas)
coincidem ({\sl dica}: a dimensão do subespaço gerado pelas
colunas de uma matriz coincide com a dimensão da imagem do
operador linear associado a essa matriz).

\smallskip

\item{6.} Sejam $V$ um espaço vetorial e $T:V\to\R^n$ um operador
linear. Escreva $T=(T_1,\ldots,T_n)$ com cada $T_i:V\to\R$ (cada
$T_i$ é linear, obviamente). Mostre que $T$ é sobrejetora se e
somente se os funcionais $T_1$, \dots, $T_n$ são linearmente
independentes em $V^*$ ({\sl dica}: se $(e_i^*)_{i=1}^n$ denota a
base dual da base canônica de $\R^n$ então $T_i=T^*(e_i^*)$; use o
item (c) do Exercício~5). Use o item (b) do Exercício~5 para obter
uma outra demonstração (além daquela dada no Exercício~4) para a
igualdade:
$$\left[\bigcap_{i=1}^n\Ker(T_i)\right]^\anul=\hbox{subespaço gerado por}\
\{T_1,\ldots,T_n\}\subset V^*.$$

\smallskip

\item{7.} Se $V$, $W$ são espaços vetoriais, $\phi_V:V\to V^{**}$,
$\phi_W:W\to W^{**}$ denotam os isomorfismos canônicos e $T:V\to
W$ é um operador linear, mostre que o diagrama:
$$\xymatrix@C+20pt{%
V\ar[r]^T\ar[d]_{\phi_V}^{\scriptscriptstyle\cong}&W\ar[d]_{\scriptscriptstyle\cong}^{\phi_W}\\
\;\;V^{**}\ar[r]+<-10pt,0pt>_-{T^{**}=(T^*)^*}&\;\;W^{**}}$$
comuta (isso significa que, fazendo as identificações usuais
$V\cong V^{**}$, $W\cong W^{**}$ então o {\sl operador
bi-transposto\/} $T^{**}=(T^*)^*$ de $T$ identifica-se com $T$).
Conclua do item (e) do Exercício~1 que $T$ é um isomorfismo se e
somente se $T^*$ o for.

\medskip

\noindent{\tt Abertos conexos de $\R^n$}.

\smallskip

\item{8.} Seja $U\subset\R^n$ um aberto conexo. Mostre que dados
$x,y\in U$ existe uma curva $\gamma:[0,1]\to\R^n$ de classe
$C^\infty$ por partes com imagem contida em $U$ e $\gamma(0)=x$,
$\gamma(1)=y$ ({\sl dica}: defina uma relação $\sim$ em $U$
fazendo $x\sim y$ se e somente se existe uma curva de classe
$C^\infty$ por partes em $U$ ligando $x$ a $y$; mostre que $\sim$
é uma relação de equivalência em $U$ e que as classes de
equivalência são abertas).

\medskip

\noindent{\tt A integral de linha como limite de somas}.

\smallskip

Os Exercícios~9, 10 e 11 abaixo são preparações para o
Exercício~12.

\smallskip

\item{9.} Sejam $U\subset\R^n$ um aberto e $\gamma:[a,b]\to\R^n$
uma curva contínua com imagem contida em $U$. Mostre que existe
$\delta>0$ tal que para todos $t,s\in[a,b]$ com $\vert
t-s\vert\le\delta$ temos que o segmento de reta
$\big[\gamma(t),\gamma(s)\big]$ está contido em $U$ ({\sl dica}:
use a continuidade uniforme de $\gamma$ e o fato que existe
$\varepsilon>0$ tal que a bola
$\Bola\big(\gamma(t);\varepsilon\big)$ está contida em $U$ para
todo $t\in[a,b]$).

\smallskip

\item{10.} Sejam $\gamma:[a,b]\to\R^n$ uma curva contínua e
$U\subset\R^n$ um aberto que contém a imagem de $\gamma$. Suponha
que $\delta>0$ é tal que o segmento
$\big[\gamma(t),\gamma(s)\big]$ está contido em $U$ para todos
$t,s\in[a,b]$ com $\vert t-s\vert\le\delta$. Mostre que existe um
compacto $K\subset U$ tal que
$\big[\gamma(t),\gamma(s)\big]\subset K$ para todos $t,s\in[a,b]$
com $\vert t-s\vert\le\delta$ ({\sl dica}: tome $K=\phi(A)$ onde
$A$ é definido por:
$$A=\big\{(t,s,u)\in[a,b]\times[a,b]\times[0,1]:\vert
t-s\vert\le\delta\big\},$$ e $\phi$ é definida por
$\phi(t,s,u)=(1-u)\gamma(t)+u\gamma(s)\in U\subset\R^n$).

\medskip

Nos exercícios a seguir, você deve considerar a integral de linha
definida em termos de limites de somas (como nos Exercícios~10--15
da aula número 17 --- 10/05). No caso de integrais de $1$-formas
$\int_\gamma\omega$ (em vez de integrais de campos vetoriais
$\int_\gamma X$) a integral é definida como limite de somas de
Riemann da forma
$\sum_{i=0}^{k-1}\omega\big(\gamma(\tau_i)\big)\cdot\big[\gamma(t_{i+1})-\gamma(t_i)\big]$.

\eject

\item{11.} Sejam $U\subset\R^n$ um aberto, $\omega:U\to{\R^n}^*$
uma $1$-forma contínua e $\gamma:[a,b]\to\R^n$ uma curva contínua
e retificável com imagem contida em $U$. Mostre que dado
$\varepsilon>0$, existe uma curva $\mu:[a,b]\to\R^n$ de classe
$C^\infty$ por partes tal que a imagem de $\mu$ está contida em
$U$, $\mu(a)=\gamma(a)$, $\mu(b)=\gamma(b)$ e
$\left\Vert\int_\gamma\omega-\int_\gamma\mu\right\Vert<\varepsilon$
({\sl dica}: tome $\delta>0$ como no Exercício~9 e $K$ como no
Exercício~10. Você já sabe que a integral $\int_\gamma\omega$
existe e portanto é possível escolher uma partição
$P=\{t_0,\ldots,t_k\}$ de $[a,b]$ com $\Vert P\Vert<\delta$ e tal
que:
$$\left\Vert\int_\gamma\omega-\sum_{i=0}^{k-1}\omega\big(\gamma(t_i)\big)\cdot\big[\gamma(t_{i+1})-\gamma(t_i)\big]\right\Vert<\varepsilon',$$
onde $\varepsilon'>0$ é escolhido a partir de $\varepsilon$ de
maneira adequada. Defina $\mu:[a,b]\to\R^n$ fazendo:
$$\mu(t)={(t-t_i)\gamma(t_{i+1})+(t_{i+1}-t)\gamma(t_i)\over
t_{i+1}-t_i},$$ para todo $t\in[t_i,t_{i+1}]$ e todo
$i=0,\ldots,k-1$. Compare o termo:
$$\omega\big(\gamma(t_i)\big)\cdot\big[\gamma(t_{i+1})-\gamma(t_i)\big]=\int_{t_i}^{t_{i+1}}
\omega\big(\gamma(t_i)\big)\cdot\mu'(t)\,\dd t$$ com a integral
$\int_{\mu\vert_{[t_i,t_{i+1}]}}\omega$. Use a continuidade
uniforme de $\omega$ em $K$ e a continuidade uniforme de $\gamma$
para refinar $P$ de maneira adequada).

\smallskip

\item{12.} Suponha que $f:U\to\R$ é uma função de classe $C^1$
definida num aberto $U\subset\R^n$ e $\gamma:[a,b]\to\R^n$ é uma
curva contínua e retificável com imagem contida em $U$. Mostre
que:
$$\int_\gamma\dd
f=f\big(\gamma(b)\big)-f\big(\gamma(a)\big).$$ ({\sl dica}: use o
Exercício~11).

\vfil\eject

\centerline{\bf Aula número $19$ (17/05)}
\bigskip
\bigskip

A aula começa cobrindo o material das seções~2, 3 e 4 da aula
número 18 (15/05) sobre $1$-formas, integração de $1$-formas e
sobre formas fechadas e exatas.

\medskip

\item{(1)} {\bf Uma ${\bf 1}$-forma fechada que não é exata}.

\smallskip

Para $(x,y)\in\R^2$, $(x,y)\ne0$ definimos:
$$\omega(x,y)=-{y\over x^2+y^2}\,\dd x+{x\over x^2+y^2}\,\dd y,$$
onde $\dd x$ e $\dd y$ denotam os vetores da base canônica de
${\R^2}^*$. Temos que $\omega$ é uma $1$-forma de classe
$C^\infty$ no aberto $\R^2\setminus\{0\}$ em $\R^2$. Um cálculo
direto mostra que:
$${\partial\over\partial y}\left(-{y\over
x^2+y^2}\right)={y^2-x^2\over(x^2+y^2)^2}={\partial\over\partial
x}\left({x\over x^2+y^2}\right),$$ e portanto $\omega$ é fechada.

\smallskip

Considere a curva fechada $\gamma:[0,2\pi]\to\R^2$ de classe $C^\infty$
definida por $\gamma(t)=(\cos t,\sen t)$. Observe que $\gamma$ é simplesmente
uma parametrização para o círculo unitário $x^2+y^2=1$ no sentido
anti-horário. Temos:
$$\int_\gamma\omega=\int_0^{2\pi}\sen^2t+\cos^2t\,\dd t=2\pi\ne0,$$
e portanto $\omega$ não pode ser exata.

\smallskip

Vamos agora analisar a $1$-forma $\omega$ de uma maneira mais
geométrica. Começamos com a seguinte:
\proclaim Definição. Se $(x,y)\in\R^2$ é um ponto diferente da origem então um
{\rm ângulo} para $(x,y)$ é um escalar $\theta\in\R$ tal que:
$$x=r\cos\theta,\quad y=r\sen\theta,$$
onde $r=\sqrt{x^2+y^2}$ denota a norma Euclideana de $(x,y)$.

Observe que um
ângulo para $(x,y)$ é simplesmente um valor qualquer que pode ser atribuído à
coordenada polar $\theta$ do ponto $(x,y)$. Observe também que se $\theta$ é
um ângulo para $(x,y)$ então $\{\theta+2k\pi:k\in\Z\}$ é o conjunto de todos
os outros possíveis ângulos para $(x,y)$.

\smallskip

Dado $\theta_0\in\R$, denote por $A_{\theta_0}$ o subconjunto do plano obtido
removendo a semi-reta fechada que contém os pontos com ângulo $\theta_0$;
explicitamente:
$$A_{\theta_0}=\R^2\setminus\big\{(t\cos\theta_0,t\sen\theta_0):t\ge0\big\}.$$
Temos que $A_{\theta_0}$ é um aberto de $\R^2$ e que para todo $(x,y)\in
A_{\theta_0}$ existe um único $\theta\in\R$ pertencente ao
intervalo aberto $(\theta_0,\theta_0+2\pi)$ que seja um ângulo para $(x,y)$;
obtemos então uma função $\theta:A_{\theta_0}\to\R$ que a cada ponto $(x,y)\in
A_{\theta_0}$ associa seu único ângulo no intervalo
$(\theta_0,\theta_0+2\pi)$. Não é difícil ver que $\theta$ é uma função de
classe $C^\infty$ (veja o Exercício~2).

\smallskip

\proclaim Definição. Se $U\subset\R^2$ é um aberto que não contém a origem
então uma {\rm função ângulo} em $U$ é uma função {\rm contínua} $\theta:U\to\R$
tal que, para todo $(x,y)\in U$, $\theta(x,y)\in\R$ é um ângulo
para o ponto $(x,y)$.

Não é difícil mostrar que toda função ângulo num aberto
$U\subset\R^2\setminus\{0\}$ é automaticamente de classe $C^\infty$ (veja o
Exercício~3). Nós mostramos acima que para todo $\theta_0\in\R$, existe
uma função ângulo no aberto $A_{\theta_0}$. Veremos adiante que {\sl não pode
existir uma função ângulo definida em $\R^2\setminus\{0\}$}.

\smallskip

Seja agora $U\subset\R^2\setminus\{0\}$ um aberto onde esteja definida uma
função ângulo $\theta:U\to\R$. Vamos calcular o diferencial de
$\theta$. Considere as funções $f:U\to\R^2$ e $g:\R^2\to\R^2$ de classe
$C^\infty$ definidas por:
$$f(x,y)=\Big(\sqrt{x^2+y^2},\theta(x,y)\Big),\quad
g(r,\vartheta)=(r\cos\vartheta,r\sen\vartheta),$$
para todos $(x,y)\in U$, $(r,\vartheta)\in\R^2$. O fato que $\theta$ é uma
função ângulo em $U$ nos diz que $g\big(f(x,y)\big)=(x,y)$ para todos $(x,y)\in
U$, i.e., que $g\circ f=\Id\vert_U$. Diferenciando essa igualdade num ponto
$(x,y)\in U$ e usando a regra da cadeia obtemos:
$$\dd g\big(f(x,y)\big)\circ\dd f(x,y)=\Id\Longrightarrow\dd f(x,y)=\dd
g\big(f(x,y)\big)^{-1};$$
a matriz Jacobiana de $g$ no ponto $f(x,y)=(r,\vartheta)$ é dada por:
$$\Jac g(r,\vartheta)=\pmatrix{\cos\vartheta&-r\sen\vartheta\cr
\sen\vartheta&r\cos\vartheta},$$
e portanto:
$$\Jac f(x,y)=\big[\Jac g(r,\vartheta)\big]^{-1}={1\over
r}\pmatrix{r\cos\vartheta&r\sen\vartheta\cr
-\sen\vartheta&\cos\vartheta},$$
para todos $(x,y)\in U$, sendo $r=\sqrt{x^2+y^2}$ e
$\vartheta=\theta(x,y)$. Como $x=r\cos\vartheta$ e $y=r\sen\vartheta$,
obtemos:
$$\Jac f(x,y)=\pmatrix{\;\;\displaystyle{x\over r}&\displaystyle{y\over r}\cr
\noalign{\medskip}
\;\;\displaystyle\hbox to 0pt{\hskip 0pt minus 1fil$-$}{y\over
r^2}&\displaystyle{x\over r^2}\cr},$$
para todo $(x,y)\in U$. Mas as derivadas parciais de $\theta$ aparecem na
segunda linha de $\Jac f(x,y)$; isso significa que:
$$\dd\theta(x,y)={\partial\theta\over\partial x}(x,y)\dd
x+{\partial\theta\over\partial y}(x,y)\dd y=-{y\over x^2+y^2}\,\dd x+{x\over
x^2+y^2}\,\dd y=\omega(x,y),$$
i.e., $\dd\theta=\omega$ em $U$!

\smallskip

Os cálculos acima mostraram que a $1$-forma $\omega$, apesar de não ser exata
em $\R^2\setminus\{0\}$, {\sl torna-se exata quando restrita a um aberto
$U\subset\R^2\setminus\{0\}$ onde é possível definir uma função ângulo}! O
fato que $\omega$ não é exata em $\R^2\setminus\{0\}$ implica em particular o
seguinte:

\proclaim Teorema. Não existe uma função ângulo em
$\R^2\setminus\{0\}$.\fimprova

\smallskip

\noindent{\tt Observação}. Apesar do fato que o teorema acima apereceu como um
corolário elegante da nossa teoria, não seria difícil demonstrá-lo
diretamente.

\smallskip

\noindent{\tt Observação}. Em função da igualdade $\dd\theta=\omega$, a
$1$-forma $\omega$ é às vezes chamada de {\sl forma elemento de ângulo no plano}.

\medskip

\item{(2)} {\bf Uma condição suficiente para que uma ${\bf 1}$-forma fechada
seja exata}.

\smallskip

Como vimos na seção anterior, é possível que uma $1$-forma fechada não seja
exata. No entanto, no exemplo estudado, foi possível tornar exata a $1$-forma
fechada em questão por uma simples restrição do domínio. Essa é na verdade a
situação geral. Vamos agora estudar condições suficientes para o domínio de
uma $1$-forma de modo que a implicação ``fechada $\Rightarrow$ exata'' seja
verdadeira.

\smallskip

\proclaim Definição. Um subconjunto $S\subset\R^n$ é dito {\rm estrelado} em
torno de um ponto $x_0\in S$ se para todo $x\in S$ o segmento de reta
$[x_0,x]$ está contido em $S$.

Observe que um subconjunto $S\subset\R^n$ é convexo se e somente se for
estrelado em torno de qualquer um de seus pontos.

\smallskip

\proclaim Teorema. Seja $U\subset\R^n$ um subconjunto aberto, estrelado em
torno de algum $x_0\in U$. Então toda $1$-forma fechada em $U$ (de classe
$C^1$) é exata.

\Prova Seja $\omega:U\to{\R^n}^*$ uma $1$-forma fechada de classe
$C^1$ e escreva $\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$, de modo que
cada $a_i:U\to\R$ é uma função de classe $C^1$ e ${\partial
a_i\over\partial x_j}={\partial a_j\over\partial x_i}$ para todos
$i,j=1,\ldots,n$. Defina uma função $f:U\to\R$ pela fórmula:
$$f(x)=\int_0^1\omega\big(x_0+t(x-x_0)\big)\cdot(x-x_0)\,\dd
t=\sum_{i=1}^n\int_0^1a_i\big(x_0+t(x-x_0)\big)\big(x_i-(x_0)_i\big)\,\dd
t,$$ para todo $x\in U$. Observe que a fórmula acima só faz
sentido pois $x_0+t(x-x_0)\in U$ para todo $t\in[0,1]$!

Dado agora
$j=1,\ldots,n$, calculamos a derivada parcial ${\partial
f\over\partial x_j}(x)$ usando a regra usual para derivação sob o
sinal de integral (veja Exercício~5):
$$\eqalign{{\partial f\over\partial
x_j}(x)&=\int_0^1a_j\big(x_0+t(x-x_0)\big)\,\dd
t+\sum_{i=1}^n\int_0^1\!t\,{\partial a_i\over\partial
x_j}\big(x_0+t(x-x_0)\big)\big(x_i-(x_0)_i\big)\,\dd t\cr
&=\int_0^1a_j\big(x_0+t(x-x_0)\big)\,\dd
t+\sum_{i=1}^n\int_0^1\!t\,{\partial a_j\over\partial
x_i}\big(x_0+t(x-x_0)\big)\big(x_i-(x_0)_i\big)\,\dd t.\cr}$$ Um
cálculo direto mostra agora que:
$${\partial f\over\partial x_j}(x)=\int_0^1{\dd\over\dd
t}\Big[ta_j\big(x_0+t(x-x_0)\big)\Big]\,\dd t;$$ aplicando o
Teorema Fundamental do Cálculo obtemos então ${\partial
f\over\partial x_j}(x)=a_j(x)$, para todo $j=1,\ldots,n$ e
portanto $\dd f=\omega$.\fimprova

\bigskip

\noindent{\tt Observação}. {\bf Será necessário cobrir agora o material sobre
o número de Lebesgue de uma cobertura, originalmente destinado à Aula número 7
(27/03), seção 2}.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Funções ângulo}.

\medskip

\item{0.} Seja $M$ um espaço métrico conexo e $N$ um espaço
métrico discreto (recorde que um espaço métrico é dito {\sl
discreto\/} quando todos os seus pontos são abertos). Mostre que
toda função contínua $f:M\to N$ é constante ({\sl dica}: se $y\in
N$ então $f^{-1}(y)\subset M$ é aberto e fechado).

\smallskip

\item{1.} Seja $U\subset\R^2$ um aberto conexo que não contém a
origem. Mostre que se $\theta_1,\theta_2:U\to\R$ são funções
ângulo em $U$ então existe $k\in\Z$ com
$\theta_1(x)=\theta_2(x)+2k\pi$ para todo $x\in U$ ({\sl dica}: a
função contínua $U\ni
x\mapsto{1\over2\pi}(\theta_1-\theta_2)(x)\in\R$ toma valores em
$\Z$; use o Exercício~0).

\smallskip

\item{2.} Seja $\theta_0\in\R$ e defina $\theta:A_{\theta_0}\to\R$
de modo que $\theta(x,y)$ é o único ângulo para o ponto $(x,y)$
que pertence ao intervalo aberto $(\theta_0,\theta_0+2\pi)$.
Mostre que $\theta$ é de classe $C^\infty$ ({\sl dica}: use o fato
que as funções trigonométricas inversas $\arccos:(-1,1)\to(0,\pi)$
e $\arcsen:(-1,1)\to\big({-{\pi\over2}},{\pi\over2}\big)$ são de
classe $C^\infty$; faça uma análise exaustiva de casos para
mostrar que $A_{\theta_0}$ pode ser coberto por setores abertos
onde $\theta(x,y)$ é da forma
$2k\pi\pm\arccos\Big(\raise3pt\hbox{${x\over\sqrt{x^2+y^2}}$}\Big)$
ou
$k\pi\pm\arcsen\Big(\raise3pt\hbox{${y\over\sqrt{x^2+y^2}}$}\Big)$,
$k\in\Z$).

\smallskip

\item{3.} Seja $U\subset\R^2$ um aberto que não contém a origem.
Mostre que toda função ângulo $\theta:U\to\R$ é de classe
$C^\infty$ ({\sl dica}: dado $x\in U$, escolha $\theta_0\in\R$ com
$x\in A_{\theta_0}$ e seja $V$ uma vizinhança aberta e conexa de
$x$ contida em $U\cap A_{\theta_0}$; considere a única função
ângulo $\widehat\theta:A_{\theta_0}\to\R$ que toma valores em
$(\theta_0,\theta_0+2\pi)$ e estude a diferença
$\theta-\widehat\theta$ em $V$, tendo em mente os Exercícios~1 e
2).

\medskip

\noindent{\tt Propriedades básicas da integral de Riemann}.

\smallskip

\item{4.} Sejam $M$ um espaço métrico e $f:[a,b]\times M\to\R$ uma função
contínua. Defina $F:M\to\R$ pela integral:
$$F(x)=\int_a^bF(t,x)\,\dd t.$$
Mostre que $F$ é contínua ({\sl dica}: estime $\big\vert
F(t,x)-F(t,x_0)\big\vert$ usando o fato que {\sl a continuidade de $f$ é
uniforme com respeito à variável $t$}, i.e., use o Exercício~18 da Aula número
7 --- 27/03).

\smallskip

\item{5.} ({\sl derivação sob o sinal de integral}) Sejam $I\subset\R$ um intervalo
e
$$[a,b]\times I\ni(t,s)\longmapsto
f(t,s)\in\R$$ uma função derivável com respeito à variável $s\in I$ e
tal que a derivada parcial ${\partial f\over\partial s}:[a,b]\times I\to\R$
seja contínua. Suponha que para todo $s\in I$ a função $t\mapsto f(t,s)$ seja
Riemann integrável em $[a,b]$ e defina $F:I\to\R$ pela integral:
$$F(s)=\int_a^bf(t,s)\,\dd t.$$
Mostre que $F$ é de classe $C^1$ e que $F'(s)=\int_a^b{\partial f\over\partial
s}(t,s)\,\dd t$ ({\sl dica}: estime o valor de:
$$\left\vert{F(s+h)-F(s)\over h}-\int_a^b{\partial f\over\partial s}(t,s)\,\dd
t\right\vert,$$
usando o teorema do valor médio para concluir que: $$f(t,s+h)-f(t,s)=h{\partial
f\over\partial s}\big(t,s+\theta(t)h\big),$$ com $0<\theta(t)<1$. Como no
Exercício~4 acima, use o fato que a continuidade de ${\partial f\over\partial
s}$ é uniforme com respeito à variável $t\in[a,b]$).

\vfil\eject

\centerline{\bf Aula número $20$ (22/05)}
\bigskip
\bigskip

A aula começa cobrindo todo o material originalmente destinado à
aula número 19 (17/05).

\medskip

\noindent{\tt Observação}. Antes de mais nada, vamos fazer uma
comparação entre a teoria de campos vetoriais e a teoria de
$1$-formas em abertos de $\R^n$. Sejam então $U\subset\R^n$ um
aberto, $\omega(x)=\sum_{i=1}^na_i(x)\dd x_i$ uma $1$-forma
contínua e $X(x)=\sum_{i=1}^na_i(x)e_i$ o campo vetorial
(contínuo) correspondente. Como já observamos, $\omega$ é exata se
e somente se o campo $X$ é {\sl conservativo\/} --- uma função
$f:U\to\R$ de classe $C^1$ satisfaz $\dd f=\omega$ se e somente se
$\nabla f=X$, i.e., se e somente se $f$ é um {\sl potencial\/}
para $X$. Suponha agora $\omega$ (e portanto $X$) de classe $C^1$.
A qual condição para $X$ corresponde a condição ``$\omega$ é
fechada''? No caso $n=3$ (ou $n=2$), define-se nos cursos
elementares de Cálculo o {\sl rotacional\/} de um campo vetorial
--- é imediato então verificar que $\omega$ é fechada se e somente se
$X$ possui {\sl rotacional zero}. Para quem estudou a teoria de
campos conservativos em $\R^3$, o teorema ``$\omega$ exata
$\Rightarrow$ $\omega$ fechada'' é mais familiar sob a forma ``$X$
conservativo $\Rightarrow$ ${\rm rot}\,X=0$''. A recíproca dessa
última implicação não vale (como vimos na aula número 19); a
obstrução para a validade dessa recíproca é enunciada
informalmente às vezes como a ``presença de buracos'' no domínio
do campo (ou, equivalentemente, da $1$-forma associada). Nosso
objetivo na seção que segue é estudar a condição de ``ausência de
buracos'' de maneira matemática formal --- isso é feito através do
conceito de {\sl homotopia}. Mostraremos que para ``domínios sem
buracos'' (chamados de {\sl simplesmente conexos}) a recíproca
``$\omega$ fechada $\Rightarrow$ $\omega$ exata'' é verdadeira. A
demonstração dessa recíproca já foi feita anteriormente para
abertos convexos (e, mais geralmente, para abertos estrelados);
tal resultado foi apenas uma preparação para o que será feito a
seguir, que é muito mais geral. Para terminar esta (longa)
observação, mencionamos que o conceito de rotacional para campos
vetoriais é típico de $\R^3$ e não faz sentido em $\R^n$, para
$n>3$; em tal caso, a condição ``$\omega$ é fechada'' significa
simplesmente ${\partial a_i\over\partial x_j}={\partial
a_j\over\partial x_i}$, nada mais. Adiante, quando estudarmos {\sl
$k$-formas diferenciais}, definiremos a noção de {\sl diferencial
exterior de uma forma diferencial}. Esse conceito generalizará os
operadores rotacional, gradiente e divergente, estudados em cursos
de Cálculo Vetorial; daí uma forma fechada será definida como uma
{\sl forma com diferencial exterior zero}.

\medskip

\item{(1)} {\bf A noção de homotopia}.

\smallskip

A partir de agora {\sl todas as curvas consideradas serão
assumidas contínuas}; mais explicitamente, uma {\sl curva\/} num
espaço métrico $M$ significará uma {\sl aplicação contínua\/}
$\gamma:[a,b]\to M$ definida em algum intervalo fechado $[a,b]$.

\smallskip

\proclaim Definição. Sejam $M$ um espaço métrico e
$\gamma,\mu:[a,b]\to M$ curvas em $M$. Uma {\rm homotopia} de
$\gamma$ para $\mu$ é uma aplicação contínua
$H:[a,b]\times[0,1]\to M$ tal que:
$$H(t,0)=\gamma(t)\quad\hbox{e}\quad H(t,1)=\mu(t),$$
para todo $t\in[a,b]$. Quando vale também a
condição:
$$H(a,s)=H(a,0)\quad\hbox{e}\quad H(b,s)=H(b,0),$$
para todo $s\in[0,1]$ então dizemos que $H$ é uma {\rm homotopia
com extremos fixos} de $\gamma$ para $\mu$. Se existe uma
homotopia de $\gamma$ para $\mu$, dizemos que $\gamma$ e $\mu$ são
{\rm homotópicas}; quando existe uma homotopia com extremos fixos
de $\gamma$ para $\mu$, dizemos que $\gamma$ e $\mu$ são {\rm
homotópicas com extremos fixos}.

\smallskip

\noindent{\tt Observação}. A definição acima, assim como toda a
teoria de homotopia que apresentaremos, normalmente é estudada no
contexto de espaços topológicos --- a presença de uma métrica é
totalmente irrelevante. Neste curso, nós nos restringimos ao caso
de espaços métricos para facilitar a vida daqueles que não tem
familiaridade com topologia.

\smallskip

Se $H:[a,b]\times[0,1]\to M$ é uma aplicação contínua então, dado
$s\in[0,1]$, é comum denotar por $H_s:[a,b]\to M$ a aplicação
$H(\cdot,s)$, i.e., $H_s(t)=H(t,s)$ para todo $t\in[a,b]$. Se $H$
é uma homotopia de $\gamma$ para $\mu$ então, para cada
$s\in[0,1]$, $H_s$ é uma curva em $M$, $H_0=\gamma$ e $H_1=\mu$.
Pensa-se então no parâmetro $s$ como sendo o ``tempo''; em cada
instante $s\in[0,1]$ temos uma curva $H_s$ em $M$ que ``varia
continuamente'' quando $s$ cresce de $0$ a $1$. No instante
inicial $s=0$, temos a curva $\gamma$; no instante final $s=1$,
temos a curva $\mu$. Uma homotopia de $\gamma$ para $\mu$
significa então uma ``deformação contínua'' de $\gamma$ até $\mu$
dentro do espaço $M$. Quando dizemos que a {\sl homotopia tem
extremos fixos}, queremos dizer justamente isso: durante a
deformação as extremidades da curva sendo deformada permanecem
fixas. Obviamente, para que $\gamma$ e $\mu$ possam ser
homotópicas com extremos fixos, o pré-requisito básico é que
$\gamma$ e $\mu$ {\sl tenham os mesmos extremos}, i.e., que
$\gamma(a)=\mu(a)$ e $\gamma(b)=\mu(b)$.

\smallskip

A seguinte variação da noção de homotopia também é importante.
\proclaim Definição. Um {\rm laço} no espaço métrico $M$ é uma
curva fechada em $M$, i.e., uma curva $\gamma:[a,b]\to M$ com
$\gamma(a)=\gamma(b)$. Se $\gamma,\mu:[a,b]\to M$ são laços então
uma homotopia $H:[a,b]\times[0,1]\to M$ de $\gamma$ para $\mu$ é
chamada uma {\rm homotopia de laços} se a curva $H_s$ é um laço em
$M$ para todo $s\in[0,1]$, i.e., se $H(a,s)=H(b,s)$ para todo
$s\in[0,1]$. Quando existe uma homotopia de laços de $\gamma$ para
$\mu$ dizemos que $\gamma$ e $\mu$ são {\rm homotópicas como
laços}.

Obviamente para que curvas $\gamma$ e $\mu$ sejam homotópicas como
laços, o pré-requisito básico é que as mesmas {\sl sejam laços}.
Observe que uma homotopia com extremos fixos entre dois laços é
uma homotopia de laços, mas uma homotopia de laços não é em geral
uma homotopia com extremos fixos (os Exercícios~7 e 8 relacionam
as duas noções de homotopia para laços).

\smallskip

\noindent{\tt Exemplo}. Seja $M$ um subconjunto convexo de $\R^n$
(munido da métrica induzida de $\R^n$). Se $\gamma,\mu:[a,b]\to M$
são curvas então a expressão:
$$H(t,s)=(1-s)\gamma(t)+s\mu(t),\quad t\in[a,b],\ s\in[0,1],$$
define uma homotopia $H:[a,b]\times[0,1]\to M$ de $\gamma$ para
$\mu$. Se $\gamma$ e $\mu$ têm os mesmos extremos então $H$ é uma
homotopia com extremos fixos; se $\gamma$ e $\mu$ são laços então
$H$ é uma homotopia de laços. Observe que a homotopia $H$ deforma
$\gamma$ para $\mu$ de modo que o ponto $\gamma(t)$ é levado para
o ponto $\mu(t)$ ao longo do segmento de reta
$[\gamma(t),\mu(t)]\subset M$.

\goodbreak
\smallskip

\proclaim Definição. Um espaço métrico $M$ é dito {\rm
simplesmente conexo} quando qualquer laço \hbox{$\gamma:[a,b]\to
M$} é homotópico como laço a uma aplicação constante, i.e., se
existe uma aplicação contínua $H:[a,b]\times[0,1]\to M$ com
$H(t,0)=\gamma(t)$, $H(a,s)=H(b,s)$ e $H(t,1)=H(a,1)$ para todos
$t\in[a,b]$, $s\in[0,1]$.

\smallskip

\noindent{\tt Exemplo}. O exemplo acima mostrou que todo
subconjunto convexo de $\R^n$ é simplesmente conexo. Se
$M\subset\R^n$ é estrelado em torno de algum ponto $x_0\in M$
então $M$ é simplesmente conexo também: de fato, se
$\gamma:[a,b]\to M$ é um laço então $H(t,s)=(1-s)\gamma(t)+sx_0$,
$s\in[0,1]$, $t\in[a,b]$, define uma homotopia de laços de
$\gamma$ até o laço constante e igual a $x_0$.

\smallskip

\noindent{\tt Exemplo}. É fácil ver que todo espaço métrico
homeomorfo a um espaço métrico simplesmente conexo é ainda
simplesmente conexo (veja Exercício~6). Em particular, se um
espaço métrico $M$ é homeomorfo a um subconjunto estrelado de
$\R^n$ então $M$ é simplesmente conexo.

\smallskip

\noindent{\tt Exemplo}. Se $M=\R^2\setminus\{0\}$ é o ``plano
furado'' então $M$ não é simplesmente conexo: o laço $[0,2\pi]\ni
t\mapsto\gamma(t)=(\cos t,\sen t)\in M$ não é homotópico como laço
a uma aplicação constante. Intuitivamente, isso ocorre porque
qualquer tentativa de deformar $\gamma$ até uma constante (sem
``arrebentar'' o laço) deve necessariamente passar pelo ``buraco''
que é a origem. Apesar do apelo intuitivo desse argumento, a
demonstração formal do fato que $\gamma$ não é homotópica como
laço a uma constante não é tão simples; tal resultado será obtido
como um corolário do teorema central da próxima seção.

\smallskip

\noindent{\tt Observação}. A noção de homotopia (sem restrições
adicionais como ``extremos fixos'' ou ``homotopia de laços'') no
contexto de curvas $\gamma:[a,b]\to M$ é muito pouco interessante:
de fato,
$[a,b]\times[0,1]\ni(t,s)\mapsto\gamma\big((1-s)t+sa\big)\in M$
define uma homotopia de $\gamma$ até uma aplicação constante, seja
lá qual for a curva $\gamma$ ou o espaço métrico $M$. Em geral, a
noção de homotopia é definida para aplicações entre espaços
topológicos quaisquer, e daí a noção de homotopia (sem restrições)
passa a ser interessante.

\medskip

\item{(2)} {\bf Integração de formas fechadas e homotopia de
curvas}.

\smallskip

O objetivo desta seção é provar o seguinte: \proclaim Teorema.
Sejam $U\subset\R^n$ um aberto e $\omega:U\to{\R^n}^*$ uma
$1$-forma fechada (de classe $C^1$). Dadas curvas
$\gamma,\mu:[a,b]\to\R^n$ de classe $C^1$ por partes, ambas com
imagem contida em $U$, suponha que vale ao menos uma das duas
seguintes condições:
\smallskip
\itemitem{\rm(i)} existe uma homotopia com extremos fixos
$H:[a,b]\times[0,1]\to U$ de $\gamma:[a,b]\to U$ para
$\mu:[a,b]\to U$ (neste caso estamos supondo em particular que
$\gamma$ e $\mu$ têm os mesmos extremos, obviamente);
\itemitem{\rm(ii)} existe uma homotopia de laços
$H:[a,b]\times[0,1]\to U$  de $\gamma:[a,b]\to U$ para
$\mu:[a,b]\to U$ (neste caso estamos supondo em particular que
$\gamma$ e $\mu$ são laços, obviamente).
\smallskip
Então $\int_\gamma\omega=\int_\mu\omega$.

\smallskip

O teorema acima implica facilmente o seguinte:
\proclaim Corolário.
Se $U\subset\R^n$ é um aberto simplesmente conexo então toda
$1$-forma fechada (de classe $C^1$) em $U$ é exata.

\Prova Se $\gamma:[a,b]\to\R^n$ é uma curva fechada (i.e., um
laço) de classe $C^1$ por partes com imagem contida em $U$ então
$\gamma$ é homotópica como laço em $U$ a uma curva constante
$\mu$. Como $\omega$ é fechada, temos
$\int_\gamma\omega=\int_\mu\omega=0$ e portanto a integral de
$\omega$ ao longo de qualquer curva fechada em $U$ de classe $C^1$
por partes é nula. Segue que $\omega$ é exata.\fimprova

\smallskip

\proclaim Corolário. Se $\omega:U\to{\R^n}^*$ é uma $1$-forma
fechada (de classe $C^1$) num aberto $U\subset\R^n$ então, se um
laço $\gamma:[a,b]\to U$ de classe $C^1$ por partes é homotópico
como laço em $U$ a uma aplicação constante então
$\int_\gamma\omega=0$.\fimprova

\smallskip

\proclaim Corolário. O laço $\gamma:[0,2\pi]\to\R^2\setminus\{0\}$
definido por $\gamma(t)=(\cos t,\sen t)$ não é homotópico como
laço a uma aplicação constante. Em particular, o ``plano furado''
$\R^2\setminus\{0\}$ não é simplesmente conexo.

\Prova A $1$-forma $\omega(x,y)={1\over x^2+y^2}(-y\dd x+x\dd y)$
em $U$ é fechada mas $\int_\gamma\omega\ne0$.\fimprova

\smallskip

Para provar o teorema principal será útil o seguinte: \proclaim
Lema. Sejam $K$ um espaço métrico compacto e $M$ um espaço métrico
qualquer. Dada uma função contínua $f:K\to M$ e uma cobertura
aberta $M=\bigcup_{i\in I}V_i$ de $M$ então existe $\delta>0$ com
a seguinte propriedade: dado um subconjunto $S\subset K$ de
diâmetro menor que $\delta$ então $f(S)\subset V_i$ para algum
$i\in I$.

\Prova Se $U_i=f^{-1}(V_i)$ então $K=\bigcup_{i\in I}U_i$ é uma
cobertura aberta do espaço métrico compacto $K$ e portanto a mesma
possui um número de Lebesgue $\delta>0$. Daí, se $S\subset K$ tem
diâmetro menor que $\delta$ então existe $i\in I$ com $S\subset
U_i=f^{-1}(V_i)$; daí $f(S)\subset V_i$.\fimprova

\smallskip

Antes de demonstrar o teorema, vamos a uma:

\smallskip

\noindent{\tt Explicação informal da prova do teorema}.\enspace
Seja $H:[a,b]\times[0,1]\to U$ uma homotopia de $\gamma$ para
$\mu$ (com extremos fixos ou uma homotopia de laços). Particione o
retângulo $[a,b]\times[0,1]$ em retângulos menores que são
mapeados por $H$ em bolas abertas contidas em $U$; obtemos então
uma ``grade'' em $[a,b]\times[0,1]$ formada por retângulos
pequenos. A cada segmento vertical $\frak v$ da grade fazemos
corresponder uma curva em $\R^n$ que parametriza o segmento cujas
extremidades são as imagens por $H$ das extremidades de $\frak v$.
Para cada segmento horizontal $\frak h$ da grade contido no lado
$[a,b]\times\{0\}$ ou no lado $[a,b]\times\{1\}$ do retângulo
$[a,b]\times[0,1]$, fazemos corresponder a curva dada pela
restrição de $H$ a $\frak h$. Se $\frak h$ é um segmento
horizontal da grade que não está contido nem no lado
$[a,b]\times\{0\}$ nem no lado $[a,b]\times\{1\}$ do retângulo
$[a,b]\times[0,1]$ então fazemos corresponder a $\frak h$ uma
curva em $\R^n$ que parametriza o segmento cujas extremidades são
as imagens por $H$ das extremidades de $\frak h$. Fizemos então
corresponder à grade de linhas retas em $[a,b]\times[0,1]$ uma
``grade'' de linhas poligonais em $U$ --- exceto pelos lados
$[a,b]\times\{0\}$ e $[a,b]\times\{1\}$ de $[a,b]\times[0,1]$, que
correspondem às curvas originais $\gamma$ e $\mu$. Vamos denotar
por $\rho$ e $\tilde\rho$ as linhas poligonais que correspondem
respectivamente aos lados $\{a\}\times[0,1]$ e $\{b\}\times[0,1]$
do retângulo $[a,b]\times[0,1]$. A integral de $\omega$ ao longo
do laço correspondente à fronteira do retângulo $[a,b]\times[0,1]$
percorrida no sentido anti-horário é igual à soma das integrais de
$\omega$ sobre os laços correspondentes aos retângulos menores da
grade percorridos no sentido anti-horário, pois ``lados comuns a
dois retângulos da grade são percorridos em sentidos opostos'' e
portanto cancelam. Por outro lado, como os laços correspondentes a
tais retângulos pequenos estão contidos em bolas contidas em $U$
(nas quais $\omega$ é exata), então a integral de $\omega$ nos
laços correspondentes a tais retângulos pequenos é nula. Obtemos
então:
$$\int_\gamma\omega+\int_{\tilde\rho}\omega-\int_\mu\omega-\int_\rho\omega=0;$$
se $H$ é uma homotopia com extremos fixos então $\rho$ e
$\tilde\rho$ são constantes. Se $H$ é uma homotopia de laços então
$\rho=\tilde\rho$. Em qualquer caso, a conclusão desejada é
obtida.\fimprova

\smallskip

Para escrever a demonstração do teorema de maneira matematicamente
mais precisa, vamos introduzir um pouco de notação.

\smallskip

\proclaim Definição. Sejam $M$ um espaço métrico e
$\gamma:[a,b]\to M$ uma curva. A curva $\tilde\gamma:[0,1]\to M$
definida por:
$$\tilde\gamma(t)=\gamma\big((1-t)a+tb\big),\quad t\in[0,1],$$
é chamada a {\rm reparametrização afim crescente} de $\gamma$
(definida no intervalo $[0,1]$).

Como integrais de linha são invariantes por reparametrizações
crescentes, em vista da definição acima, não há perda de
generalidade em estudar apenas curvas definidas no intervalo
$[0,1]$ (veja também o Exercício~2).

\smallskip

\proclaim Definição. Sejam $M$ um espaço métrico e
$\gamma:[0,1]\to M$ uma curva. A {\rm reparametrização reversa} de
$\gamma$ é a curva $\gamma^{-1}:[0,1]\to M$ definida por:
$$\gamma^{-1}(t)=\gamma(1-t),\quad t\in[0,1].$$
Se $\mu:[0,1]\to M$ é uma outra curva com $\gamma(1)=\mu(0)$ então
a {\rm concatenação} de $\gamma$ e $\mu$ é a curva
$(\gamma\cdot\mu):[0,1]\to M$ definida por:
$$(\gamma\cdot\mu)(t)=\cases{\gamma(2t),&$t\in\big[0,{\textstyle{1\over2}}\big],$\cr
\noalign{\smallskip}
\mu(2t-1),&$t\in\big[{\textstyle{1\over2}},1\big].$\cr}$$

Obviamente a continuidade de $\gamma$ implica na continuidade de
$\gamma^{-1}$; é fácil ver também que a continuidade de $\gamma$ e
de $\mu$ implicam na continuidade de $\gamma\cdot\mu$ (recorde
item~(a), Exercício~8, Aula número 7 --- 27/03).

\smallskip

A concatenação de curvas {\sl não\/} é uma operação associativa: a
curva $(\gamma\cdot\mu)\cdot\lambda$ corresponde a ``$\gamma$ e
$\mu$ percorridas com a velocidade quadruplicada seguidas de
$\lambda$ percorrida com a velocidade dobrada''; por outro lado, a
curva $\gamma\cdot(\mu\cdot\lambda)$ corresponde a ``$\gamma$
percorrida com a velocidade dobrada seguida de $\mu$ e $\lambda$
percorridas com a velocidade quadruplicada''. É verdade, no
entanto, que a diferença entre $(\gamma\cdot\mu)\cdot\lambda$ e
$\gamma\cdot(\mu\cdot\lambda)$ é apenas uma questão de
parametrização (veja o Exercício~5 (a)). Para facilitar a notação
(economizando parênteses), vamos convencionar que
$\gamma_1\cdot\gamma_2\cdots\gamma_n$ significa
$(\cdots(\gamma_1\cdot\gamma_2)\cdots)\cdot\gamma_n)$.

\goodbreak
\smallskip

\noindent{\bf Prova do Teorema}.\enspace Sejam $U\subset\R^n$ um
aberto, $\omega:U\to{\R^n}^*$ uma $1$-forma fechada (de classe
$C^1$) e $\gamma,\mu:[a,b]\to\R^n$ curvas de classe $C^1$ por
partes com imagens contidas em $U$. Suponha que exista uma
homotopia $H:[a,b]\times[0,1]\to U$ de $\gamma$ para $\mu$ que
seja ou uma homotopia com extremos fixos ou uma homotopia de
laços.

Como $\omega$ é fechada, sabemos que a restrição de $\omega$ a
qualquer bola aberta contida em $U$ é exata, pois bolas são
convexas. Como $U$ pode ser escrito como uma união de bolas
abertas, o lema acima implica que existe $\delta>0$ tal que para
todo subconjunto $S\subset[a,b]\times[0,1]$ com diâmetro menor que
$\delta$ temos que $H(S)\subset U$ está contido em alguma bola
aberta contida em $U$. Escolha agora partições
$a=t_0<t_1<\cdots<t_k=b$ e $0=s_0<s_1<\cdots<s_l=1$ dos intervalos
$[a,b]$ e $[0,1]$ respectivamente, de modo que cada retângulo
$[t_i,t_{i+1}]\times[s_j,s_{j+1}]$, $i=0,\ldots,k-1$,
$j=0,\ldots,l-1$, tenha diâmetro menor que $\delta$; em
particular, $H\big([t_i,t_{i+1}]\times[s_j,s_{j+1}]\big)$ está
contido em alguma bola aberta contida em $U$.

Para cada $i=0,\ldots,k-1$ e cada $j=1,\ldots,l-1$, denote por
$\frak h_{ij}:[0,1]\to\R^n$ a curva $\frak
h_{ij}(u)=(1-u)H(t_i,s_j)+uH(t_{i+1},s_j)$, $u\in[0,1]$, i.e.,
$\frak h_{ij}$ é uma parametrização afim do segmento ligando
$H(t_i,s_j)$ a $H(t_{i+1},s_j)$. Para $i=0,\ldots,k-1$, $j=0$,
definimos $\frak h_{ij}:[0,1]\to\R^n$ como sendo a
reparametrização afim crescente de $\gamma\vert_{[t_i,t_{i+1}]}$;
similarmente, para $i=0,\ldots,k-1$, $j=l$, definimos $\frak
h_{ij}:[0,1]\to\R^n$ como sendo a reparametrização afim crescente
de $\mu\vert_{[t_i,t_{i+1}]}$. Para $i=0,\ldots,k$,
$j=0,\ldots,l-1$, definimos $\frak v_{ij}:[0,1]\to\R^n$ como sendo
uma parametrização afim do segmento ligando $H(t_i,s_j)$ a
$H(t_i,s_{j+1})$, i.e., $\frak
v_{ij}(u)=(1-u)H(t_i,s_j)+uH(t_i,s_{j+1})$, $u\in[0,1]$. Defina
curvas $\rho,\tilde\rho:[0,1]\to\R^n$ fazendo:
$$\rho=\frak v_{00}\cdot\frak v_{01}\cdots\frak v_{0(l-1)},\quad
\tilde\rho=\frak v_{k0}\cdot\frak v_{k1}\cdots\frak v_{k(l-1)}.$$

Para cada $i=0,\ldots,k-1$, $j=0,\ldots,l-1$, considere a curva
$R_{ij}:[0,1]\to\R^n$ definida por:
$$R_{ij}=\frak h_{ij}\cdot\frak v_{(i+1)j}\cdot\frak
(h_{i(j+1)})^{-1}\cdot(\frak v_{ij})^{-1}.$$ Temos que $R_{ij}$ é
uma curva fechada de classe $C^1$ por partes cuja imagem está
contida numa bola aberta contida em $U$; como $\omega$ é exata
nessa bola, temos que $\int_{R_{ij}}\omega=0$ para todos
$i=0,\ldots,k-1$, $j=0,\ldots,l-1$. Além do mais:
$$\eqalign{0=\sum_{i=0}^{k-1}\sum_{j=0}^{l-1}\int_{R_{ij}}\omega&=\sum_{i=0}^{k-1}\int_{\frak h_{i0}}\omega
+\sum_{j=0}^{l-1}\int_{\frak
v_{kj}}\omega-\sum_{i=0}^{k-1}\int_{\frak
h_{il}}\omega-\sum_{j=0}^{l-1}\int_{\frak v_{0j}}\omega\cr
&=\int_\gamma\omega+\int_{\tilde\rho}\omega-\int_\mu\omega-\int_\rho\omega.\cr}$$
Se $H$ é uma homotopia com extremos fixos então $\rho$ e
$\tilde\rho$ são curvas constantes; se $H$ é uma homotopia de
laços então $\rho=\tilde\rho$. Em qualquer caso, temos
$\int_{\tilde\rho}\omega-\int_\rho\omega=0$ e portanto
$\int_\gamma\omega=\int_\mu\omega$.\fimprova

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Curvas e homotopia}.

\medskip

\item{1.} Dado um espaço métrico $M$, mostre que as relações
``$\gamma$ é homotópica a $\mu$ com extremos fixos'' e ``$\gamma$
é homotópica a $\mu$ como laço'' definem relações de equivalência
respectivamente no conjunto de todas as curvas $\gamma:[a,b]\to M$
e no conjunto de todos os laços $\gamma:[a,b]\to M$ [{\sl dica}: a parte
menos fácil é a transitividade. Para demonstrá-la, observe que se
$H,\widetilde H:[a,b]\times[0,1]\to M$ são aplicações contínuas
com $H(t,1)=\widetilde H(t,0)$ para todo $t\in[a,b]$ então a
fórmula:
$$K(t,s)=\cases{H(t,2s),&$s\in\big[0,{1\over2}\big]$,\cr
\noalign{\smallskip} \widetilde
H(t,2s-1),&$s\in\big[{1\over2},1\big]$,\cr}$$ $t\in[a,b]$, define
uma aplicação contínua $K:[a,b]\times[0,1]\to M$. Use o
Exercício~38 da aula número 5 (20/03) para mostrar que $K$ é de fato contínua).

\smallskip

\item{2.} Sejam $M$ um espaço métrico e $\gamma,\mu:[a,b]\to M$
curvas. Sejam $\tilde\gamma,\tilde\mu:[0,1]\to M$ as
reparametrizações afins crescentes de $\gamma$ e $\mu$
respectivamente. Mostre que:
\smallskip
\itemitem{(a)} $\gamma$ é homotópica a $\mu$ com extremos fixos se
e somente se $\tilde\gamma$ é homotópica a $\tilde\mu$ com
extremos fixos;
\itemitem{(b)} $\gamma$ é homotópica a $\mu$ como laço se e
somente se $\tilde\gamma$ é homotópica a $\tilde\mu$ como laço.

\smallskip

\item{3.} Sejam $M$ um espaço métrico e $\gamma,\mu:[0,1]\to M$
curvas com $\gamma(1)=\mu(0)$. Mostre que:
\smallskip
\itemitem{(a)} se $\tilde\gamma:[0,1]\to M$ é homotópica a
$\gamma$ com extremos fixos então a concatenação
$\tilde\gamma\cdot\mu$ é homotópica à $\gamma\cdot\mu$ com
extremos fixos;
\itemitem{(b)} se $\tilde\mu:[0,1]\to M$ é homotópica a $\mu$ com
extremos fixos então a concatenação $\gamma\cdot\tilde\mu$ é
homotópica a $\gamma\cdot\mu$ com extremos fixos;
\itemitem{(c)} se $\tilde\gamma:[0,1]\to M$ é homotópica a
$\gamma$ com extremos fixos então $\tilde\gamma^{-1}$ é homotópica
a $\gamma^{-1}$ com extremos fixos.
\smallskip
\noindent({\sl dica}: é fácil construir as homotopias; a única
coisa não trivial é verificar sua continuidade. Para isso, use o
Exercício~38 da aula número 5 --- 20/03).

\smallskip

\item{4.} Sejam $M$ um espaço métrico, $\gamma:[0,1]\to M$ uma
curva e $\sigma:[0,1]\to[0,1]$ uma aplicação contínua. Mostre que:
\smallskip
\itemitem{(a)} se $\sigma(0)=0$ e $\sigma(1)=1$ então
$\gamma\circ\sigma$ é homotópica a $\gamma$ com extremos fixos;
\itemitem{(b)} se $\sigma(0)=\sigma(1)$ então $\gamma\circ\sigma$ é
homotópica com extremos fixos a uma curva constante.
\smallskip
\noindent({\sl dica}: olhe para
$\gamma\big((1-s)t+s\sigma(t)\big)$ no item (a) e para
$\gamma\big((1-s)\sigma(t)+s\sigma(0)\big)$ no item (b)).

\smallskip

\item{5.} Sejam $M$ um espaço métrico e
$\gamma,\mu,\lambda:[0,1]\to M$ curvas com $\gamma(1)=\mu(0)$ e
$\mu(1)=\lambda(0)$.
\smallskip
\itemitem{(a)} Determine uma aplicação contínua
$\sigma:[0,1]\to[0,1]$ tal que
$\big[(\gamma\cdot\mu)\cdot\lambda\big]\circ\sigma=\gamma\cdot(\mu\cdot\lambda)$
e $\sigma(0)=0$, $\sigma(1)=1$ ({\sl dica}: é possível escolher
$\sigma$ {\sl afim por partes}).
\itemitem{(b)} Conclua do item (a) e do Exercício~4 que
$(\gamma\cdot\mu)\cdot\lambda$ é homotópica com extremos fixos a
$\gamma\cdot(\mu\cdot\lambda)$.
\itemitem{(c)} Para $x\in M$ denote por $\frak o_x:[0,1]\to M$ a
{\sl curva constante e igual a $x$}, i.e., $\frak o_x(t)=x$ para
todo $t\in[0,1]$. Determine funções contínuas
$\sigma,\tilde\sigma:[0,1]\to[0,1]$ com $\gamma\cdot\frak
o_{\gamma(1)}=\gamma\circ\tilde\sigma$, $\frak
o_{\gamma(0)}\cdot\gamma=\gamma\circ\sigma$,
$\sigma(0)=\tilde\sigma(0)=0$ e $\sigma(1)=\tilde\sigma(1)=1$
({\sl dica}: novamente é possível escolher $\sigma$ e
$\tilde\sigma$ afins por partes).
\itemitem{(d)} Conclua do item (c) e do Exercício~4 que
$\gamma\cdot\frak o_{\gamma(1)}$ e $\frak
o_{\gamma(0)}\cdot\gamma$ são homotópicas a $\gamma$ com extremos
fixos.
\itemitem{(e)} Determine funções contínuas
$\sigma,\tilde\sigma:[0,1]\to[0,1]$ tais que
$\gamma\circ\sigma=\gamma\cdot\gamma^{-1}$,
$\gamma\circ\tilde\sigma=\gamma^{-1}\cdot\gamma$,
$\sigma(0)=\sigma(1)=0$ e $\tilde\sigma(0)=\tilde\sigma(1)=1$
({\sl dica}: novamente é possível escolher $\sigma$ e
$\tilde\sigma$ afins por partes).
\itemitem{(f)} Conclua do item (e) e do Exercício~4 que
$\gamma\cdot\gamma^{-1}$ e $\gamma^{-1}\cdot\gamma$ são homotópicas
com extremos fixos a curvas constantes.

\smallskip

\item{6.} Sejam $M$, $N$ espaços métricos e $f:M\to N$ uma função
contínua. Mostre que se $\gamma,\mu:[a,b]\to M$ são homotópicas
com extremos fixos então $f\circ\gamma$ e $f\circ\mu$ são
homotópicas com extremos fixos. Similarmente, mostre que se
$\gamma$ e $\mu$ são homotópicas como laços então $f\circ\gamma$ e
$f\circ\mu$ são homotópicas como laços. Conclua que se dois
espaços métricos $M$ e $N$ são homeomorfos então $M$ é
simplesmente conexo se e somente se $N$ o for.

\smallskip

\item{7.} Sejam $M$ um espaço métrico e $\gamma,\mu:[0,1]\to M$
laços. Suponha que $H:[0,1]\times[0,1]\to M$ é uma homotopia de
laços de $\gamma$ para $\mu$ e defina $\lambda:[0,1]\to M$ fazendo
$\lambda(s)=H(0,s)=H(1,s)$, $s\in[0,1]$. Mostre que os laços $\mu$
e $\lambda^{-1}\cdot\gamma\cdot\lambda$ são homotópicos {\sl com
extremos fixos\/} ({\sl dica}: defina curvas $\frak l,\frak
r,\frak t,\frak b:[0,1]\to[0,1]\times[0,1]$ fazendo $\frak
l(u)=(0,u)$, $\frak r(u)=(1,u)$, $\frak t(u)=(u,1)$, $\frak
b(u)=(u,0)$; como o retângulo $[0,1]\times[0,1]$ é convexo, as curvas $\frak
l^{-1}\cdot\frak b\cdot\frak r$ e $\frak t$ são homotópicas com
extremos fixos. Aplique o Exercício~6 com $f=H$).

\smallskip

\item{8.} Sejam $M$ um espaço métrico, $\gamma:[0,1]\to
M$ um laço e $\lambda:[0,1]\to M$ uma curva com
$\lambda(0)=\gamma(0)$. Mostre que os laços
$\lambda^{-1}\cdot\gamma\cdot\lambda$ e $\gamma$ são homotópicos
{\sl como laços\/} [{\sl dica}: para $s\in[0,1]$, seja
$\lambda_s:[0,1]\to M$ a curva definida por
$\lambda_s(u)=\lambda(us)$. Defina uma homotopia $H$ fazendo
$H_s=\lambda_s^{-1}\cdot\gamma\cdot\lambda_s$ e mostre a
continuidade de $H$ usando o Exercício~38 da aula número 5
(20/03). Observe que $\lambda_0$ é uma curva constante e complete
a demonstração usando o Exercício~5 para concluir que
$\lambda_0^{-1}\cdot\gamma\cdot\lambda_0$ é homotópica a $\gamma$
com extremos fixos].

\smallskip

\item{9.} Dado um espaço métrico $M$ e um laço $\gamma:[a,b]\to
M$, mostre que $\gamma$ é homotópico como laço a uma aplicação
constante se e somente se $\gamma$ é homotópico com extremos fixos
a uma aplicação constante ({\sl dica}: use os Exercícios~7 e 5).

\smallskip

\item{10.} Dado um espaço métrico $M$, mostre que as seguintes
condições são equivalentes:
\smallskip
\itemitem{(i)} $M$ é simplesmente conexo;
\itemitem{(ii)} todo laço em $M$ é homotópico {\sl com extremos
fixos\/} a um laço constante;
\itemitem{(iii)} dadas curvas $\gamma,\mu:[a,b]\to M$ com os mesmos
extremos então $\gamma$ e $\mu$ são homotópicas com extremos
fixos.
\smallskip
Se $M$ é conexo por caminhos, mostre que (i), (ii) e (iii) são
também equivalentes a qualquer uma das seguinte condições:
\smallskip
\itemitem{(iv)} existe $x_0\in M$ tal que todo laço
$\gamma:[a,b]\to M$ com $\gamma(a)=\gamma(b)=x_0$ é homotópico com
extremos fixos a um laço constante;
\itemitem{(v)} existe $x_0\in M$ tal que todo laço
$\gamma:[a,b]\to M$ com $\gamma(a)=\gamma(b)=x_0$ é homotópico
como laço a uma aplicação constante.
\smallskip
({\sl dica}: para (i)$\Leftrightarrow$(ii) e (iv)$\Leftrightarrow$(v),
use o Exercício~9. Para (ii)$\Rightarrow$(iii) use o Exercício~5,
levando em conta que, sob (ii), o laço $\gamma\cdot\mu^{-1}$ é homotópico com
extremos fixos a uma constante. (iii)$\Rightarrow$(i) e
(i)$\Rightarrow$(iv) são óbvios. Se $M$ é conexo por caminhos, para
mostrar (iv)$\Rightarrow$(i) considere um laço $\gamma:[0,1]\to M$
com $\gamma(0)=\gamma(1)=x_1$ e uma curva $\lambda:[0,1]\to M$ com
$\lambda(0)=x_0$ e $\lambda(1)=x_1$. Use o Exercício~5 e o fato
que, sob (iv), $\lambda\cdot\gamma\cdot\lambda^{-1}$ é homotópico com
extremos fixos a um laço constante).

\bigskip

\noindent{\bf O significado dos Exercícios acima}.

\smallskip

O Exercício~2 diz que no estudo da teoria de homotopia de curvas e
laços pode-se assumir sempre sem perda de generalidade que o intervalo onde
as curvas estão definidas é $[0,1]$. O Exercício~1 diz que homotopia
com extremos fixos e homotopia de laços definem relações de
equivalência. Vamos denotar por $[\gamma]$ a {\sl classe de
equivalência\/} formada por todas as curvas homotópicas a $\gamma$
com extremos fixos. O Exercício~3 diz que as operações de
composição e inversão de curvas são bem-definidas no espaço
quociente formado pelas classes de homotopia $[\gamma]$, i.e., faz sentido definir
$[\gamma]\cdot[\mu]=[\gamma\cdot\mu]$ e
$[\gamma]^{-1}=\big[\gamma^{-1}\big]$ (se $\gamma(1)=\mu(0)$). O item
(a) do Exercício~4 diz que classes de homotopia são invariantes
por reparametrização. O Exercício~5 diz que concatenação de curvas
é uma operação associativa a nível de classes de homotopia (mas
não a nível de curvas!); as curvas constantes funcionam como {\sl
elementos neutros\/} para concatenação e a curva $\gamma^{-1}$
funciona como um {\sl elemento inverso\/} de $\gamma$. O
Exercício~6 diz que aplicações contínuas entre espaços induzem
funções nos correspondentes espaços de classes de homotopias de
curvas. Os Exercícios~7, 8 e 9 relacionam homotopia de laços com
homotopia com extremos fixos: basicamente, dois laços são
homotópicos como laços se e somente se um é homotópico com
extremos fixos a um {\sl conjugado\/} do outro.

\smallskip

As classes de homotopia com extremos fixos de curvas em $M$ formam
uma estrutura algébrica chamada um {\sl grupóide}; este é chamado
o {\sl grupóide fundamental\/} do espaço $M$. Fixado $x_0\in M$ e
considerando apenas classes de homotopia (com extremos fixos) de
laços $\gamma:[0,1]\to M$ com $\gamma(0)=\gamma(1)=x_0$, obtemos
um {\sl grupo}; este é chamado o {\sl grupo fundamental\/} do
espaço $M$ com ponto base $x_0$ e é normalmente denotado por
$\pi_1(M,x_0)$. Uma curva contínua $\lambda:[0,1]\to M$ com
$\lambda(0)=x_0$, $\lambda(1)=x_1$ induz um isomorfismo entre os
grupos fundamentais $\pi_1(M,x_0)$ e $\pi_1(M,x_1)$ dado por
$[\gamma]\mapsto\big[\lambda^{-1}\cdot\gamma\cdot\lambda\big]$.
Para espaços conexos por caminhos então o grupo fundamental é (a
menos de isomorfismos) independente do ponto base. Um espaço é
simplesmente conexo justamente quando seu grupo fundamental é
trivial.

Em geral, é possível definir a noção de homotopia não apenas para curvas, mas
também para aplicações quaisquer entre espaços topológicos. O grupo
fundamental de $M$ pode então também ser definido em termos de
classes de homotopia de aplicações do círculo em $M$. A partir
daí, pode-se também estudar classes de homotopia de aplicações da
{\sl esfera $n$-dimensional\/} em $M$; obtêm-se então o chamado
{\sl $n$-ésimo grupo de homotopia de $M$}, denotado por
$\pi_n(M,x_0)$. A teoria de homotopia é um campo importante dentro
da topologia algébrica e nós aqui mal esbarramos no assunto.

\vfil\eject

\centerline{\bf Aula número $21$ (24/05)}
\bigskip
\bigskip

A aula número 21 cobriu o material sobre número de Lebesgue de
coberturas (aula número 7 --- 27/03, seção 2) e o material sobre a
relação entre homotopia de curvas e integrais de linha (aula
número 20 --- 22/05, seção 2).

\bigskip\bigskip\bigskip\bigskip

\centerline{\bf Aula número $22$ (29/05)}
\bigskip
\bigskip

\item{(1)} {\bf O teorema da função inversa}.

\smallskip

O teorema da função inversa é um exemplo típico da idéia básica do
Cálculo Diferencial: aproximar funções não lineares por funções
lineares. O objetivo principal do teorema da função inversa é
relacionar a bijetividade da aproximação linear de uma função
(i.e., sua diferencial) com a bijetividade da função original numa
região pequena. Começamos primeiro com a implicação fácil do
teorema.

\smallskip

\proclaim Teorema. Sejam $U,\,V\subset\R^n$ abertos e $f:U\to V$
uma função bijetora. Suponha que $f:U\to\R^n$ seja diferenciável
num ponto $x\in U$ e que $f^{-1}:V\to\R^n$ seja diferenciável no
ponto $f(x)$. Então $\dd f(x):\R^n\to\R^n$ é um isomorfismo e $\dd
f^{-1}\big(f(x)\big)=\dd f(x)^{-1}$ (``a diferencial da inversa é
a inversa da diferencial'').

\Prova Temos $f^{-1}\circ f=\Id\vert_U$ e $f\circ f^{-1}=\Id\vert_V$;
aplicando a regra da cadeia obtemos:
$$\dd f^{-1}\big(f(x)\big)\circ\dd f(x)=\Id,\quad\dd
f(x)\circ\dd f^{-1}\big(f(x)\big)=\Id,$$ e a conclusão
segue.\fimprova

\smallskip

\noindent{\tt Observação}. Na verdade, poderíamos ter dado um
enunciado um pouco melhor para o teorema acima: poderíamos ter
assumido que $U$ é um aberto de $\R^m$ e que $V$ é um aberto de
$\R^n$ (i.e., não assumimos a princípio que seja $m=n$). A
condição $m=n$ apareceria então na {\sl tese\/} do teorema. Para
demonstrar esse enunciado mais geral observamos que, sob as
hipóteses do teorema, $\dd f(x)$ é um isomorfismo de $\R^m$ para
$\R^n$, o que implica (da Álgebra Linear!) que $m=n$.

\smallskip

O teorema acima nos diz duas coisas: em primeiro lugar, não há
chance alguma de uma função diferenciável admitir uma inversa
diferenciável se a sua diferencial em cada ponto não for um
isomorfismo. Em segundo lugar, o teorema nos diz como calcular a
diferencial da função inversa, {\sl mas é necessário saber a
priori que a função inversa é diferenciável}. Nosso objetivo agora
é estabelecer condições suficientes para que a função inversa seja
diferenciável (obviamente, devemos supor que $\dd f(x)$ seja um
isomorfismo, antes de mais nada --- mas será que isso basta?).
Depois, nosso problema será achar condições para que a
inversibilidade de $\dd f(x)$ implique na inversibilidade de $f$
numa vizinhança de $x$.

\smallskip

Antes de provar o teorema seguinte, vamos motivar um pouco seu
enunciado fazendo um estudo informal do caso de funções de uma
variável. Seja então $f:I\to\R$ uma função diferenciável e
injetora definida num intervalo $I\subset\R$. Suponha que
$f'(x)\ne0$ para algum $x\in I$ (isso corresponde à hipótese que
$\dd f(x)$ seja um isomorfismo; veja o Exercício~1). Escrevendo
$\Delta y=f(x+\Delta x)-f(x)$ então a derivada $f'(x)$ é dada pelo
limite $\lim_{\Delta x\to0}{\Delta y\over\Delta x}$; a derivada de
$f^{-1}$ no ponto $y=f(x)$ é dada pelo limite $\lim_{\Delta
y\to0}{\Delta x\over\Delta y}$. Como ${\Delta y\over\Delta x}$
tende a $f'(x)\ne0$ quando $\Delta x\to0$, temos que ${\Delta
x\over\Delta y}$ tende a ${1\over f'(x)}$, {\sl mas quando $\Delta
x\to0$} (e não, a princípio, quando $\Delta y\to0$). Para concluir
que $f^{-1}$ é diferenciável no ponto $y=f(x)$, precisamos saber
então que $\Delta y\to0$ implica $\Delta x\to0$; essa hipótese
corresponde à {\sl continuidade de $f^{-1}$ no ponto $y$}. Temos
então o seguinte:

\proclaim Teorema. (``teoreminha'' da função inversa) Sejam
$U,\,V\subset\R^n$ abertos e $f:U\to V$ uma função bijetora;
suponha que $f:U\to\R^n$ é diferenciável num ponto $x\in U$, que
$\dd f(x):\R^n\to\R^n$ é um isomorfismo e que $f^{-1}:V\to\R^n$ é
contínua no ponto $f(x)\in V$. Então $f^{-1}$ é diferenciável no
ponto $f(x)$ e $\dd f^{-1}\big(f(x)\big)=\dd f(x)^{-1}$.

\Prova Como $f$ é diferenciável no ponto $x$, podemos escrever:
$$f(x+h)=f(x)+\dd f(x)\cdot h+\rho(h)\Vert h\Vert,\eqno{(1)}$$
com $\lim_{h\to0}\rho(h)=0$. Tome agora $y=f(x)$ e escreva:
$$f^{-1}(y+k)=f^{-1}(y)+\dd f(x)^{-1}\!\cdot k+\sigma(k)\Vert
k\Vert;\eqno{(2)}$$ a demonstração ficará completa se mostrarmos
que $\lim_{k\to0}\sigma(k)=0$.

Para cada $k\in\R^n$ com $y+k\in V$ associamos o vetor
$h=f^{-1}(y+k)-f^{-1}(y)\in\R^n$ (mais precisamente, definimos uma
função $k\mapsto\phi(k)=f^{-1}(y+k)-f^{-1}(y)$ e escrevemos $h$ em
vez de $\phi(k)$). Resolvendo a igualdade
$h=f^{-1}(y+k)-f^{-1}(y)$ para $k$, obtemos que ($x+h$ pertence a
$U$ e que) $k=f(x+h)-f(x)$. De (2) vem então:
$$h=\dd f(x)^{-1}\!\cdot\big[f(x+h)-f(x)\big]+\sigma(k)\Vert k\Vert.$$
Usando (1) obtemos $f(x+h)-f(x)=\dd f(x)\cdot h+\rho(h)\Vert
h\Vert$ e daí:
$$h=h+\Vert h\Vert\,\dd f(x)^{-1}\!\cdot\rho(h)+\sigma(k)\Vert
k\Vert\Longrightarrow\sigma(k)=-{\Vert h\Vert\over\Vert
k\Vert}\,\dd f(x)^{-1}\!\cdot\rho(h).$$ A continuidade de $f^{-1}$
no ponto $y=f(x)$ implica que $h\to0$ quando $k\to0$, i.e., que
$\lim_{k\to0}f^{-1}(y+k)-f^{-1}(y)=0$. Como
$\lim_{h\to0}\rho(h)=0$ e como $\dd f(x)^{-1}$ é contínua (pois é
linear), segue que também $\lim_{k\to0}\dd
f(x)^{-1}\!\cdot\rho(h)=0$. Para completar a demonstração, é
suficiente então mostrar que o quociente:
$${\Vert h\Vert\over\Vert k\Vert}={\Vert h\Vert\over\big\Vert
f(x+h)-f(x)\big\Vert}$$ é limitado para $k$ numa vizinhança da
origem, ou seja, que ${\Vert f(x+h)-f(x)\Vert\over\Vert h\Vert}$
fica longe de zero para $k$ numa vizinhança da origem. Como a
aplicação linear $\dd f(x)$ é contínua e não se anula na esfera
unitária de $\R^n$ (que é compacta), temos:
$$c=\inf_{\Vert u\Vert=1}\big\Vert\dd f(x)\cdot u\big\Vert=\min_{\Vert u\Vert=1}\big\Vert\dd f(x)\cdot
u\big\Vert>0;$$ como $\lim_{h\to0}\rho(h)=0$, temos que
$\big\Vert\rho(h)\big\Vert<{c\over2}$ para $h$ em alguma
vizinhança $Z$ da origem em $\R^n$. Daí, usando (1), temos que
para $h\in Z$:
$$\big\Vert f(x+h)-f(x)\big\Vert=\big\Vert\dd f(x)\cdot
h+\rho(h)\Vert h\Vert\big\Vert\ge\big\Vert\dd f(x)\cdot
h\big\Vert-\big\Vert\rho(h)\big\Vert\Vert h\Vert\ge c\Vert
h\Vert-{c\over2}\Vert h\Vert={c\over2}\Vert h\Vert.$$ Como
$\lim_{k\to0}h=\lim_{k\to0}f^{-1}(y+k)-f^{-1}(y)=0$, temos que
$h\in Z$ para $k$ em alguma vizinhança da origem em $\R^n$; nessa
vizinhança, ${\Vert h\Vert\over\Vert k\Vert}\le{2\over c}$, i.e.,
a quantidade ${\Vert h\Vert\over\Vert k\Vert}$ é limitada. Isso
completa a demonstração.\fimprova

\smallskip

\noindent{\tt Observação}. Se $I\subset\R$ é um intervalo e
$f:I\to\R$ é uma função contínua e injetora então sabe-se que
$f(I)$ é um intervalo (pelo Teorema do Valor Intermediário) e que
a função inversa $f^{-1}:f(I)\to\R$ também é contínua (isso segue
dos seguintes resultados da Análise Real: (i) {\sl toda função
contínua e injetora num intervalo é monótona}; (ii) {\sl toda
função monótona cuja imagem é um intervalo é contínua}; veja {\sl
Curso de Análise}, vol.\ I, Elon Lages Lima, pg.\ 186). Daí, a
continuidade da inversa no ``teoreminha da função inversa'' é uma
hipótese redundante se $n=1$, $U$ é um intervalo e $f$ é contínua
em $U$. Na verdade, se $U\subset\R^n$ é um aberto e $f:U\to\R^n$ é
uma função contínua e injetora então $f(U)$ é aberto em $\R^n$ e
$f:U\to f(U)$ é um homeomorfismo, i.e., $f^{-1}:f(U)\to\R^n$ é
contínua. Isso segue do famoso {\sl Teorema da Invariância do
Domínio\/} --- tal teorema é bem mais sofisticado que o resultado
de Análise Real mencionado acima. Sua demonstração é normalmente
feita utilizando recursos da Topologia Algébrica (mais
precisamente, utiliza-se recursos da teoria de {\sl Homologia
Singular} de um espaço topológico). Na demonstração do teorema da
função inversa adiante, mostraremos a continuidade da função
inversa através de recursos muito mais elementares: usaremos no
entanto a hipótese mais forte que a função $f$ seja de classe
$C^1$ e que sua diferencial em cada ponto seja um isomorfismo.

\smallskip

\noindent{\tt Não-Exemplo}. A função $f:\R\to\R$ dada por
$f(x)=x^3$ é um homeomorfismo de classe $C^\infty$ mas $f'(0)=0$,
i.e., $\dd f(0)$ não é um isomorfismo. Obviamente então $f^{-1}$
não é diferenciável na origem.

\smallskip

A seguinte definição facilita o enunciado do teorema da função
inversa: \proclaim Definição. Sejam $U,\,V\subset\R^n$ abertos e
$f:U\to V$ uma função. Dizemos que $f$ é um {\rm difeomorfismo}
quando $f$ é bijetora e tanto $f:U\to\R^n$ como $f^{-1}:V\to\R^n$
são diferenciáveis.

\smallskip

Na prática estaremos mais interessados em bijeções de classe $C^k$
com inversas de classe $C^k$. Temos o seguinte: \proclaim Teorema.
Sejam $U,\,V\subset\R^n$ abertos e $f:U\to V$ um difeomorfismo. Se
$f:U\to\R^n$ é de classe $C^k$ ($0\le k\le\infty$) então também
$f^{-1}:V\to\R^n$ é de classe $C^k$.

\Prova Seja $\GL(n,\R)$ o conjunto dos operadores lineares
inversíveis de $\R^n$ em $\R^n$ (esse é chamado o {\sl grupo
linear geral\/} de $\R^n$; recorde o complemento à Aula número 4
--- 15/03). Temos que $\GL(n,\R)$ é um aberto no espaço $\Lin(\R^n,\R^n)$ de todos os operadores
lineares de $\R^n$ em $\R^n$, pois $T\in\GL(n,\R)$ se e somente se
$\det(T)\ne0$ e o determinante é uma função contínua em
$\Lin(\R^n,\R^n)$ (pois é um polinômio nas entradas da matriz que
representa $T$). Além do mais, a {\sl função inversão\/} ${\cal
I}:\GL(n,\R)\to\Lin(\R^n,\R^n)$ dada por ${\cal I}(T)=T^{-1}$ é de
classe $C^\infty$, pois as entradas da matriz que representa
$T^{-1}$ são dadas por quocientes de polinômios nas entradas da
matriz que representa $T$ (recorde que a inversa de uma matriz
quadrada $A$ é dada pela transposta da matriz dos cofatores de
$A$, dividida por $\det(A)$). Se $f:U\to V$ é um difeomorfismo
então sabemos que $\dd f^{-1}\big(f(x)\big)=\dd f(x)^{-1}={\cal
I}\big(\dd f(x)\big)$ para todo $x\in U$, ou seja, $\dd
f^{-1}(y)={\cal I}\big[\dd f\big(f^{-1}(y)\big)\big]$ para todo
$y\in V$. Temos então:
$$\dd f^{-1}={\cal I}\circ\dd f\circ f^{-1}.$$
Usando indução em $k$, a igualdade acima e o fato que ${\cal I}$ é
de classe $C^\infty$ segue agora facilmente que se $f$ é um
difeomorfismo de classe $C^k$ então também $f^{-1}$ é de classe
$C^k$.\fimprova

\smallskip

O teorema da função inversa afirma, entre outras coisas, que uma
função satisfazendo certas hipóteses possue como imagem um aberto
em $\R^n$. Na verdade, essa é a parte mais difícil de demonstrar.
De fato, mostrar que a imagem de uma função $f$ é um conjunto
aberto envolve um problema de existência de soluções de equações:
deve-se mostrar que para $y$ perto de $f(a)$ a equação $f(x)=y$
admite solução. Essa parte do argumento fará parte da demonstração
do {\sl Lema da Perturbação da Identidade\/} que será mostrado
logo adiante. Antes de mais nada, no entanto, precisamos de algum
princípio de existência de soluções de equações. Esse é o nosso
próximo passo.

\smallskip

\proclaim Definição. Sejam $M$ um conjunto e $\phi:M\to M$ uma
função. Dizemos que $x\in M$ é um {\rm ponto fixo} de $\phi$ se
$\phi(x)=x$.

\smallskip

Tipicamente equações da forma $f(x)=y$ (com $x\in\R^m$,
$y\in\R^n$) podem ser transformadas em problemas de ponto fixo:
basta escrever $f(x)=y$ sob a forma equivalente $f(x)+x-y=x$. Daí
resolver $f(x)=y$ corresponde a determinar um ponto fixo para
$\phi(x)=f(x)+x-y$. Essa observação indica a importância do
seguinte: \proclaim Teorema. (do ponto fixo de Banach) Sejam
$(M,d)$ um espaço métrico completo (não vazio) e $\phi:M\to M$ uma
contração. Então $\phi$ tem um único ponto fixo. Além do mais,
para todo $x_0\in M$, se definirmos uma seqüência $(x_n)_{n\ge0}$
em $M$ recursivamente fazendo $x_n=\phi(x_{n-1})$, $n\ge1$, então
$(x_n)_{n\ge0}$ converge para o único ponto fixo de $\phi$.

\Prova Em primeiro lugar, é óbvio que $\phi$ pode ter no máximo um
ponto fixo, já que $\phi$ contrai distâncias: se $x,y\in M$ são
pontos fixos de $\phi$ e $\lambda<1$ é uma constante de Lipschitz
para $\phi$ então:
$$d(x,y)=d\big(\phi(x),\phi(y)\big)\le\lambda\,
d(x,y)\Longrightarrow0\le(1-\lambda)d(x,y)\le0\Longrightarrow
d(x,y)=0,$$ e portanto $x=y$.

Para completar a demonstração, seja $x_0\in M$ um ponto qualquer e
defina $(x_n)_{n\ge0}$ recursivamente como no enunciado do
teorema. Só precisamos mostrar que $(x_n)_{n\ge1}$ é uma seqüência
de Cauchy; de fato, nesse caso deve existir
$x=\lim_{n\to+\infty}x_n$, pois $M$ é completo e além do mais,
como $\phi$ é contínua:
$$\lim_{n\to+\infty}x_n=x\Longrightarrow\lim_{n\to+\infty}\underbrace{\phi(x_n)}_{x_{n+1}}=\phi(x)=x=\lim_{n\to+\infty}x_{n+1},$$
i.e., $x$ é um ponto fixo de $\phi$.

Para mostrar que $(x_n)_{n\ge0}$ é uma seqüência de Cauchy,
devemos estimar a distância $d(x_n,x_m)$; começamos estimando
$d(x_n,x_{n+1})$. Temos:
$$d(x_n,x_{n+1})\le\lambda\,d(x_{n-1},x_n)\le\lambda^2d(x_{n-2},x_{n-1})\le\cdots\le\lambda^nd(x_0,x_1);$$
daí, para $n<m$:
$$\eqalign{d(x_n,x_m)&\le
d(x_n,x_{n+1})+d(x_{n+1},x_{n+2})+\cdots+d(x_{m-1},x_m)\cr
&\le\big(\lambda^n+\lambda^{n+1}+\cdots+\lambda^{m-1}\big)d(x_0,x_1)\cr
&\le\left(\;\sum_{k=n}^{+\infty}\lambda^k\right)d(x_0,x_1)={\lambda^n\over1-\lambda}\,d(x_0,x_1);\cr}$$
como $\lim_{n\to+\infty}d(x_0,x_1){\lambda^n\over1-\lambda}=0$, a
conclusão segue.\fimprova

\smallskip

\noindent{\tt Observação}. O teorema acima é um dos grandes
``princípios de existência'' da matemática. Quando aplicado para
operadores integrais em espaços de funções ele produz o famoso
{\sl teorema de existência e unicidade para soluções de equações
diferenciais}. O teorema acima também é a base matemática para
diversos métodos de Cálculo Numérico, como o {\sl Método de Newton
para obtenção de raízes de funções $f:\R\to\R$}.

\smallskip

A idéia principal da demonstração do teorema da função inversa é a
seguinte: se $f$ é diferenciável e $\dd f(x)$ é um isomorfismo
então, numa vizinhança do ponto $x$, a função $f$ é uma
``perturbação pequena'' do isomorfismo $\dd f(x)$. Queremos então
estudar o comportamento de uma perturbação pequena de um
isomorfismo $T:\R^n\to\R^n$; começamos com o caso em que $T$ é
igual a identidade no seguinte: \proclaim Lema. (da perturbação da
identidade) Se $U\subset\R^n$ é um aberto e $g:U\to\R^n$ é uma
contração então a aplicação $f:U\to\R^n$ definida por
$f(x)=x+g(x)$, $x\in U$, é um homeomorfismo sobre sua imagem
$f(U)$; além do mais, $f(U)$ é um aberto em $\R^n$.

\Prova Se $\lambda<1$ é uma constante de Lipschitz para $g$ então, para todos
$x,y\in U$:
$$\eqalign{\big\Vert f(x)-f(y)\big\Vert&=\big\Vert x-y+g(x)-g(y)\big\Vert\ge\Vert
x-y\Vert-\big\Vert g(x)-g(y)\big\Vert\cr
&\ge\Vert x-y\Vert-\lambda\Vert x-y\Vert=(1-\lambda)\Vert x-y\Vert.\cr}$$
Como $1-\lambda>0$, as desigualdades acima implicam que $f$ é injetora e que
sua inversa $f^{-1}:f(U)\to U$ é Lipschitziana com constante
${1\over1-\lambda}$; em particular, $f^{-1}$ é contínua.

Na verdade, a parte não trivial da demonstração é mostrar que $f(U)$ é aberto
em $\R^n$. Para isso, considere um ponto arbitrário de $f(U)$, digamos $f(a)$
com $a\in U$. Devemos mostrar que se $\big\Vert y-f(a)\big\Vert$ é
suficientemente pequeno então a equação $f(x)=y$ admite uma solução $x\in
U$. Fixe então $y\in\R^n$ e vamos tentar determinar condições suficientes
sobre $y$ para que $f(x)=y$ tenha solução. Temos:
$$f(x)=y\Longleftrightarrow-f(x)=-y\Longleftrightarrow
x-f(x)=x-y\Longleftrightarrow x-f(x)+y=x,$$
i.e., transformamos a equação $f(x)=y$ num problema de ponto fixo!
Explicitamente, definindo $h:U\to\R^n$ por $h(x)=x-f(x)+y=y-g(x)$ então
$f(x)=y$ equivale a $h(x)=x$. Do fato que $g$ é uma contração, segue
trivialmente que $h$ também o é --- mas cuidado, isso ainda não resolveu o
problema! O teorema do ponto fixo de Banach só se aplica para contrações
definidas num espaço métrico {\sl completo\/} e {\sl tomando valores nele
mesmo}. Devemos determinar então um subconjunto fechado $M\subset\R^n$ contido
em $U$ e tal que $h(M)\subset M$ (recorde que, como $\R^n$ é completo, um
subespaço $M\subset\R^n$ é completo se e somente se for um subconjunto
fechado). Como no final das contas deveremos escolher $y$ perto de $f(a)$,
espera-se que a solução $x$ de $f(x)=y$ esteja perto de $a$ e portanto um
candidato natural para $M$ é uma bola fechada centrada em $a$ (contida em
$U$). Vamos então seguir esta estratégia.

Escolha $r_0>0$ tal que $\Bola[a;r_0]\subset U$. Queremos
determinar condições suficientes sobre $y$ que tornem possível a
escolha de $r>0$, $r\le r_0$, com a seguinte propriedade: se
$\Vert x-a\Vert\le r$ então $\big\Vert h(x)-a\big\Vert\le r$.
\smallskip
Calculamos então:

\vskip -8pt

$$\eqalign{\big\Vert h(x)-a\big\Vert&=\big\Vert y-g(x)-a\big\Vert=\big\Vert
y-f(a)+g(a)-g(x)\big\Vert\cr &\le\big\Vert
y-f(a)\big\Vert+\big\Vert g(a)-g(x)\big\Vert\cr &\le\big\Vert
y-f(a)\big\Vert+\lambda\Vert x-a\Vert.\cr}$$ Para concluir que
$\big\Vert h(x)-a\big\Vert\le r$ (sob $\Vert x-a\Vert\le r$),
precisamos então da desigualdade:
$$\big\Vert y-f(a)\big\Vert\le(1-\lambda)r;$$
a desigualdade acima será satisfeita se escolhermos
$r\ge{1\over1-\lambda}\big\Vert y-f(a)\big\Vert$. A restrição $r\le r_0$ nos
obriga então a supor:
$$\big\Vert y-f(a)\big\Vert\le(1-\lambda)r_0.$$
Agora tudo se encaixa: se começamos com $y\in\R^n$ satisfazendo a
desigualdade acima então, definindo $r=r_0$, obteremos que $\Vert
x-a\Vert\le r$ implica $\big\Vert h(x)-a\big\Vert\le r$, i.e., que
$h\big(\Bola[a;r]\big)\subset\Bola[a;r]$. Aplicando então o
teorema do ponto fixo de Banach para a contração $h$ no espaço
métrico completo $\Bola[a;r]$, obtemos $x\in\Bola[a;r]\subset U$
com $h(x)=x$, i.e., com $f(x)=y$. Juntando tudo vem:
$$\Bola\big[f(a);(1-\lambda)r_0\big]\subset f(U),$$
e portanto $f(a)$ é um ponto interior de $f(U)$.\fimprova

\smallskip

\proclaim Corolário. (perturbação de um isomorfismo) Sejam
$T:\R^n\to\R^n$ um isomorfismo linear, $U\subset\R^n$ um aberto e
$g:U\to\R^n$ uma função Lipschitziana com constante de Lipschitz
menor que $\big\Vert T^{-1}\big\Vert^{-1}$ (onde $\norma$ denota a
norma de operadores). Então, definindo uma aplicação $f:U\to\R^n$
por $f(x)=T(x)+g(x)$, $x\in U$, temos que $f:U\to f(U)$ é um
homeomorfismo e que $f(U)$ é aberto em $\R^n$.

\Prova Temos $f(x)=T\big(x+(T^{-1}\circ g)(x)\big)$, para todo
$x\in U$, ou seja: $$f=T\circ\big(\Id\vert_U+T^{-1}\circ g\big).$$
Como $T$ é um homeomorfismo de $\R^n$ sobre $\R^n$, a conclusão
seguirá do lema de perturbação da identidade se mostrarmos que
$T^{-1}\circ g$ é uma contração. Mas isso é muito simples; temos:
$$\big\Vert T^{-1}\big(g(x)\big)-T^{-1}\big(g(y)\big)\big\Vert\le k\big\Vert
T^{-1}\big\Vert\Vert x-y\Vert,$$
para todos $x,y\in U$, onde $k<\big\Vert T^{-1}\big\Vert^{-1}$ é uma constante
de Lipschitz para $g$.\fimprova

\smallskip

\proclaim Lema. Seja $f:U\to\R^n$ uma função de classe $C^1$
definida num aberto $U\subset\R^n$. Então o conjunto dos pontos
$x\in U$ para os quais $\dd f(x):\R^n\to\R^n$ é um isomorfismo é
aberto em $\R^n$.

\Prova O conjunto mencionado no enunciado é nada mais que a imagem
inversa do aberto $\GL(n,\R)$ em $\Lin(\R^n,\R^n)$ pela função
contínua $\dd f:U\to\Lin(\R^n,\R^n)$. Segue que esse conjunto é
aberto em $U$ e portanto aberto em $\R^n$.\fimprova

\smallskip

Finalmente, estamos em condição de demonstrar o: \proclaim
Teorema. (da função inversa) Seja $f:U\to\R^n$ uma função de
classe $C^k$ ($1\le k\le\infty$) definida num aberto
$U\subset\R^n$. Se para um certo $x_0\in U$ temos que $\dd
f(x_0):\R^n\to\R^n$ é um isomorfismo então existe uma vizinhança
aberta $V$ de $x_0$ em $\R^n$ contida em $U$ tal que $f\vert_V$ é
injetora, $f(V)$ é aberto em $\R^n$ e $f\vert_V:V\to f(V)$ é um
difeomorfismo; em particular, a função inversa
$(f\vert_V)^{-1}:f(V)\to\R^n$ também é de classe $C^k$.

\Prova É suficiente achar uma vizinhança aberta $W$ de $x_0$ em
$U$ tal que $f(W)$ é aberto em $\R^n$ e $f\vert_W:W\to f(W)$ é um
homeomorfismo. De fato, uma vez encontrada $W$, definimos
$V=\big\{x\in W:\dd f(x)\ \hbox{é um isomorfismo}\big\}$ e daí
$V\subset\R^n$ é aberto, $f(V)\subset\R^n$ é aberto e
$f\vert_V:V\to f(V)$ é um homeomorfismo; segue do ``teoreminha''
da função inversa que $(f\vert_V)^{-1}:f(V)\to\R^n$ é
diferenciável e portanto de classe $C^k$ (já que $f\vert_V$ é de
classe $C^k$). Para encontrar $W$, vamos usar o princípio da
perturbação de um isomorfismo por funções com constante de
Lipschitz pequenas. Defina $g:U\to\R^n$ satisfazendo a igualdade:
$$f(x)=\dd f(x_0)\cdot x+g(x),$$
para todo $x\in U$; temos então:
$$\dd g(x)=\dd f(x)-\dd f(x_0).$$
Como $\dd f$ é contínua, em alguma vizinhança aberta $W$ de $x_0$
em $U$ teremos:
$$\big\Vert\dd g(x)\big\Vert\le{1\over2}\big\Vert\dd f(x_0)^{-1}\big\Vert^{-1}$$
e daí (escolhendo $W$ convexa), podemos aplicar a desigualdade do
valor médio para concluir que $g\vert_W$ é Lipschitziana com
constante de Lipschitz menor que $\big\Vert\dd
f(x_0)^{-1}\big\Vert^{-1}$. A conclusão segue.\fimprova

\smallskip

\proclaim Definição. Uma aplicação $f:M\to N$ entre espaços
métricos $M$, $N$ é dita {\rm aberta} quando $f$ leva abertos de
$M$ em abertos de $N$, i.e., se para todo $U\subset M$ aberto em
$M$ temos que $f(U)$ é aberto em $N$. Similarmente, dizemos que
$f$ é {\rm fechada} quando $f$ leva fechados de $M$ em fechados de
$N$.

\smallskip

\proclaim Definição. Seja $U\subset\R^n$ um aberto. Uma aplicação
$f:U\to\R^n$ é dita um {\rm difeomorfismo local} quando todo $x\in
U$ admite uma vizinhança aberta $V\subset\R^n$ contida em $U$ tal
que $f(V)$ é um aberto de $\R^n$ e $f\vert_V:V\to f(V)$ é um
difeomorfismo.

É fácil ver que todo difeomorfismo local $f:U\to\R^n$ é uma
aplicação aberta. Além do mais, se $f:U\to\R^n$ é um difeomorfismo
local injetor então $f(U)$ é aberto em $\R^n$ e $f:U\to f(U)$ é um
difeomorfismo (veja também o Exercício~2).

\smallskip

\proclaim Corolário. Seja $f:U\to\R^n$ uma aplicação de classe
$C^k$ ($1\le k\le\infty$) definida num aberto $U\subset\R^n$. Se
$\dd f(x)$ é um isomorfismo para todo $x\in U$ então $f$ é um
difeomorfismo local. Em particular, $f$ é uma aplicação aberta e
$f(U)$ é um aberto em $\R^n$. Se $f$ for injetora então $f:U\to
f(U)$ é um difeomorfismo (com inverso $f^{-1}:f(U)\to\R^n$ de
classe $C^k$).\fimprova

\smallskip

\noindent{\tt Exemplo}. O exemplo padrão de um difeomorfismo local
não injetor é a ``mudança das coordenadas polares para as
cartesianas'', i.e., a aplicação $f:(0,+\infty)\times\R\to\R^2$
definida por $f(r,\vartheta)=(r\cos\vartheta,r\sen\vartheta)$.
Temos que $f$ é de classe $C^\infty$ e que a matriz Jacobiana de
$f$ no ponto $(r,\vartheta)$ tem determinante igual a $r\ne0$ para
todo $(r,\vartheta)\in(0,+\infty)\times\R$. Segue do teorema da
função inversa que $f$ é de fato um difeomorfismo local;
obviamente $f$ não pode ser injetora pois
$f(r,\vartheta)=f(r,\vartheta+2\pi)$ para todos $r$, $\vartheta$.
Por outro lado, dado $\theta_0\in\R$ a restrição de $f$ à faixa
aberta $(0,+\infty)\times(\theta_0,\theta_0+2\pi)$ é um
difeomorfismo de classe $C^\infty$ sobre o aberto $A_{\theta_0}$
obtido de $\R^2$ removendo a semi-reta fechada
$\big\{(t\cos\theta_0,t\sen\theta_0):t\ge0\big\}$.

\medskip

\item{(2)} {\bf Derivadas parciais ``gordas''}.

\smallskip

\proclaim Definição. Sejam $U\subset\R^m$ um aberto, $f:U\to\R^n$
uma função, $x\in U$ um ponto e $S\subset\R^m$ um subespaço
vetorial. Dizemos que $f$ é {\rm diferenciável no ponto $x$ na
direção de $S$} quando existe um operador linear $T:S\to\R^n$ tal
que, definindo uma aplicação $r$ pela fórmula:
$$f(x+h)=f(x)+T(h)+r(h),\quad h\in S,\ x+h\in U,$$
então $\lim_{h\to0}{r(h)\over\Vert h\Vert}=0$ (observe que $r$ é
definida numa vizinhança aberta da origem {\rm em $S$}!).

É fácil ver que a aplicação linear $T:S\to\R^n$ considerada na
definição acima é única quando existe (pois, para todo $v\in S$,
$T(v)$ coincide com a derivada direcional ${\partial
f\over\partial v}(x)$). A aplicação linear $T:S\to\R^n$ é chamada
então a {\sl derivada direcional de $f$ no ponto $x$ na direção do
subespaço $S$} e é denotada por ${\partial f\over\partial S}(x)$
ou simplesmente por $\partial_Sf(x)$.

\smallskip

\noindent{\tt Observação}. Sejam $f:U\to\R^n$ uma aplicação
definida num aberto $U\subset\R^m$, $x\in U$ um ponto e
$S\subset\R^m$ um subespaço. Defina um subconjunto $\widehat
U\subset S$ e uma aplicação $\hat f:\widehat U\to\R^n$ fazendo:
$$\widehat U=\big\{w\in S:x+w\in U\big\},\qquad\hat
f(w)=f(x+w),\quad w\in\widehat U;$$ temos que $\widehat U$ é
aberto em $S$ (pois é a pré-imagem do aberto $U\subset\R^m$ pela
aplicação contínua $S\ni w\mapsto x+w\in\R^m$) e $0\in\widehat U$.
É fácil ver que {\sl $f$ é diferenciável no ponto $x$ na direção
de $S$ se e somente se $\widehat f$ é diferenciável no ponto
$0\in\widehat U$}; nesse caso, a derivada direcional de $f$ no
ponto $x$ na direção de $S$ coincide com a diferencial de $\hat f$
no ponto $0$, i.e., ${\partial f\over\partial S}(x)=\dd\hat f(0)$.

\smallskip

\noindent{\tt Observação}. Quando $S\subset\R^m$ é um subespaço
unidimensional (digamos, gerado por um vetor $v\in\R^m$) então a
existência de ${\partial f\over\partial S}(x)$ é equivalente à
existência da derivada direcional usual ${\partial f\over\partial
v}(x)$; além do mais, o operador linear ${\partial f\over\partial
S}(x)$ definido em $S$ satisfaz ${\partial f\over\partial
S}(x)\cdot v={\partial f\over\partial v}(x)$. Nesse sentido a
definição de derivada na direção de um subespaço dada nesta seção
generaliza a noção usual de derivada direcional.

\smallskip

\noindent{\tt Exemplo}. Quando $f:U\to\R^n$ ($U\subset\R^m$
aberto) é diferenciável num ponto $x\in U$ então $f$ é
diferenciável no ponto $x$ na direção de {\sl qualquer\/}
subespaço $S\subset\R^m$ e a derivada direcional de $f$ no ponto
$x$  na direção de $S$ é simplesmente a restrição da diferencial
total $\dd f(x)$ a $S$, ou seja:
$${\partial f\over\partial S}(x)=\dd f(x)\vert_S.$$
Esse é o caso que de fato nos interessa.

\smallskip

Suponha agora que tenhamos escrito $\R^m$ como uma soma direta de
subespaços $V_1,\ldots,V_k\subset\R^m$, ou seja,
$\R^m=\bigoplus_{i=1}^kV_i$ (recorde que isso significa que todo
vetor $v\in\R^m$ se escreve de modo único na forma
$v=\sum_{i=1}^kv_i$ com cada $v_i\in V_i$; veja Exercícios~2--6,
aula número 9 (03/04), para detalhes). Se $U\subset\R^m$ é um
aberto e $f:U\to\R^n$ é uma função então podemos pensar em $f$
como uma função de $k$ variáveis, identificando cada $k$-upla
$(x_1,\ldots,x_k)\in\prod_{i=1}^kV_i$ com o vetor
$x=\sum_{i=1}^kx_i\in\R^m$. Para $x\in U$, as derivadas
direcionais ${\partial f\over\partial V_i}(x)\in\Lin(V_i,\R^n)$,
$i=1,\ldots,k$ (se existirem) são chamadas as {\sl derivadas
parciais ``gordas''\/} de $f$ com respeito à decomposição em soma
direta $\R^m=\bigoplus_{i=1}^kV_i$; uma vez subentendida a
decomposição $\R^m=\bigoplus_{i=1}^kV_i$ escrevemos:
$$\partial_if(x)={\partial f\over\partial V_i}(x)\in\Lin(V_i,\R^n),\quad i=1,\ldots,k.$$
É fácil ver que a derivada parcial ``gorda'' $\partial_if(x)$
coincide com a diferencial no ponto $x_i\in V_i$ da aplicação
$v_i\mapsto f(x_1+\cdots+x_{i-1}+v_i+x_{i+1}+\cdots+x_k)$ definida
num aberto de $V_i$, i.e., $\partial_if(x)$ coincide com a
diferencial no ponto $x_i\in V_i$ da aplicação obtida de $f$
quando ``deixamos livre'' apenas a $i$-ésima variável.

Se $f$ é diferenciável no ponto $x\in U$ então, como já
observamos:
$$\partial_if(x)={\partial f\over\partial V_i}(x)=\dd
f(x)\vert_{V_i},$$ para todo $i=1,\ldots,k$. Segue então da
linearidade de $\dd f(x)$ que:
$$\dd f(x)\cdot h=\sum_{i=1}^k\partial_if(x)\cdot h_i,$$ onde $h=\sum_{i=1}^kh_i$ e $h_i\in V_i$,
$i=1,\ldots,k$.

\smallskip

\noindent{\tt Observação}. O nome ``derivada parcial gorda'' {\sl
não\/} é oficial, apesar do fato que o conceito em si é padrão (é
comum usar apenas o termo ``derivada parcial'', sem distinção em
relação ao nome usado para as derivadas parciais usuais).

\smallskip

\noindent{\tt Exemplo}. Se $\R^m=\bigoplus_{i=1}^kV_i$ e cada
$V_i$ é gerado por alguns dos vetores da base canônica de $\R^m$
então a derivada parcial ``gorda'' $\partial_if(x)$ é o operador
linear de $V_i$ em $\R^n$ cuja representação matricial com
respeito às bases canônicas é a matriz cujas colunas contém as
derivadas parciais de $f$ correspondentes aos vetores da base
canônica de $\R^m$ que pertencem a $V_i$. Observe então que a
derivada parcial ``gorda'' $\partial_if(x)$ é simplesmente um
aglomerado de derivadas parciais comuns; se $k=m$ e cada $V_i$ é
gerado somente pelo $i$-ésimo vetor da base canônica então as
derivadas parciais ``gordas'' podem ser identificadas com as
derivadas parciais usuais ${\partial f\over\partial x_i}(x)$.

\smallskip

Em certas situações é também interessante considerar decomposições
em soma direta do {\sl contra-domínio\/} de uma função
$f:U\subset\R^m\to\R^n$. Suponha então que
$\R^n=\bigoplus_{i=1}^rW_i$. Denote por $\pi_i:\R^n\to W_i$ o
$i$-ésimo operador de projeção e por $\iota_i:W_i\to\R^n$ a
inclusão de $W_i$ em $\R^n$. A aplicação $f_i:U\to W_i$ definida
por $f_i=\pi_i\circ f$ é chamada a {\sl $i$-ésima coordenada\/} de
$f$ com respeito à decomposição $\R^n=\bigoplus_{i=1}^rW_i$. Das
igualdades:
$$f=\sum_{i=1}^r\iota_i\circ f_i,\qquad f_i=\pi_i\circ f,\quad
i=1,\ldots,r,$$ segue que $f$ é diferenciável num ponto $x\in U$
se e somente se cada $f_i$ é diferenciável em $x$ e que $f$ é de
classe $C^k$ se e somente se cada $f_i$ é de classe $C^k$; quando
$f$ é diferenciável em $x$ segue da regra da cadeia que $\dd
f_i(x)=\pi_i\circ\dd f(x)$ (``a diferencial da $i$-ésima
coordenada é a $i$-ésima coordenada da diferencial'').

\smallskip

\noindent{\tt Observação}. Em tudo que estudamos até agora com
respeito a Cálculo no $\R^n$, pode-se substituir os espaços
Euclideanos $\R^n$ por espaços vetorais reais de dimensão finita
arbitrários munidos de normas arbitrárias --- em geral nos
restringimos a $\R^n$ na exposição apenas por ``razões
psicológicas''. Observamos que na discussão acima, as
decomposições em soma direta $\R^m=\bigoplus_{i=1}^kV_i$ e
$\R^n=\bigoplus_{i=1}^rW_i$ não estão em princípio {\sl de forma
alguma relacionadas com as bases canônicas de $\R^m$ e $\R^n$};
seria então até mais simples visualizar a teoria utlizando espaços
vetoriais reais de dimensão finita arbitrários no lugar de $\R^m$
e $\R^n$.

\smallskip

\noindent{\tt Exemplo}. ({\sl matrizes de blocos}) Sejam $V$, $W$
espaços vetoriais reais de dimensão finita e suponha que sejam
dadas decomposições em soma direta $V=\bigoplus_{j=1}^kV_j$,
$W=\bigoplus_{i=1}^rW_i$. Se $T:V\to W$ é uma transformação linear
então para cada $i=1,\ldots,r$, $j=1,\ldots,k$ obtemos uma
transformação linear $T_{ij}=\pi_i\circ
T\vert_{V_j}\in\Lin(V_j,W_i)$, onde $\pi_i:W\to W_i$ denota o
operador de projeção na $i$-ésima coordenada. Reciprocamente, se
para cada $i=1,\ldots,r$, $j=1,\ldots,k$ for dada uma
transformação linear $T_{ij}:V_j\to W_i$ então {\sl existe uma
única transformação linear $T:V\to W$ tal que $\pi_i\circ
T\vert_{V_j}=T_{ij}$ para todos $i,j$}; explicitamente, $T$ é dada
por:
$$T(v)=\sum_{i=1}^r\sum_{j=1}^kT_{ij}(v_j),$$
onde $v=\sum_{j=1}^kv_j$ e $v_j\in V_j$, $j=1,\ldots,k$. As
transformações lineares $T_{ij}\in\Lin(V_j,W_i)$ são chamadas as
{\sl componentes\/} de $T$ com respeito às decomposições
$V=\bigoplus_{j=1}^kV_j$ e $W=\bigoplus_{i=1}^rW_i$.

Escolha agora uma base em cada espaço $V_j$ e uma base em cada
espaço $W_i$. Concatenando as bases dos espaços $V_j$ obtemos uma
base de $V$ e concatenando as bases dos espaços $W_i$ obtemos uma
base de $W$. Se $T\in\Lin(V,W)$ é tal que $\pi_i\circ
T\vert_{V_j}=T_{ij}\in\Lin(V_j,W_i)$ e se $A_{ij}$ é a matriz que
representa $T_{ij}$ com respeito às bases fixadas em $V_j$ e $W_i$
então a matriz $A$ que representa $T$ com respeito às bases
fixadas em $V$ e $W$ é obtida por {\sl concatenação dos blocos
$A_{ij}$}; escrevemos:
$$A=\pmatrix{A_{11}&A_{12}&\cdots&A_{1k}\cr
A_{21}&A_{22}&\cdots&A_{2k}\cr \vdots&\vdots&\ddots&\vdots\cr
A_{r1}&A_{r2}&\cdots&A_{rk}\cr}.$$ A descrição dada acima para a
matriz $A$ é normalmente conhecida como uma descrição por {\sl
matrizes de blocos\/} ({\sl block-matrices}); a idéia é que
descrevemos a matriz $A$ através de submatrizes menores, sem
individualizar suas entradas. A grande vantagem da notação de
matriz de blocos é a sua compatibilidade com a regra usual de
multiplicação de matrizes; de fato, dadas matrizes de blocos:
$$A=\pmatrix{A_{11}&A_{12}&\cdots&A_{1k}\cr
A_{21}&A_{22}&\cdots&A_{2k}\cr \vdots&\vdots&\ddots&\vdots\cr
A_{r1}&A_{r2}&\cdots&A_{rk}\cr},\quad
B=\pmatrix{B_{11}&B_{12}&\cdots&B_{1s}\cr
B_{21}&B_{22}&\cdots&B_{2s}\cr \vdots&\vdots&\ddots&\vdots\cr
B_{k1}&B_{k2}&\cdots&B_{ks}\cr},$$ então o produto $AB$ é dado
por:
$$AB=\pmatrix{C_{11}&C_{12}&\cdots&C_{1s}\cr
C_{21}&C_{22}&\cdots&C_{2s}\cr \vdots&\vdots&\ddots&\vdots\cr
C_{r1}&C_{r2}&\cdots&C_{rs}\cr},$$ onde
$C_{ij}=\sum_{\lambda=1}^kA_{i\lambda}B_{\lambda j}$ para todos
$i=1,\ldots,r$, $j=1,\ldots,s$ ({\sl note bem}: não se pode
confundir $A_{i\lambda}B_{\lambda j}$ com $B_{\lambda
j}A_{i\lambda}$ aqui!).

Se $f$ é uma função definida num aberto de
$V=\bigoplus_{j=1}^kV_j$, tomando valores em
$W=\bigoplus_{i=1}^rW_i$, diferenciável num ponto $x$ de seu
domínio, então $\dd f(x):V\to W$ é um operador linear e sua
componente $\pi_i\circ\dd f(x)\vert_{V_j}$ é exatamente a derivada
parcial ``gorda'' $\partial_jf_i(x)$ da $i$-ésima coordenada
$f_i=\pi_i\circ f$ de $f$. Fixando bases nos espaços $V_j$, $W_i$
e concatenando-as para obter bases em $V$ e $W$ respectivamente,
vemos então que a matriz Jacobiana de $f$ no ponto $x$ (com
respeito às bases escolhidas) pode ser descrita como uma matriz de
blocos formada pelas matrizes que representam as derivadas
parciais ``gordas'' $\partial_jf_i(x)$.

\eject

\item{(3)} {\bf O teorema da função implícita}.

\smallskip

O teorema da função implícita fornece condições suficientes para
que seja possível resolver para $y$ uma igualdade do tipo
``$f(x,y)=\hbox{constante}$'' obtendo (localmente) uma função
diferenciável $y=\phi(x)$.

\smallskip

\proclaim Teorema. (da função implícita) Seja $f:U\to\R^n$ uma
função de classe $C^k$ ($1\le k\le\infty$) definida num aberto $U$
do produto $\R^m\times\R^n$ e seja $(x_0,y_0)\in U$; escreva
$c=f(x_0,y_0)$. Suponha que a derivada parcial ``gorda''
$\partial_2f(x_0,y_0):\R^n\to\R^n$ é um isomorfismo. Então existem
uma vizinhança aberta $V$ de $x_0$ em $\R^m$, uma vizinhança
aberta $W$ de $y_0$ em $\R^n$ e uma função $\phi:V\to\R^n$ de
classe $C^k$ com $\phi(V)\subset W$, $V\times W\subset U$ e tal
que para todos $(x,y)\in V\times W$ temos $f(x,y)=c$ se e somente
se $y=\phi(x)$. Além do mais, a diferencial da função $\phi$ no
ponto $x_0$ é dada por:
$$\dd\phi(x_0)=-\partial_2f(x_0,y_0)^{-1}\circ\partial_1f(x_0,y_0).$$

\Prova Uma vez estabelecida a existência de $\phi$, a dedução da
fórmula para $\dd\phi(x_0)$ é simples: diferenciamos a igualdade
$f\big(x,\phi(x)\big)=c$ no ponto $x_0$ usando a regra da cadeia
obtendo:
$$\dd
f(x_0,y_0)\circ\big(\Id,\dd\phi(x_0)\big)=\partial_1f(x_0,y_0)+\partial_2f(x_0,y_0)\circ\dd\phi(x_0)=0,$$
já que $\phi(x_0)=y_0$. A conclusão segue.

Procedemos agora com a prova da existência de $\phi$. Defina
$G:U\to\R^m\times\R^n$ fazendo $G(x,y)=\big(x,f(x,y)\big)$, para
todos $(x,y)\in U$. Temos que $G$ é de classe $C^k$ e sua
diferencial no ponto $(x_0,y_0)\in U$ é dada por:
$$\dd G(x_0,y_0)\cdot(h,k)=\big(h,\partial_1f(x_0,y_0)\cdot
h+\partial_2f(x_0,y_0)\cdot k\big),$$ para todos $h\in\R^m$,
$k\in\R^n$. Usando a fórmula acima e o fato que
$\partial_2f(x_0,y_0)$ é um isomorfismo vê-se facilmente que $\dd
G(x_0,y_0):\R^m\times\R^n\to\R^m\times\R^n$ é um isomorfismo. Pelo
teorema da função inversa, existe uma vizinhança aberta de
$(x_0,y_0)$ em $U$ (que podemos tomar como sendo um produto
$V_1\times W$ de um aberto $V_1$ de $\R^m$ por um aberto $W$ de
$\R^n$) tal que $G(V_1\times W)$ é aberto em $\R^m\times\R^n$ e
$G\vert_{V_1\times W}:V_1\times W\to G(V_1\times W)$ é um
difeomorfismo (cujo inverso é de classe $C^k$). Temos que
$G(x_0,y_0)=(x_0,c)$ pertence ao aberto $G(V_1\times
W)\subset\R^m\times\R^n$ e portanto, como a aplicação
$x\mapsto(x,c)$ é contínua, existe uma vizinhança aberta $V$ de
$x_0$ em $V_1$ tal que $(x,c)\in G(V_1\times W)$ para todo $x\in
V$. Defina agora $\phi:V\to\R^n$ fazendo:
$$\phi(x)=\pi_2\big[(G\vert_{V_1\times W})^{-1}(x,c)\big],$$
para todo $x\in V$, onde $\pi_2:\R^m\times\R^n\to\R^n$ denota a
projeção na segunda coordenada. Obviamente $\phi$ é de classe
$C^k$, $\phi(V)\subset W$ e $V\times W\subset V_1\times W\subset
U$. Finalmente, para $x\in V$, $y\in W$ temos:
$$f(x,y)=c\Longleftrightarrow
G(x,y)=(x,c)\Longleftrightarrow(G\vert_{V_1\times
W})^{-1}(x,c)=(x,y)\Longleftrightarrow
\phi(x)=y.\fimprova$$

\smallskip

\noindent{\tt Observação}. A hipótese que
$\partial_2f(x_0,y_0):\R^n\to\R^n$ seja um isomorfismo significa
que as últimas $n$ colunas da matriz Jacobiana de $f$ no ponto
$(x_0,y_0)$ formam uma matriz inversível.

\smallskip

\noindent{\tt Observação}. Obviamente o teorema da função
implícita continua válido (com a mesma demonstração) se tomarmos
no lugar de $\R^{m+n}$ um espaço vetorial real de dimensão finita
arbitrário $Z$, uma decomposição em soma direta qualquer
$Z=Z_1\oplus Z_2$ (em vez de $Z_1=\R^m$, $Z_2=\R^n$). Em
particular, podemos tomar $Z=\R^{m+n}$ e os subespaços
$Z_1,Z_2\subset Z$ gerados por vetores da base canônica de
$\R^{m+n}$, mas {\sl não sendo $Z_2$ necessariamente gerado pelos
$n$ últimos vetores}! Do ponto de vista prático, temos o seguinte:
considere um sistema:
$$\left\{\eqalign{f_1(z_1,\ldots,z_{m+n})&=c_1,\cr
f_2(z_1,\ldots,z_{m+n})&=c_2,\cr \noalign{\vdots}
f_n(z_1,\ldots,z_{m+n})&=c_n,\cr}\right.$$ com $n$ equações e
$m+n$ incógnitas $z_1,\ldots,z_{m+n}$, sendo cada $f_i$ uma função
de classe $C^k$ ($1\le k\le\infty$) definida num aberto de
$\R^{m+n}$. Suponha que tenha sido encontrada alguma solução
$z_0\in\R^{m+n}$ do sistema e suponha que nesse ponto $z_0$
podemos encontrar $n$ colunas $j_1,\ldots,j_n$ da matriz Jacobiana
de $f=(f_1,\ldots,f_n)$ que formam uma matriz ($n\times n$)
inversível. Aplicamos então o teorema da função implícita com $Z=\R^{m+n}$,
$Z_2\subset Z$ o subespaço gerado pelos vetores $e_{j_1}$, \dots, $e_{j_n}$ da
base canônica de $\R^{m+n}$ e $Z_1\subset Z$ o subespaço gerado pelos demais
vetores da base
canônica de $\R^{m+n}$ (de modo que $Z=Z_1\oplus Z_2$). Concluímos então que,
numa vizinhança de $z_0$, pode-se ``resolver'' o sistema escrevendo as $n$
incógnitas $z_{j_1},\ldots,z_{j_n}$ em função das restantes $m$
incógnitas, utilizando funções de classe $C^k$.

É interessante
fazer agora uma comparação com o caso de sistemas lineares: quando as colunas
$j_1,\ldots,j_n$ da matriz de coeficientes de um sistema linear ($n$ equações,
$n+m$ incógnitas) formam uma
matriz inversível então podemos resolver o sistema (globalmente)
escrevendo a solução em {\sl forma paramétrica\/} em termos das
{\sl variáveis livres\/} $z_{j_1},\ldots,z_{j_n}$. O teorema da
função implícita nos diz então que o mesmo vale para sistemas não
lineares, {\sl mas só localmente}!

\smallskip

Dados conjuntos arbitrários $A$, $B$ e uma função $\phi:A\to B$,
recorde que o {\sl gráfico\/} de $\phi$ é o conjunto:
$$\graph(\phi)=\big\{\big(x,\phi(x)\big):x\in A\big\}\subset
A\times B.$$

\smallskip

\noindent{\tt Observação}. Em geral, em cursos de teoria dos
conjuntos, aquilo que definimos acima como sendo o gráfico de
$\phi$ não é nada mais do que a própria função $\phi$. Embora
dentro da teoria moderna dos conjuntos {\sl todos\/} os objetos
matemáticos sejam conjuntos (até os números naturais!), no
dia a dia do trabalho matemático, raramente pensa-se em funções
como sendo conjuntos. Daí o nome ``gráfico de $\phi$'' para a
própria função $\phi$.

\smallskip

Podemos agora enunciar o teorema da função implícita da seguinte
maneira: {\sl se \hbox{$f:U\to\R^n$} é uma função de classe $C^k$ ($1\le
k\le\infty$) definida num aberto $U\subset\R^m\times\R^n$,
$(x_0,y_0)\in U$, $f(x_0,y_0)=c\in\R^n$ e
$\partial_2f(x_0,y_0):\R^n\to\R^n$ é um isomorfismo então existem
uma vizinhança aberta $V$ de $x_0$ em $\R^m$, uma vizinhança
aberta $W$ de $y_0$ em $\R^n$ de modo que $V\times W\subset U$ e
que $f^{-1}(c)\cap(V\times W)$ é o gráfico de uma função
$\phi:V\to W$ de classe $C^k$}.

A (única) função $\phi$ cujo gráfico é $f^{-1}(c)\cap(V\times W)$
é dita {\sl definida implicitamente\/} pela igualdade $f(x,y)=c$ em torno do
ponto $(x_0,y_0)$.

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Álgebra Linear}.

\medskip

\item{1.} Mostre que uma transformação linear $T:\R\to\R$ é um
isomorfismo se e somente se $T(1)\ne0$. Nesse caso, mostre que
$T^{-1}(1)={1\over T(1)}$.

\medskip

\noindent{\tt Aplicações abertas e difeomorfismos locais}.

\smallskip

\item{2.} Sejam $M$, $N$ espaços métricos e $f:M\to N$ uma função.
Dizemos que $f$ é um {\sl homeomorfismo local\/} quando todo ponto
$x\in M$ possui uma vizinhança aberta $V$ em $M$ tal que $f(V)$ é
aberto em $N$ e $f\vert_V:V\to f(V)$ é um homeomorfismo. Mostre
que:
\smallskip
\itemitem{(a)} todo homeomorfismo local é uma aplicação aberta e contínua;
\itemitem{(b)} se $f$ é um homeomorfismo local injetor então
$f:M\to f(M)$ é um homeomorfismo (e $f(M)$ é aberto em $N$);
\itemitem{(c)} se $f$ é {\sl localmente injetora\/} (i.e., injetora numa
vizinhança de cada ponto), aberta e contínua então $f$ é um
homeomorfismo local;
\itemitem{(d)} se $U\subset\R^m$ é um aberto e $f:U\to\R^m$ é um
difeomorfismo local então $f$ também é um homeomorfismo local.

\smallskip

\item{3.} Sejam $M$, $N$ espaços métricos. Mostre que:
\smallskip
\itemitem{(a)} se $f:M\to N$ é uma aplicação aberta e $N'\subset N$ contém a
imagem de $f$ então também $f:M\to N'$ é uma aplicação aberta;
\itemitem{(b)} se $N'$ é um aberto de $N$ e $f:M\to N'$ é uma aplicação aberta
então também $f:M\to N$ é uma aplicação aberta;
\itemitem{(c)} se $M'$ é um aberto de $M$ e $f:M\to N$ é uma aplicação aberta
então também $f\vert_{M'}:M'\to N$ é uma aplicação aberta;
\itemitem{(d)} repita os itens (a)--(c) trocando ``$f$ é uma aplicação
aberta'' por ``$f$ é uma aplicação fechada'', ``$M'$ é aberto em $M$'' por
``$M'$ é fechado em $M$'' e ``$N'$ é aberto em $N$'' por ``$N'$ é fechado em
$N$''.

\smallskip

\item{4.} Dados espaços métricos $M$, $N$, $P$, considere o diagrama
comutativo:
$$\xymatrix{M\ar[d]_{q}\ar[dr]^f\\N\ar[r]_{\bar f}&P}$$
onde $q$ é uma aplicação contínua e sobrejetora que é aberta ou
fechada. Mostre que $f$ é contínua se e somente se $\bar f$ é contínua (a
aplicação $\bar f$ diz-se obtida de $f$ por {\sl passagem ao quociente}).

\noindent[{\sl dica}: para $A\subset P$ observe que $\bar
f^{-1}(A)=q\big(f^{-1}(A)\big)$].

\vfil\eject

\noindent{\tt Derivadas parciais ``gordas'' e matrizes de blocos}.

\smallskip

\item{5.} Sejam $A$ uma matriz real $n\times n$, $B$ uma matriz
real $m\times m$ e $C$ uma matriz real $m\times n$; considere a
matriz real $(n+m)\times(n+m)$ definida em notação de blocos por:
$$X=\pmatrix{A&0\cr C&B\cr}.$$
Mostre que $X$ é inversível se e somente se $A$ e $B$ o forem; em
caso afirmativo, calcule a inversa de $X$ ({\sl dica}: procure uma
inversa da forma:
$$X'=\pmatrix{A'&D'\cr C'&B'\cr}$$
para $X$).

\vfil\eject

\centerline{\bf Aula número $23$ (31/05)}
\bigskip
\bigskip

A aula número $23$ cobriu parte da demonstração do teorema da função inversa e
o material das seções (2) e (3) da aula número 22 --- 29/05 (derivadas parciais
``gordas'' e o teorema da função implícita).

\bigskip\bigskip\bigskip\bigskip

\centerline{\bf Aula número $24$ (05/06)}
\bigskip
\bigskip

\item{(1)} {\bf Sistemas de coordenadas}.

\smallskip

Esta seção funciona como uma preparação psicológica para a noção de variedade
diferenciável e para os enunciados das formas locais das imersões, submersões
e para o teorema do posto.

\smallskip

Começamos com uma pergunta: qual deve ser a definição correta para
o conceito de ``sistema de coordenadas''? Bom, a idéia básica é a
seguinte: começamos com um ``mundo abstrato'' $X$ onde temos uma
certa quantidade de ``habitantes''. Um certo habitante de $X$
deseja usar algum sistema de coordenadas para descrever (ao menos
uma parte de) $X$. Tal habitante deve então associar a cada ponto
de $X$ uma $n$-upla $(x_1,\ldots,x_n)$ de números reais, que
seriam as ``coordenadas'' desse ponto $x$. Um bom sistema de
coordenadas deve ter a propriedade que pontos diferentes possuem
coordenadas diferentes (senão seria uma tremenda confusão!). Essa
visão caricata do conceito de sistema de coordenadas motiva a
seguinte: \proclaim Definição. Seja $X$ um conjunto qualquer. Um
{\rm sistema de coordenadas} em $X$ é uma função bijetora
$\varphi:U\to\widetilde U$, onde $U$ é um subconjunto de $X$ e
$\widetilde U$ é um subconjunto aberto de $\R^n$ para algum $n$.

A exigência de que $\widetilde U$ seja {\sl aberto\/} em $\R^n$ é feita por
razões técnicas e é importante na teoria de variedades diferenciáveis. Num
primeiro momento, seria razoável exigir apenas que $\widetilde U$ fosse um
subconjunto arbitrário de $\R^n$.

\smallskip

\noindent{\tt Exemplo}. Seja $X=\R^n$. Fazendo $U=\widetilde U=\R^n$ e
$\varphi$ igual à aplicação identidade então o sistema de coordenadas
$\varphi:U\to\widetilde U$ em $X=\R^n$ é chamado o {\sl sistema de coordenadas
cartesianas}.

\smallskip

\noindent{\tt Exemplo}. Escolha $\theta_0\in\R$ e seja
$U=A_{\theta_0}\subset\R^2$ o aberto de $\R^2$ cujo complementar é
a semi-reta fechada
$\big\{(t\cos\theta_0,t\sen\theta_0):t\ge0\big\}$. Seja
$\widetilde U=(0,+\infty)\times(\theta_0,\theta_0+2\pi)$ e defina
$\varphi:U\to\widetilde U$ fazendo $\varphi(x,y)=(\rho,\theta)$,
onde $\rho=\sqrt{x^2+y^2}$ e $\theta$ é o único ângulo para
$(x,y)$ em $(\theta_0,\theta_0+2\pi)$, i.e., $\theta$ é o único
elemento de $(\theta_0,\theta_0+2\pi)$ tal que $x=\rho\cos\theta$
e $y=\rho\sen\theta$. É fácil ver que a aplicação
$\varphi:U\to\widetilde U$ é de fato bijetora e é portanto um
sistema de coordenadas em $\R^2$; esse é chamado o {\sl sistema de
coordenadas polares\/} (relativo à escolha de $\theta_0$) no plano
$\R^2$.

\smallskip

\noindent{\tt Exemplo}. Escolha $\theta_0\in\R$ e defina
$A_{\theta_0}$ como no exemplo anterior. Considere os abertos
$U=A_{\theta_0}\times\R\subset\R^3$, $\widetilde
U=(0,+\infty)\times(\theta_0,\theta_0+2\pi)\times\R\subset\R^3$ e
defina $\varphi:U\to\widetilde U$ fazendo
$\varphi(x,y,z)=(\rho,\theta,z)$, onde $\rho=\sqrt{x^2+y^2}$ e
$\theta$ é o único ângulo para o ponto $(x,y)$ pertencente a
$(\theta_0,\theta_0+2\pi)$. Temos que $\varphi:U\to\widetilde U$ é
um sistema de coordenadas no espaço $\R^3$ chamado o {\sl sistema
de coordenadas cilíndricas\/} (relativo à escolha de $\theta_0$)
no espaço $\R^3$.

\smallskip

\noindent{\tt Exemplo}. Sejam
$U=\R^3\setminus\big(\left(-\infty,0\right]\times\{0\}\times\R\big)$,
$\widetilde U=(0,+\infty)\times({-\pi},\pi)\times(0,\pi)$ e
$\varphi:U\to\widetilde U$ a aplicação definida por
$\varphi(x,y,z)=(r,\theta,\phi)$, onde $r\in(0,+\infty)$,
$\theta\in({-\pi},\pi)$ e $\phi\in(0,\pi)$ são os únicos escalares
para os quais as relações:
$$x=r\cos\theta\sen\phi,\quad y=r\sen\theta\sen\phi,\quad z=r\cos\phi,$$
são satisfeitas (note que $r=\sqrt{x^2+y^2+z^2}$). Temos que $\varphi$ é um
sistema de coordenadas em $\R^3$ chamado o {\sl sistema de coordenadas
esféricas\/} do espaço $\R^3$.

\smallskip

\noindent{\tt Exemplo}. Seja $\frak B=(b_i)_{i=1}^n$ uma base arbitrária de
$\R^n$. Seja $\varphi:\R^n\to\R^n$ a única transformação linear tal que
$\varphi(b_i)$ é o $i$-ésimo vetor da base canônica de $\R^n$ para todo
$i=1,\ldots,n$. Temos que $\varphi$ é um isomorfismo e portanto um sistema de
coordenadas (com $U=\widetilde U=\R^n$) em $\R^n$. Note que para todo
$x\in\R^n$ temos que $\varphi(x)$ coincide precisamente com a $n$-upla formada
pelas coordenadas de $x$ na base $\frak B$. Dizemos que $\varphi$ é um {\sl
sistema de coordenadas linear\/} em $\R^n$. Quando $\frak B$ é a base
canônica, temos que $\varphi=\Id$, i.e., reobtemos as coordenadas
cartesianas. Em geral, o sistema de coordenadas $\varphi$ corresponde à idéia
de usar ``eixos de coordenadas oblíquos'' e ``escalas de medida arbitrárias''
em cada um dos eixos. Mais geralmente, fixada uma base $\frak B$ em $\R^n$ e
um ponto ${\cal O}\in\R^n$ então podemos definir um sistema de coordenadas
$\varphi:\R^n\to\R^n$ fazendo $\varphi(x)$ igual às coordenadas de $x-{\cal
O}$ na base $\frak B$. Dizemos então que $\varphi$ é um {\sl sistema de
coordenadas afim com origem ${\cal O}$}. Quando ${\cal O}=0$, estamos de volta
ao caso de um sistema de coordenadas linear.

\smallskip

A definição de sistema de coordenadas que demos no início da seção é um tanto
geral demais para nossos propósitos imediatos. De fato, observe que todos os
sistemas de coordenadas mencionados nos exemplos acima se enquadram na
seguinte definição mais restrita.
\proclaim Definição. Um {\rm sistema de coordenadas de classe $C^k$ ($1\le
k\le\infty$) em $\R^n$}
é um difeomorfismo $\varphi:U\to\widetilde U$ de classe $C^k$, onde tanto $U$
como $\widetilde U$ são abertos em $\R^n$. Por um {\rm sistema de coordenadas
de classe $C^0$} em $\R^n$ entendemos simplesmente um homeomorfismo
$\varphi:U\to\widetilde U$, onde $U$ e $\widetilde U$ são abertos em $\R^n$.

Observe que todos os exemplos mencionados acima são sistemas de coordenadas de
classe $C^\infty$ em $\R^n$.

\smallskip

Para finalizar, apresentamos alguns exemplos um pouco diferentes
(que não correspondem a sistemas de coordenadas em $\R^n$).

\noindent{\tt Exemplo}. Denote por $S^2$ a {\sl esfera unitária
bidimensional}, ou seja:
$$S^2=\big\{(x,y,z)\in\R^3:x^2+y^2+z^2=1\big\}.$$
Seja $U\subset S^2$ o aberto (relativo a $S^2$) definido por:
$$U=S^2\setminus\big(\{0\}\times\left[0,+\infty\right)\times\R\big),$$
i.e., $U$ é o complementar em $S^2$ de um meridiano fechado.
Definimos uma aplicação $\varphi:U\to\R^2$ fazendo
$\varphi(x,y,z)=(\theta,\phi)$ onde $\theta$ é a ``longitude'' de
$(x,y,z)$ e $\phi$ é a ``latitude'' de $(x,y,z)$; mais
explicitamente, $\theta\in({-\pi},\pi)$,
$\phi\in\big({-{\pi\over2}},{\pi\over2}\big)$ são unicamente
determinados pelas relações:
$$x=\cos\phi\sen\theta,\quad y=-\cos\phi\cos\theta,\quad z=\sen\phi.$$
Temos que $\varphi$ é uma bijeção sobre o aberto $\widetilde
U=({-\pi},\pi)\times\big({-{\pi\over2}},{\pi\over2}\big)$ em
$\R^2$. Portanto $\varphi$ é um sistema de coordenadas na esfera
unitária $S^2$.

\smallskip

\noindent{\tt Exemplo}. Seja $V$ um espaço vetorial real de
dimensão $n<+\infty$ e seja $\frak B=(b_i)_{i=1}^n$ uma base para
$V$. Existe uma única aplicação linear $\varphi:V\to\R^n$ que leva
o vetor $b_i$ sobre o $i$-ésimo vetor $e_i$ da base canônica de
$\R^n$. A aplicação $\varphi$ é um isomorfismo que leva cada vetor
$v\in V$ sobre a $n$-upla que contém as coordenadas de $v$ na base
$\frak B$. Temos que $\varphi:V\to\R^n$ é um sistema de
coordenadas em $V$; diz-se que $\varphi$ é um {\sl sistema de
coordenadas linear\/} no espaço vetorial $V$. Na verdade, o
presente exemplo é apenas uma pequena generalização do exemplo
onde mencionamos sistemas de coordenadas lineares em $\R^n$ (veja
também o Exercício~2 para uma generalização dos sistemas de
coordenadas afins). Observe no entanto que se $V$ é um espaço
vetorial real arbitrário de dimensão $n$ então não há um {\sl
sistema de coordenadas canônico\/} em $V$ (por isso um espaço
vetorial real genérico de dimensão $3$ é um modelo mais adequado
para o ``espaço físico'' do que $\R^3$, já que o ``espaço físico''
não possui uma base canônica --- na verdade, {\sl espaços afins\/}
de dimensão $3$ são um modelo ainda melhor, já que o ``espaço
físico'' não possui sequer uma {\sl origem\/} canônica).

\smallskip

\noindent{\tt Observação}. Quem já estudou um pouco de teoria de
cardinais em cursos de teoria dos conjuntos sabe que existe uma
bijeção $\varphi:S^2\to\R$ da esfera unitária $S^2$ sobre a reta
real $\R$ (isso segue por exemplo do teorema de
Schröder--Bernstein e do fato que $\R^3$ tem a mesma cardinalidade
que $\R$). Tal bijeção $\varphi$ é a rigor um sistema de
coordenadas em $S^2$ pela nossa definição geral, apesar do fato
que esse sistema de coordenadas $\varphi$ deve parecer ``um tanto
estranho''. Quando estudarmos a noção de variedade diferenciável
faremos algumas restrições adicionais sobre a noção de sistema de
coordenadas que eliminam patologias desagradáveis como essa.

\smallskip

\proclaim Definição. Sejam $X$, $Y$ conjuntos e $\varphi:U\subset
X\to\widetilde U\subset\R^m$, $\psi:V\subset Y\to\widetilde
V\subset\R^n$ sistemas de coordenadas para $X$ e $Y$
respectivamente. Seja $f:X\to Y$ uma função tal que $f(U)\subset
V$ e considere a função $\tilde f:\widetilde U\to\widetilde V$
dada pela composição $\tilde f=\psi\circ f\circ\varphi^{-1}$.
Dizemos que $\tilde f$ é a função que {\rm representa} $f$ com
respeito aos sistemas de coordenadas $\varphi$ e $\psi$.

A relação entre $f$ e $\tilde f$ é representada pelo seguinte
diagrama comutativo:
$$\xymatrix{U\ar[r]^f\ar[d]_\varphi^{\scriptscriptstyle\cong}&V\ar[d]^\psi_{\scriptscriptstyle\cong}\\
\widetilde U\ar[r]_{\tilde f}&\widetilde V}$$ o símbolo $\cong$
foi usado para indicar que $\varphi$ e $\psi$ são bijeções. No
Exercício~1 pedimos para vocês relacionarem a noção acima com a
noção usual da Álgebra Linear de ``matrizes que representam
operadores lineares em bases''.
\medskip

\item{(2)} {\bf A versão linear do teorema do posto}.

\smallskip

Em álgebra linear é muito estudado o problema de diagonabilidade
de operadores lineares $T:V\to V$, onde $V$ é um espaço vetorial
de dimensão finita. O problema consiste em achar uma base de $V$
de modo que a matriz que representa $T$ nessa base seja diagonal.
Tal tarefa não é sempre realizável, i.e., existem operadores que
não são diagonalizáveis. A vantagem básica de diagonalizar um
operador linear é basicamente óbvia: quer-se um sistema de
coordenadas no qual $T$ seja descrito de maneira simples.

Vamos tratar aqui um problema muito mais simples do que o da
diagonalização: dada uma transformação linear $T:V\to W$ (com $V$,
$W$ espaços vetoriais possivelmente distintos, de dimensão
finita), queremos encontrar bases de $V$ e $W$ que tornem a
representação matricial de $T$ o mais ``simples'' possível. Note
que mesmo quando $V=W$ tal problema é mais simples do que o
problema usual de diagonalização; de fato, permitimos aqui que
sejam usadas {\sl bases diferentes\/} no domínio e no
contra-domínio de $T$.

\smallskip

Temos o seguinte: \proclaim Teorema. Sejam $V$, $W$ espaços
vetoriais com $\Dim(V)=m<+\infty$ e $\Dim(W)=n<+\infty$. Dada uma
transformação linear $T:V\to W$ então existem bases $\frak B$ e
$\frak B'$ para $V$ e $W$ respectivamente de modo que a matriz
$[T]_{\frak B\frak B'}$ que representa $T$ com respeito a tais
bases é dada (em notação de blocos) por:
$$[T]_{\frak B\frak B'}=\pmatrix{{\rm I}_k&0_{k\times(m-k)}\cr
0_{(n-k)\times k}&0_{(n-k)\times(m-k)}\cr},$$ onde ${\rm I}_k$
denota a matriz identidade $k\times k$ e $0_{\alpha\times\beta}$
denota a matriz nula $\alpha\times\beta$. Além do mais, o número
$k$ é precisamente o posto de $T$ (i.e., a dimensão de $\Img(T)$).

\Prova Em primeiro lugar, se tais bases existirem então $k$ deve
coincidir com o posto de $T$ pois o posto de $T$ deve ser igual ao
posto da matriz $[T]_{\frak B\frak B'}$ (que é $k$). Vamos agora
mostrar a existência das bases $\frak B$ e $\frak B'$. Escolha uma
base qualquer de $\Ker(T)$ e complete-a até uma base de $V$;
obtemos então uma base $\frak B=(b_i)_{i=1}^m$ de $V$ tal que
$(b_i)_{i=k+1}^m$ é uma base de $\Ker(T)$. Temos que
$(b_i)_{i=1}^k$ é uma base para um subespaço $S\subset V$ tal que
$V=S\oplus\Ker(T)$. Daí $T$ leva $S$ isomorficamente sobre
$T(V)=\Img(T)\subset W$ (veja Exercício~3); concluímos então que
$b'_i=T(b_i)$, $i=1,\ldots,k$ nos dá uma base para a imagem de
$T$. Escolha agora $\frak B'=(b'_i)_{i=1}^n$ como sendo um
completamento qualquer de $(b'_i)_{i=1}^k$ até uma base de $W$.
Segue agora facilmente que $[T]_{\frak B\frak B'}$ assume a forma
desejada.\fimprova

\smallskip

\noindent{\tt Observação}. Usando o resultado do Exercício~1,
vemos que se $\frak B$ e $\frak B'$ são bases como no enunciado do
teorema acima e se $\varphi_{\frak B}:V\to\R^m$, $\varphi_{\frak
B'}:W\to\R^n$ são os correspondentes sistemas de coordenadas então
temos um diagrama comutativo:
$$\xymatrix{V\ar[r]^T\ar[d]^{\scriptscriptstyle\cong}_{\varphi_{\frak
B}}&W\ar[d]_{\scriptscriptstyle\cong}^{\varphi_{\frak B'}}\\
\R^m\ar[r]_{F}&\R^n}$$ onde $F:\R^m\to\R^n$ é dada por:
$$F(x_1,\ldots,x_k,\ldots,x_m)=(x_1,\ldots,x_k,\underbrace{0,\ldots,0}_{n-k\ {\rm zeros}}),$$
para todo $x=(x_1,\ldots,x_m)\in\R^m$. Encontramos então sistemas
de coordenadas (lineares) em $V$ e $W$ que tornam a representação
de $T$ (ou seja, $F$) bem simples!

\smallskip

\noindent{\tt Observação}. Se $T:V\to W$ é injetora então $k=m\le
n $ e o teorema nos dá sistemas de coordenadas nos quais a
representação de $T$ é dada por:
$$F(x_1,\ldots,x_m)=(x_1,\ldots,x_m,\underbrace{0,\ldots,0}_{n-m\
{\rm zeros}}).$$ Nesse caso, podemos fazer ainda uma pequena
melhora no enunciado do teorema: é possível {\sl para toda\/} base
$\frak B$ de $V$ encontrar uma base $\frak B'$ de $W$ na qual
$[T]_{\frak B\frak B'}$ tem a forma desejada. De fato, se $\frak
B=(b_i)_{i=1}^m$ é uma base qualquer de $V$ então $b'_i=T(b_i)$,
$i=1,\ldots,m$ é um conjunto linearmente independente e portanto
pode ser completado a uma base $\frak B'$ para $W$. Segue que
$[T]_{\frak B\frak B'}$ é dada por:
$$[T]_{\frak B\frak B'}=\pmatrix{{\rm I}_m\cr0_{(n-m)\times m}}.$$

\smallskip

\noindent{\tt Observação}. Se $T:V\to W$ é sobrejetora então
$k=n\le m$ e o teorema nos dá sistemas de coordenadas nos quais a
representação de $T$ é dada por:
$$F(x_1,\ldots,x_m)=(x_1,\ldots,x_n).$$
Nesse caso, podemos também fazer uma pequena melhora no enunciado
do teorema: é possível {\sl para toda\/} base $\frak B'$ de $W$
encontrar uma base $\frak B$ de $V$ na qual $[T]_{\frak B\frak
B'}$ tem a forma desejada. De fato, se $\frak B'=(b'_i)_{i=1}^n$ é
uma base qualquer para $W$, escolha um subespaço $S\subset V$ com
$V=S\oplus\Ker(T)$ (veja o Exercício~0); daí $T\vert_S:S\to W$ é
um isomorfismo e portanto $b_i=(T\vert_S)^{-1}(b'_i)$,
$i=1,\ldots,n$ nos dá uma base de $S$. Seja $(b_i)_{i=n+1}^m$ uma
base qualquer de $\Ker(T)$, de modo que $\frak B=(b_i)_{i=1}^n$ é
uma base de $V$. Segue que $[T]_{\frak B\frak B'}$ é dada por:
$$[T]_{\frak B\frak B'}=\pmatrix{{\rm I}_n&0_{n\times(m-n)}}.$$

\smallskip

\noindent{\tt Observação}. Uma aplicação linear $T:V\to W$ que é
injetora ou sobrejetora é muitas vezes chamada uma {\sl aplicação
linear de posto máximo}. Isso se deve ao seguinte fato simples: se
$\dim(V)\le\Dim(W)$ então o maior posto possível para uma
transformação linear $T:V\to W$ é $\Dim(V)$ e $T$ tem posto
$\Dim(V)$ se e somente se for injetora; além do mais, se
$\Dim(V)\ge\Dim(W)$ então o maior posto possível para $T:V\to W$ é
$\Dim(W)$ e $T$ tem posto $\Dim(W)$ se e somente se for
sobrejetora.

\medskip

\item{(3)} {\bf A forma local das imersões}.

\smallskip

Nosso objetivo agora é generalizar os resultados da seção anterior
para o caso de transformações não lineares (mas diferenciáveis).
Começamos com a generalização do teorema da seção anterior no caso
de transformações lineares injetoras.

\smallskip

\proclaim Definição. Seja $f:U\to\R^n$ uma função definida num
aberto $U\subset\R^m$. Se $f$ é diferenciável num ponto $x\in U$ e
se a transformação linear $\dd f(x):\R^m\to\R^n$ é injetora então
dizemos que $f$ é uma {\rm imersão no ponto $x$}. Se $f$ é
diferenciável em $U$ e se $\dd f(x)$ é injetora para todo $x\in U$
então dizemos simplesmente que $f$ é uma {\rm imersão}.

Obviamente só é possível que $f:U\subset\R^m\to\R^n$ seja uma
imersão num ponto $x\in U$ se $m\le n$.

\smallskip

\proclaim Teorema. (forma local das imersões) Suponha que
$f:U\to\R^n$ é uma função de classe $C^k$ ($1\le k\le\infty$)
definida num aberto $U\subset\R^m$ e suponha que $f$ é uma imersão
num ponto $x_0\in U$. Então existem abertos $V\subset\R^m$,
$W,\,\widetilde W \subset\R^n$ e um difeomorfismo
$\varphi:W\to\widetilde W$ de classe $C^k$ com $x_0\in V\subset
U$, $f(V)\subset W$ e de modo que:
$$(\varphi\circ
f)(x_1,\ldots,x_m)=(x_1,\ldots,x_m,\underbrace{0,\ldots,0}_{n-m\
{\rm zeros}}),$$ para todo $x=(x_1,\ldots,x_m)\in V$.

\Prova Seja $S\subset\R^n$ um subespaço (de dimensão $n-m$) tal
que:
$$\R^n=\Img\big(\dd f(x_0)\big)\oplus S.$$
Defina uma aplicação
$G:U\times S\to\R^n$ fazendo:
$$G(x,y)=f(x)+y,$$
para todos $x\in U$, $y\in S$. Obviamente $G$ é de classe $C^k$ e
a diferencial: $$\dd G(x_0,0):\R^m\oplus S\longrightarrow\R^n$$ é
dada por:
$$\dd G(x_0,0)\cdot(h,k)=\dd f(x_0)\cdot h+k,$$
para todos $h\in\R^m$, $k\in S$. Segue facilmente do fato que $\dd
f(x_0)$ é injetora e de $\R^n=\Img\big(\dd f(x_0)\big)\oplus S$
que $\dd G(x_0,0)$ é um isomorfismo. Pelo teorema da função
inversa, $G$ leva uma vizinhança aberta de $(x_0,0)$ em $U\times
S$ (que podemos escolher da forma $V\times V'$, com $V\subset U$ e
$V'\subset S$ abertos) difeomorficamente sobre uma vizinhança
aberta $W=G(V\times V')$ de $G(x_0,0)=f(x_0)$ em $\R^n$. Escolha
agora um isomorfismo qualquer $T_0:S\to\R^{n-m}$ e considere o
isomorfismo $T:\R^m\oplus S\to\R^n$ definido por
$T(x,y)=\big(x,T_0(y)\big)$, $x\in\R^m$, $y\in S$. Temos agora que
a aplicação $\varphi:W\to\R^n$ definida por
$\varphi=T\circ(G\vert_{V\times V'})^{-1}$ é um difeomorfismo de
classe $C^k$ sobre o aberto $\widetilde W=T(V\times V')=V\times
T_0(V')$ em $\R^n$. Além do mais, se $x\in V$ então $(x,0)\in
V\times V'$ e:
$$G(x,0)=f(x)\in W=G(V\times V');$$ finalmente, temos:
$$\varphi\big(f(x)\big)=\big(T\circ(G\vert_{V\times
V'})^{-1}\big)\big(f(x)\big)=T(x,0)=(x,0)=(x_1,\ldots,x_m,\underbrace{0,\ldots,0}_{n-m\
{\rm zeros}}),$$ para todo $x=(x_1,\ldots,x_m)\in V$. Isso
completa a demonstração.\fimprova

\smallskip

O teorema acima nos dá sistemas de coordenadas nos quais uma
imersão pode (num aberto pequeno) ser representada de maneira mais
simples (na verdade, só é necessário escolher um sistema de
coordenadas no {\sl contra-domínio\/} da função --- no domínio
podemos continuar usando as coordenadas cartesianas). Segue então
que para demonstrar propriedades locais de imersões é possível
supor sem perda de generalidade que a imersão em questão é da
forma $(x_1,\ldots,x_m)\mapsto(x_1,\ldots,x_m,0,\ldots,0)$. Temos
os seguintes corolários imediatos desse princípio.

\proclaim Corolário. Se uma função $f:U\subset\R^m\to\R^n$ de
classe $C^1$ é uma imersão num ponto $x_0\in U$ então $f$ é ainda
uma imersão numa vizinhança aberta de $x_0$ em $U$.

\Prova Observe que
$(x_1,\ldots,x_m)\mapsto(x_1,\ldots,x_m,0,\ldots,0)$ é uma
imersão.\fimprova

Na verdade o corolário acima é também uma conseqüência simples da
continuidade de $\dd f$ e do Exercício~7.

\smallskip

\proclaim Corolário. Uma imersão $f:U\subset\R^m\to\R^n$ de classe
$C^1$ é localmente injetora, i.e., todo ponto de $U$ possui uma
vizinhança onde $f$ é injetora.

\Prova Observe que
$(x_1,\ldots,x_m)\mapsto(x_1,\ldots,x_m,0,\ldots,0)$ é
injetora.\fimprova

\smallskip

\noindent{\tt Observação}. Intuitivamente, se $f$ é uma imersão
então a imagem de $f$ ``possui a mesma dimensão'' (num sentido que
será feito preciso no futuro) que o domínio de $f$. A forma local
das imersões confirma essa idéia intuitiva.

\medskip

\item{(4)} {\bf A forma local das submersões}.

\smallskip

\proclaim Definição. Seja $f:U\to\R^n$ uma função definida num
aberto $U\subset\R^m$. Se $f$ é diferenciável num ponto $x\in U$ e
se a transformação linear $\dd f(x):\R^m\to\R^n$ é sobrejetora então
dizemos que $f$ é uma {\rm submersão no ponto $x$}. Se $f$ é
diferenciável em $U$ e se $\dd f(x)$ é sobrejetora para todo $x\in U$
então dizemos simplesmente que $f$ é uma {\rm submersão}.

Obviamente só é possível que $f:U\subset\R^m\to\R^n$ seja uma
submersão num ponto $x\in U$ se $m\ge n$.

\smallskip

\proclaim Teorema. (forma local das submersões) Seja $f:U\to\R^n$
uma função de classe $C^k$ ($1\le k\le\infty$) definida num aberto
$U\subset\R^m$ e suponha que $f$ é uma submersão num ponto $z_0\in
U$. Então existem abertos $V,\,\widetilde V\subset\R^m$ e um
difeomorfismo $\varphi:V\to\widetilde V$ de classe $C^k$ de modo
que $z_0\in V\subset U$ e:
$$\big(f\circ\varphi^{-1}\big)(v_1,\ldots,v_m)=(v_1,\ldots,v_n),$$
para todo $v=(v_1,\ldots,v_m)\in\widetilde V$.

\Prova Seja $S\subset\R^m$ um subespaço (de dimensão $n$) tal que:
$$\R^m=\Ker\big(\dd f(z_0)\big)\oplus S.$$
Defina uma aplicação
$G:U\to\R^n\oplus\Ker\big(\dd f(z_0)\big)$ fazendo:
$$G(x,y)=\big(f(x,y),x\big),$$
para todos $x\in\Ker\big(\dd f(z_0)\big)$, $y\in S$ tais que $(x,y)\in
U\subset\Ker\big(\dd f(z_0)\big)\oplus S=\R^m$. Obviamente $G$ é de classe
$C^k$ e sua diferencial no ponto $z_0=(x_0,y_0)\in\Ker\big(\dd
f(z_0)\big)\oplus S$ é dada por:
$$\dd G(x_0,y_0)\cdot(h,k)=\big(\partial_1f(x_0,y_0)\cdot
h+\partial_2f(x_0,y_0)\cdot k,h\big),$$
para todos $h\in\Ker\big(\dd f(z_0)\big)$, $k\in S$. Como
$\partial_2f(x_0,y_0)=\dd f(z_0)\vert_S:S\to\R^n$ é um isomorfismo (veja o
Exercício~3), segue
facilmente que:
$$\dd G(x_0,y_0):\R^m=\Ker\big(\dd f(z_0)\big)\oplus
S\longrightarrow\R^n\oplus\Ker\big(\dd f(z_0)\big),$$ é um
isomorfismo. Pelo teorema da função inversa, $G$ leva uma
vizinhança aberta $V$ de $z_0=(x_0,y_0)$ em $U$ difeomorficamente
sobre uma vizinhança aberta $V'$ de $G(z_0)=\big(f(z_0),x_0)$ em
$\R^n\oplus\Ker\big(\dd f(z_0)\big)$. Escolha um isomorfismo
qualquer $T_0:\Ker\big(\dd f(z_0)\big)\to\R^{m-n}$ e considere o
isomorfismo $T:\R^n\oplus\Ker\big(\dd f(z_0)\big)\to\R^m$ definido
por $T(u,x)=\big(u,T_0(x)\big)$, $u\in\R^n$, $x\in\Ker\big(\dd
f(z_0)\big)$. Temos agora que $\widetilde V=T(V')$ é um aberto de
$\R^m$ e que a aplicação $\varphi:V\to\widetilde V$ definida por:
$$\varphi=T\circ G\vert_V,$$
é um difeomorfismo de classe $C^k$.
Para finalizar, seja $v=(v_1,\ldots,v_m)\in\widetilde V$. Temos que
$T^{-1}(v)=(u,x)\in V'$, onde $u=(v_1,\ldots,v_n)\in\R^n$ e $x\in\Ker\big(\dd
f(z_0)\big)$ satisfaz $T_0(x)=(v_{n+1},\ldots,v_m)$. Daí:
$$\varphi^{-1}(v)=(G\vert_V)^{-1}(u,x)=(x,y),$$
onde $y\in S$ é caracterizado pelo fato que $(x,y)\in V$ e $f(x,y)=u$. A
conclusão agora é obtida observando que:
$$\big(f\circ\varphi^{-1}\big)(v_1,\ldots,v_m)=\big(f\circ\varphi^{-1}\big)(v)=f(x,y)=u=(v_1,\ldots,v_n).\fimprova$$

\smallskip

O teorema acima nos dá sistemas de coordenadas nos quais uma
submersão pode (num aberto pequeno) ser representada de maneira
mais simples (na verdade, só é necessário escolher um sistema de
coordenadas no {\sl domínio\/} da função --- no contra-domínio
podemos continuar usando as coordenadas cartesianas). Segue então
que para demonstrar propriedades locais de submersões é possível
supor sem perda de generalidade que a submersão em questão é da
forma $(v_1,\ldots,v_m)\mapsto(v_1,\ldots,v_n)$. Temos os
seguintes corolários imediatos desse princípio.

\proclaim Corolário. Se uma função $f:U\subset\R^m\to\R^n$ de
classe $C^1$ é uma submersão num ponto $z_0\in U$ então $f$ é
ainda uma submersão numa vizinhança aberta de $z_0$ em $U$.

\Prova Observe que $(v_1,\ldots,v_m)\mapsto(v_1,\ldots,v_n)$ é uma
submersão.\fimprova

Na verdade o corolário acima é também uma conseqüência simples da
continuidade de $\dd f$ e do Exercício~7.

\smallskip

\proclaim Corolário. Uma submersão $f:U\subset\R^m\to\R^n$ de
classe $C^1$ é uma aplicação aberta.

\Prova Observe que a aplicação
$(v_1,\ldots,v_m)\mapsto(v_1,\ldots,v_n)$ (e também sua restrição
a um aberto qualquer de $\R^m$) é uma aplicação aberta (veja os
Exercícios~11 e 12).\fimprova

\medskip

\item{(5)} {\bf O teorema do posto}.

\smallskip

O próximo teorema generaliza tanto a forma local das imersões
quanto a forma local das submersões.

\proclaim Teorema. (do posto) Seja $f:U\to\R^n$ uma função de classe $C^k$
($1\le k\le\infty$) definida num aberto $U\subset\R^m$. Suponha que o posto de
$\dd f(x)$ é (constante e) igual a $r$ para todo $x\in U$, para algum
$r=0,\ldots,\min\{m,n\}$. Então para todo $z_0\in U$ existem abertos
$V,\widetilde V\subset\R^m$, $W,\widetilde W\subset\R^n$ e difeomorfismos
$\varphi:V\to\widetilde V$, $\psi:W\to\widetilde W$ de classe $C^k$ com
$z_0\in V\subset U$, $f(V)\subset W$ e:
$$\psi\big[f\big(\varphi^{-1}(v_1,\ldots,v_m)\big)\big]=(v_1,\ldots,v_r,\underbrace{0,\ldots,0}_{n-r\
{\rm zeros}}),$$
para todo $v=(v_1,\ldots,v_m)\in\widetilde V$.

\Prova Seja $S\subset\R^n$ um subespaço (de dimensão $n-r$) tal que:
$$\R^n=\Img\big(\dd f(z_0)\big)\oplus S.\eqno{(1)}$$
Segue então da continuidade de $\dd f$ e do lema a seguir que
$\Img\big(\dd f(z)\big)+S=\R^n$ para todo $z$ em alguma vizinhança
aberta $V_0$ de $z_0$ em $U$; como $\Img\big(\dd f(z)\big)$ tem
dimensão $r$, obtemos:
$$\R^n=\Img\big(\dd f(z)\big)\oplus S,\eqno{(2)}$$
para todo $z$ pertencente a $V_0$. Seja
$\pi:\R^n\to\Img\big(\dd f(z_0)\big)$ o operador de projeção correspondente à
soma direta (1). Segue de (2) e do Exercício~4 que $\pi$ leva $\Img\big(\dd
f(z)\big)$ isomorficamente sobre $\Img\big(\dd f(z_0)\big)$. Concluímos então
que a aplicação:
$$\pi\circ f\vert_{V_0}:V_0\longrightarrow\Img\big(\dd f(z_0)\big)$$
é uma submersão (de classe $C^k$), já que $\dd(\pi\circ
f\vert_{V_0})(z)=\pi\circ\dd f(z)$, para
todo $z\in V_0$. Observe também que a injetividade da restrição de $\pi$ a
$\Img\big(\dd f(z)\big)$ implica que:
$$\Ker\big(\pi\circ\dd f(z)\big)=\Ker\big(\dd f(z)\big),\eqno{(3)}$$
para todo $z\in V_0$.

Escolha um isomorfismo qualquer $T:\Img\big(\dd f(z_0)\big)\to\R^r$;
obviamente $T\circ\pi\circ f\vert_{V_0}$ é ainda uma
submersão. Pela forma local das submersões, existem abertos
$V,\widetilde V\subset\R^m$ e um difeomorfismo $\varphi:V\to\widetilde V$ de
classe $C^k$ com $z_0\in V\subset V_0$ e:
$$\big(T\circ\pi\circ f\circ\varphi^{-1}\big)(v_1,\ldots,v_m)=(v_1,\ldots,v_r),\eqno{(4)}$$
para todo $v=(v_1,\ldots,v_m)\in\widetilde V$; podemos também
supor que $\widetilde V$ é da forma $\widetilde V=\widetilde
V_1\times\widetilde V_2$, onde $\widetilde V_1$ é um aberto de
$\R^r$ e $\widetilde V_2$ é um aberto conexo de $\R^{m-r}$.
Diferenciando (4) num ponto $v=\varphi(z)\in\widetilde V$ e
aplicando ao $i$-ésimo vetor $e_i$ da base canônica de $\R^m$
obtemos:
$$\big[T\circ\pi\circ\dd
f(z)\circ\dd\varphi(z)^{-1}\big]\cdot e_i=0,\quad
i=r+1,\ldots,m,$$ para todo $z\in V$. Como $T$ é um isomorfismo,
usando (3) e a fórmula acima concluímos que
$\dd\varphi(z)^{-1}\cdot e_i\in\Ker\big(\dd f(z)\big)$,
$i=r+1,\ldots,m$ e portanto:
$$\dd\big(f\circ\varphi^{-1}\big)\big(\varphi(z)\big)\cdot e_i=\big[\dd f(z)\circ\dd\varphi(z)^{-1}\big]\cdot e_i=0,\quad i=r+1,\ldots,m,$$
para todo $z\in V$. Segue que para todo $u\in\widetilde
V_1\subset\R^r$, a função $f\circ\varphi^{-1}(u,\cdot)$ definida
no aberto conexo $\widetilde V_2\subset\R^{m-r}$ possui
diferencial identicamente nula e portanto é constante; isso
significa que $f\circ\varphi^{-1}$ {\sl não depende\/} das últimas
$m-r$ variáveis, i.e., existe uma função $\alpha:\widetilde
V_1\to\R^n$ de classe $C^k$ com:
$$f\circ\varphi^{-1}(u,u')=\alpha(u),\eqno{(5)}$$
para todos $u\in\widetilde V_1$, $u'\in\widetilde V_2$ (para
definir $\alpha$, escolha qualquer $u'_0\in\widetilde V_2$ e ponha
$\alpha(u)=f\circ\varphi^{-1}(u,u'_0)$, $u\in\widetilde V_1$).
Considere as coordenadas $\alpha_1:\widetilde V_1\to\Img\big(\dd
f(z_0)\big)$ e $\alpha_2:\widetilde V_1\to S$ de $\alpha$ com
respeito à decomposição $\R^n=\Img\big(\dd f(z_0)\big)\oplus S$. A
igualdade (4) nos diz que:
$$T\big(\alpha_1(u)\big)=u,\eqno{(6)}$$
para todo $u\in\widetilde V_1$. Definimos agora o difeomorfismo
$\psi:W\to\widetilde W$ de classe $C^k$ fazendo
$W=T^{-1}(\widetilde V_1)\times S\subset\Img\big(\dd
f(z_0)\big)\oplus S=\R^n$, $\widetilde W=\widetilde
V_1\times\R^{n-r}\subset\R^n$ e:
$$\psi(w,w')=\Big(T(w),T'\big[w'-\alpha_2\big(T(w)\big)\big]\Big),\quad w\in
T^{-1}(\widetilde V_1),\ w'\in S,$$ onde $T':S\to\R^{n-r}$ é um
isomorfismo qualquer. Segue agora de (5) e de (6) que $f(V)\subset
W$ e que:
$$\big(\psi\circ f\circ\varphi^{-1}\big)(u,u')=(u,0),$$
para todos $u\in\widetilde V_1\subset\R^r$, $u'\in\widetilde
V_2\subset\R^{m-r}$.\fimprova

\smallskip

\proclaim Lema. Sejam $V$, $W$ espaços vetoriais reais de dimensão
finita e $S\subset W$ um subespaço. Então o conjunto dos
operadores lineares $T:V\to W$ tais que $\Img(T)+S=W$ é aberto em
$\Lin(V,W)$.

\Prova Considere a aplicação $\lambda:\Lin(V,W)\to\Lin(V\oplus
S,W)$ definida por:
$$\lambda(T)\vert_V=T,\quad\lambda(T)\vert_S=\hbox{inclusão de $S$
em $W$},$$ para todo $T\in\Lin(V,W)$. Temos que $\lambda$ é
contínua (pois é afim) e que $\Img(T)+S=W$ se e somente se
$\lambda(T)$ é sobrejetora. A conclusão segue do fato que o
conjunto dos operadores lineares sobrejetores de $V\oplus S$ em
$W$ é aberto em $\Lin(V\oplus S,W)$ (veja Exercício~7).\fimprova

\smallskip

O teorema do posto nos diz que toda aplicação $f$ cuja diferencial
tem posto constante num aberto pode ser escrita de maneira mais
simples em sistemas de coordenadas apropriados (ao contrário das
formas locais das imersões e das submersões, é possível que
tenhamos que mudar as coordenadas {\sl tanto no domínio como no
contra-domínio} de $f$ para obter a representação desejada para a
função $f$).

\vfil\eject

\noindent{\bf Exercícios}.

(não é para entregar, mas é bom dar uma olhada e quem tiver
problemas me procura).

\bigskip

\noindent{\tt Conjuntos e funções}.

\medskip

\item{-1.} Sejam $X$, $Y$ conjuntos e $f:X\to Y$ uma função.
\smallskip
\itemitem{(a)} Se $X$ é não vazio, mostre que $f$ é injetora se e
somente se admite uma {\sl inversa à esquerda}, i.e., se e somente
se existe uma função $g:Y\to X$ tal que $g\circ f$ é igual à
identidade de $X$.
\itemitem{(b)} Mostre que $f$ é sobrejetora se e somente se admite
uma {\sl inversa à direita}, i.e., se e somente se existe uma
função $g:Y\to X$ tal que $f\circ g$ é igual à identidade de $Y$.

\medskip

\noindent{\tt Álgebra Linear}.

\smallskip

\item{0.} Seja $V$ um espaço vetorial de dimensão finita. Mostre
que todo subespaço $S\subset V$ admite um {\sl subespaço
complementar}, i.e., um subespaço $S'\subset V$ com $V=S\oplus S'$
(na verdade, esse resultado também vale para $\Dim(V)=+\infty$,
mas a demonstração depende do {\sl Lema de Zorn}).

\smallskip

\item{1.} Sejam $V$, $W$ espaços vetoriais reais de dimensão
finita e $\frak B=(b_i)_{i=1}^m$, $\frak B'=(b'_i)_{i=1}^n$ bases
para $V$ e para $W$ respectivamente. Denote por:
$$\varphi_{\frak B}:V\longrightarrow\R^m,\quad\varphi_{\frak
B'}:W\longrightarrow\R^n,$$
respectivamente
os sistemas de coordenadas em $V$ e $W$ associados a $\frak B$ e a
$\frak B'$, i.e., $\varphi_{\frak B}$ é o isomorfismo que leva
$\frak B$ sobre a base canônica de $\R^m$ e $\varphi_{\frak B'}$ é
o isomorfismo que leva $\frak B'$ sobre a base canônica de $\R^n$.
Dada uma transformação linear $T:V\to W$, denote por $A$ a matriz
real $n\times m$ que representa $T$ com respeito às bases $\frak
B$ e $\frak B'$; denote também por $L_A:\R^m\to\R^n$ o {\sl
operador de multiplicação por $A$}, i.e., $L_A(x)=Ax$ para todo
$x\in\R^m$, onde interpretamos $x$ como uma matriz coluna
$m\times1$. Mostre que o seguinte diagrama:
$$\xymatrix{V\ar[r]^T\ar[d]^{\scriptscriptstyle\cong}_{\varphi_{\frak
B}}&W\ar[d]_{\scriptscriptstyle\cong}^{\varphi_{\frak B'}}\\
\R^m\ar[r]_{L_A}&\R^n}$$ comuta, i.e., mostre que $\varphi_{\frak
B'}\circ T=L_A\circ\varphi_{\frak B}$. Isso significa que $L_A$ é
a função que representa $T$ com respeito aos sistemas de
coordenadas $\varphi_{\frak B}$ e $\varphi_{\frak B'}$!

\smallskip

\item{2.} Sejam $V$ um espaço vetorial real e $P$ um conjunto; seja
dada também uma aplicação $\rho:V\times P\to P$ satisfazendo as
seguinte propriedades:
\smallskip
\itemitem{(i)} $\rho\big(v,\rho(w,p)\big)=\rho(v+w,p)$ para todos
$v,w\in V$, $p\in P$;
\itemitem{(ii)} $\rho(0,p)=p$ para todo $p\in P$;
\itemitem{(iii)} se para algum $v\in V$, $p\in P$ temos
$\rho(v,p)=p$ então $v=0$;
\itemitem{(iv)} para todos $p,q\in P$ existe $v\in V$ com
$\rho(v,p)=q$.
\smallskip
A trinca $(P,V,\rho)$ é dita um {\sl espaço afim\/} e $V$ é dito o
{\sl espaço vetorial paralelo} a tal espaço afim. Tipicamente
pensa-se em $P$ como um ``conjunto de pontos'' e, para $p\in P$,
$v\in V$, escreve-se $p+v$ em vez de $\rho(v,p)$, i.e., diz-se que
$\rho(v,p)$ é a {\sl soma do vetor $v$ com o ponto $p$}. Mostre
que dados ${\cal O}\in P$ e $\frak B=(b_i)_{i=1}^n$ uma base para
$V$ então para cada $p\in P$ existe um único
$x=(x_1,\ldots,x_n)\in\R^n$ tal que $p={\cal
O}+\sum_{i=1}^nx_ib_i$; definindo $\varphi(p)=x$, mostre que
obtêm-se uma bijeção $\varphi:P\to\R^n$. Diz-se que $\varphi$ é o
{\sl sistema de coordenadas afim\/} em $P$ com {\sl origem\/}
${\cal O}$ e ``eixos'' $(b_i)_{i=1}^n$.

\smallskip

\noindent{\tt Observação}. Para quem já estudou um pouco de teoria
de ação de grupos: as condições impostas acima sobre $\rho:V\times
P\to P$ dizem que $\rho$ é uma {\sl ação livre e transitiva\/} do
grupo abeliano aditivo $(V,+)$ no conjunto $P$ (livre $=$ ``sem
pontos fixos'').

\smallskip

\item{3.} Sejam $V$, $W$ espaços vetoriais e $T:V\to W$ uma transformação
linear. Mostre que as seguintes condições são equivalentes sobre um subespaço
$S\subset V$:
\smallskip
\itemitem{(i)} $V=\Ker(T)\oplus S$;
\itemitem{(ii)} $T\vert_S:S\to\Img(T)$ é um isomorfismo.
\smallskip
\noindent[{\sl dica}: supondo (ii), para mostrar que $V=\Ker(T)+S$, tome $v\in
V$ e olhe para o vetor $(T\vert_S)^{-1}\big(T(v)\big)\in S$].

\smallskip

\item{4.} Seja $V$ um espaço vetorial e sejam $V_1,V_2,V_2
'$ subespaços de $V$ tais que:
$$V=V_1\oplus V_2,\quad V=V_1\oplus V'_2.$$
Denote por $\pi:V\to V_2$ a projeção em $V_2$ relativa à decomposição
$V=V_1\oplus V_2$ e por $\pi':V\to V'_2$ a projeção em $V'_2$ relativa à
decomposição $V=V_1\oplus V'_2$. Mostre que:
$$\pi'\vert_{V_2}:V_2\longrightarrow
V'_2\quad\hbox{e}\quad\pi\vert_{V'_2}:V'_2\longrightarrow V_2$$
são isomorfismos mutuamente inversos.

\smallskip

\item{5.} Se $A\in\Matr mn(\R)$ é uma matriz real $m\times n$ e se
$I=\{i_1,\ldots,i_k\}\subset\{1,\ldots,m\}$,
$J=\{j_1,\ldots,j_l\}\subset\{1,\ldots,n\}$ são subconjuntos não
vazios então denotamos por $A_{IJ}$ a submatriz $k\times l$ de $A$
que possui apenas as linhas $i_1,\ldots,i_k$ de $A$ e as colunas
$j_1,\ldots,j_l$ de $A$. Se $k=l$ então dizemos que $\det(A_{IJ})$
é um {\sl determinante menor de ordem $k$ de $A$}. Mostre que $A$
possui posto $k$ se e somente se $A$ possui um determinante menor
não nulo de ordem $k$ e todo determinante menor de ordem $k+1$ é
nulo [{\sl dica}: Use o fato que o ``posto coluna'' e o ``posto
linha'' de uma matriz coincidem (veja Exercício~5, item (f), Aula
número 18 --- 15/05); o posto é por definição igual ao ``posto
linha'' e ao ``posto coluna''. Se $A$ tem posto $k$ escolha $k$
colunas linearmente independentes em $A$ e use o fato que a
submatriz de $A$ constituída por essas $k$ colunas possui ``posto linha''
igual a $k$. Mostre também que se $A$ possui um determinante menor
não nulo de ordem $k+1$ então o posto de $A$ é maior ou igual a
$k+1$].

\smallskip

\item{6.} Mostre que se $A\in\Matr mn(\R)$ possui posto $k$ então
$A$ possui uma vizinhança em $\Matr mn(\R)$ formada só por
matrizes de posto maior ou igual a $k$ (diz-se então que o posto é
uma função {\sl semi-contínua inferiormente}).

\noindent[{\sl dica}: pelo Exercício~5, $A$ possui um determinante
menor não nulo de ordem $k$. Use a continuidade da função
determinante para concluir que numa vizinhança pequena de $A$ em
$\Matr mn(\R)$ toda matriz possuirá também um determinante menor
não nulo de ordem $k$].

\smallskip

\item{7.} Dados espaços vetoriais reais de dimensão finita $V$,
$W$, mostre que os conjuntos:
$$\big\{T\in\Lin(V,W):T\ \hbox{é
injetora}\big\}\quad\hbox{e}\quad\big\{T\in\Lin(V,W):T\ \hbox{é
sobrejetora}\big\},$$ são abertos em $\Lin(V,W)$ ({\sl dica}: o
posto de um operador linear coincide com o posto de uma matriz que
o representa numa base qualquer. Use o Exercício~6).

\smallskip

\item{8.} Dados espaços vetoriais $V$, $W$, mostre que um operador linear
$T:V\to W$ é injetor se e somente se $T$ admite um {\sl inverso
linear à esquerda}, i.e., se e somente se existe um operador
linear $S:W\to V$ com $S\circ T$ igual à identidade de $V$ ({\sl
dica}: se $T$ é injetor então $T:V\to\Img(T)$ é um isomorfismo.
Defina $S:W\to V$ como sendo uma extensão linear arbitrária de
$T^{-1}:\Img(T)\to V$).

\smallskip

\item{9.} Dados espaços vetoriais $V$, $W$, mostre que um operador linear
$T:V\to W$ é sobrejetor se e somente se $T$ admite um {\sl inverso
linear à direita}, i.e., se e somente se existe um operador linear
$S:W\to V$ com $T\circ S$ igual à identidade de $W$ ({\sl dica}:
se $Z$ é um subespaço de $V$ com $V=\Ker(T)\oplus Z$ então, pelo
Exercício~3, $T\vert_Z:Z\to W$ é um isomorfismo. Defina
$S=(T\vert_Z)^{-1}$).

\smallskip

\noindent{\tt Observação}. O resultado dos Exercícios~8 e 9 produzem uma
solução interessante para o Exercício~7, usando também a continuidade das
aplicações $T\mapsto S\circ T$, $T\mapsto T\circ S$ e o fato que o conjunto
dos isomorfismos de um espaço $V$ em si mesmo é aberto em $\Lin(V,V)$ (para
achar essa solução, generalize os Exercícios~8 e 9, mostrando que:
$$\eqalign{\hbox{$T\in\Lin(V,W)$ injetora}&\Longleftrightarrow\hbox{existe
$S\in\Lin(W,V)$ com $S\circ T$ um isomorfismo},\cr
\hbox{$T\in\Lin(V,W)$
sobrejetora}&\Longleftrightarrow\hbox{existe $S\in\Lin(W,V)$ com $T\circ S$ um
isomorfismo},\cr}$$
para todo $T\in\Lin(V,W)$).

\smallskip

\item{10.} Sejam $V$, $W$ espaços vetoriais reais de dimensão
finita e $S\subset V$ um subespaço. Mostre que o conjunto dos
operadores lineares $T:V\to W$ tais que $S\cap\Ker(T)=\{0\}$ é
aberto em $\Lin(V,W)$ ({\sl dica}: $S\cap\Ker(T)=\{0\}$ se e
somente se $T\vert_S\in\Lin(S,W)$ é injetor; use o Exercício~7).

\medskip

\noindent{\tt Aplicações abertas}.

\smallskip

\item{11.} Sejam $M$, $N$ espaço métricos e considere $M\times N$
munido da métrica produto. Mostre que as projeções $M\times N\to
M$ e $M\times N\to N$ são aplicações abertas ({\sl dica}: use o
Exercício~20, item (c), aula número 5 --- 20/03).

\smallskip

\item{12.} Sejam $M$, $N$ espaço métricos e $f:M\to N$ uma função.
Suponha que todo ponto $x\in M$ possui uma vizinhança aberta $V$
em $M$ tal que $f\vert_V:V\to N$ é uma aplicação aberta. Mostre
que $f$ é uma aplicação aberta.

\vfil\eject

\centerline{\bf Aula número $25$ (07/06)}
\bigskip
\bigskip

\item{(1)} {\bf Variedades diferenciáveis}.

\smallskip

Uma variedade diferenciável é um ``mundo abstrato'' onde pode-se estudar
Cálculo Diferencial. O seguinte esquema é basicamente auto-explicativo:
\smallskip
\halign{\hfil#\hfil&#$\quad\longleftrightarrow\quad$&#\hfil\cr
Espaços vetoriais&&``mundo abstrato'' onde se estuda transformações lineares\cr
Espaços topológicos&&``mundo abstrato'' onde se estuda limite e
continuidade\cr
Espaços de medida&&``mundo abstrato'' onde se estuda integração\cr
Variedades diferenciáveis&&``mundo abstrato'' onde se estuda Cálculo
Diferencial\cr}

\smallskip

A definição geral de variedade diferenciável é talvez um pouco
abstrata demais para um primeiro contato com o assunto ---
apresentaremos tal definição mais tarde. Nesse curso nos
restringiremos à noção de {\sl subvariedades do espaço Euclideano
$\R^n$}, que é mais intuitiva. Subvariedades $k$-dimensionais de
$\R^n$ generalizam o conceito de superfície regular em $\R^3$ (o
caso de superfícies regulares de $\R^3$ é reobtido com $k=2$,
$n=3$).

\smallskip

Existem várias definições equivalentes para o conceito de subvariedade
$k$-dimensional de $\R^n$ e tais equivalências serão discutidas
adiante. A seguir apresentamos a nossa definição. No que segue,
identificaremos $\R^k$ com o subespaço de $\R^n$ gerado pelos $k$ primeiros
vetores da base canônica, para $k=0,1,\ldots,n$; mais explicitamente, escrevemos:
$$\R^k=\big\{(x_1,\ldots,x_k,0,\ldots,0)\in\R^n:x_1,\ldots,x_k\in\R\big\}.$$

\smallskip

\proclaim Definição. Seja $M\subset\R^n$ um subconjunto não vazio. Dizemos que
$M$ é uma {\rm subvariedade $k$-dimensional de $\R^n$} de classe $C^r$ ($1\le
r\le\infty$) se para todo $x\in M$ existe um difeomorfismo
$\varphi:A\to\widetilde A$ de classe $C^r$ com $A$, $\widetilde A$ abertos em
$\R^n$, $x\in A$ e:
$$\varphi(A\cap M)=\widetilde A\cap\R^k.$$

A definição acima deve ser encarada da seguinte maneira: o subespaço
$\R^k\subset\R^n$ é o ``modelo padrão'' de subvariedade $k$-dimensional de
$\R^n$. Um outro subconjunto $M\subset\R^n$ é chamado uma subvariedade
$k$-dimensional se em torno de cada ponto de $M$ podemos encontrar um sistema
de coordenadas de classe $C^r$ de modo que os ``usuários'' desse sistema de
coordenadas ``vejam'' $M$ como $\R^k\subset\R^n$.

\smallskip

\noindent{\tt Exemplo}. Seja $M$ um subespaço vetorial de $\R^n$ de dimensão
$k$. É fácil ver que existe um isomorfismo linear $T:\R^n\to\R^n$ tal que
$T(M)=\R^k$ (escolha uma base de $M$, estende-a a uma base de $\R^n$ e defina
$T$ mandando tal base sobre a base canônica de $\R^n$). Daí $A=\widetilde
A=\R^n$ são abertos em $\R^n$, $\varphi=T$ é um difeomorfismo de classe $C^\infty$.

%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%

\vskip 5cm
\centerline{\titfont To be continued\dots}


\bye
